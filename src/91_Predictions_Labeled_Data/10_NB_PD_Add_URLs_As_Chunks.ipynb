{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add URLs as Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from datasets import Dataset, ClassLabel, Value, Features\n",
    "from datasets import concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "TOPIC = \"cannabis\" #\"energie\" #\"kinder\" \"cannabis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "URLs per batch and topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_id</th>\n",
       "      <th>url</th>\n",
       "      <th>used_at</th>\n",
       "      <th>duration</th>\n",
       "      <th>yt_video_id</th>\n",
       "      <th>package_version</th>\n",
       "      <th>enddevice</th>\n",
       "      <th>batch</th>\n",
       "      <th>Group</th>\n",
       "      <th>start_date</th>\n",
       "      <th>...</th>\n",
       "      <th>start_intervention</th>\n",
       "      <th>start_knowledge</th>\n",
       "      <th>topic</th>\n",
       "      <th>series</th>\n",
       "      <th>annotation_type</th>\n",
       "      <th>good_for_training</th>\n",
       "      <th>good_for_augmentation</th>\n",
       "      <th>category</th>\n",
       "      <th>is_direct_topic_annotated</th>\n",
       "      <th>is_direct_topic_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>273726366948</td>\n",
       "      <td>mingle.respondi.de/</td>\n",
       "      <td>2023-06-13 14:12:16</td>\n",
       "      <td>393</td>\n",
       "      <td></td>\n",
       "      <td>1210041502.0</td>\n",
       "      <td>mobile</td>\n",
       "      <td>15</td>\n",
       "      <td>Search</td>\n",
       "      <td>2023-06-13 14:12:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-13 14:18:35</td>\n",
       "      <td>2023-06-14 18:09:40</td>\n",
       "      <td>kinder</td>\n",
       "      <td></td>\n",
       "      <td>domain_discarded</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>other</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>273726366948</td>\n",
       "      <td>mingle.respondi.de/</td>\n",
       "      <td>2023-06-13 20:09:47</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>1210041502.0</td>\n",
       "      <td>mobile</td>\n",
       "      <td>15</td>\n",
       "      <td>Search</td>\n",
       "      <td>2023-06-13 14:12:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-13 14:18:35</td>\n",
       "      <td>2023-06-14 18:09:40</td>\n",
       "      <td>kinder</td>\n",
       "      <td></td>\n",
       "      <td>domain_discarded</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>other</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>273746614716</td>\n",
       "      <td>mingle.respondi.de/</td>\n",
       "      <td>2023-06-14 16:19:10</td>\n",
       "      <td>492</td>\n",
       "      <td></td>\n",
       "      <td>1210041502.0</td>\n",
       "      <td>mobile</td>\n",
       "      <td>15</td>\n",
       "      <td>Control</td>\n",
       "      <td>2023-06-14 16:19:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-14 16:26:33</td>\n",
       "      <td>2023-06-15 15:34:00</td>\n",
       "      <td>kinder</td>\n",
       "      <td></td>\n",
       "      <td>domain_discarded</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>other</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>273746614716</td>\n",
       "      <td>mingle.respondi.de/</td>\n",
       "      <td>2023-06-14 16:28:00</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1210041502.0</td>\n",
       "      <td>mobile</td>\n",
       "      <td>15</td>\n",
       "      <td>Control</td>\n",
       "      <td>2023-06-14 16:19:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-14 16:26:33</td>\n",
       "      <td>2023-06-15 15:34:00</td>\n",
       "      <td>kinder</td>\n",
       "      <td></td>\n",
       "      <td>domain_discarded</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>other</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>273746614716</td>\n",
       "      <td>mingle.respondi.de/</td>\n",
       "      <td>2023-06-14 16:36:39</td>\n",
       "      <td>13</td>\n",
       "      <td></td>\n",
       "      <td>1210041502.0</td>\n",
       "      <td>mobile</td>\n",
       "      <td>15</td>\n",
       "      <td>Control</td>\n",
       "      <td>2023-06-14 16:19:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-14 16:26:33</td>\n",
       "      <td>2023-06-15 15:34:00</td>\n",
       "      <td>kinder</td>\n",
       "      <td></td>\n",
       "      <td>domain_discarded</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>other</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           p_id                  url             used_at  duration  \\\n",
       "0  273726366948  mingle.respondi.de/ 2023-06-13 14:12:16       393   \n",
       "1  273726366948  mingle.respondi.de/ 2023-06-13 20:09:47         2   \n",
       "2  273746614716  mingle.respondi.de/ 2023-06-14 16:19:10       492   \n",
       "3  273746614716  mingle.respondi.de/ 2023-06-14 16:28:00         1   \n",
       "4  273746614716  mingle.respondi.de/ 2023-06-14 16:36:39        13   \n",
       "\n",
       "  yt_video_id package_version enddevice  batch    Group           start_date  \\\n",
       "0                1210041502.0    mobile     15   Search  2023-06-13 14:12:00   \n",
       "1                1210041502.0    mobile     15   Search  2023-06-13 14:12:00   \n",
       "2                1210041502.0    mobile     15  Control  2023-06-14 16:19:00   \n",
       "3                1210041502.0    mobile     15  Control  2023-06-14 16:19:00   \n",
       "4                1210041502.0    mobile     15  Control  2023-06-14 16:19:00   \n",
       "\n",
       "   ...   start_intervention      start_knowledge   topic series  \\\n",
       "0  ...  2023-06-13 14:18:35  2023-06-14 18:09:40  kinder          \n",
       "1  ...  2023-06-13 14:18:35  2023-06-14 18:09:40  kinder          \n",
       "2  ...  2023-06-14 16:26:33  2023-06-15 15:34:00  kinder          \n",
       "3  ...  2023-06-14 16:26:33  2023-06-15 15:34:00  kinder          \n",
       "4  ...  2023-06-14 16:26:33  2023-06-15 15:34:00  kinder          \n",
       "\n",
       "    annotation_type good_for_training good_for_augmentation category  \\\n",
       "0  domain_discarded             False                  True    other   \n",
       "1  domain_discarded             False                  True    other   \n",
       "2  domain_discarded             False                  True    other   \n",
       "3  domain_discarded             False                  True    other   \n",
       "4  domain_discarded             False                  True    other   \n",
       "\n",
       "  is_direct_topic_annotated is_direct_topic_full  \n",
       "0                                          False  \n",
       "1                                          False  \n",
       "2                                          False  \n",
       "3                                          False  \n",
       "4                                          False  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels = pd.read_json('../../data/raw/pages_with_labels.json', orient='records', lines=True)\n",
    "df_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels[\"is_topic\"] = df_labels[\"is_direct_topic_full\"].map({'True': True, 'False': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['p_id', 'url', 'used_at', 'duration', 'yt_video_id', 'package_version',\n",
       "       'enddevice', 'batch', 'Group', 'start_date', 't', 'wave', 'end_date',\n",
       "       'start_intervention', 'start_knowledge', 'topic', 'series',\n",
       "       'annotation_type', 'good_for_training', 'good_for_augmentation',\n",
       "       'category', 'is_direct_topic_annotated', 'is_direct_topic_full',\n",
       "       'is_topic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages:  761232\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pages: \", len(df_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels_topic = df_labels[df_labels.topic == TOPIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages of this topic:  234120\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of pages of this topic: \", len(df_labels_topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove Duplicates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages with labels:  234120\n",
      "Number of pages with labels after removing duplicates:  88922\n"
     ]
    }
   ],
   "source": [
    "# Removing duplicates based on 'view_url', 'batch_id', and 'topic'\n",
    "print(\"Number of pages with labels: \", len(df_labels_topic))\n",
    "df_labels_topic = df_labels_topic.drop_duplicates(subset=['url', 'batch', 'topic'], keep='first')\n",
    "print(\"Number of pages with labels after removing duplicates: \", len(df_labels_topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to Huggingface Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "www.example.com\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def extract_domain(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the domain from a given URL. Prepends 'http://' if no scheme is found to ensure correct parsing. Returns an empty string if the URL is invalid or empty.\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    \n",
    "    if not urlparse(url).scheme:\n",
    "        url = \"http://\" + url\n",
    "    \n",
    "    return urlparse(url).netloc.split('/')[0]\n",
    "\n",
    "print(extract_domain(\"www.example.com/path/to/resource\"))  # Output: www.example.com\n",
    "print(extract_domain(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = {\n",
    "    '_id': df_labels_topic['p_id'].apply(lambda x: 'dummy_id_' + str(x)),\n",
    "    'batch_id': df_labels_topic['batch'],  \n",
    "    'domain': df_labels_topic['url'].apply(extract_domain),  # Extract domain from URL\n",
    "    'view_url': df_labels_topic['url'], \n",
    "    'lang': 'na', \n",
    "    'text': \"\",\n",
    "    'text_length': 0,\n",
    "    'word_count': 0,\n",
    "    'topic': df_labels_topic['topic'],  \n",
    "    'category': df_labels_topic['category'],\n",
    "    'good_for_training': df_labels_topic['good_for_training'],\n",
    "    'good_for_augmentation': df_labels_topic['good_for_augmentation'],\n",
    "    'annotation_type': df_labels_topic['annotation_type'],\n",
    "    'is_topic': df_labels_topic['is_topic'],\n",
    "    'token_count': 0,\n",
    "    'chunk_id': 0,\n",
    "    #'label': df_labels_topic['label'],\n",
    "}\n",
    "\n",
    "df_urls = pd.DataFrame(transformed_data)\n",
    "dataset_urls = Dataset.from_pandas(df_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/88922 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 88922/88922 [00:16<00:00, 5531.18 examples/s]\n",
      "Casting the dataset: 100%|██████████| 88922/88922 [00:00<00:00, 1482812.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Convert boolean labels to integers (True to 1, False to 0)\n",
    "dataset_urls = dataset_urls.map(lambda example: {'label': int(example['is_topic'])})\n",
    "\n",
    "# Define a ClassLabel feature for the converted integer labels\n",
    "class_label_feature = ClassLabel(num_classes=2, names=['False', 'True'])\n",
    "\n",
    "# Update the features of the dataset\n",
    "new_features = dataset_urls.features.copy()\n",
    "new_features['label'] = class_label_feature\n",
    "\n",
    "# Cast the dataset to the new features\n",
    "dataset = dataset_urls.cast(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'token_count', 'chunk_id', '__index_level_0__', 'label'],\n",
       "    num_rows: 88922\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Chunkified Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\n",
    "    f\"../../data_ccu/tmp/processed_dataset_{TOPIC}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}/processed_dataset_{TOPIC}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 3815\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 507\n",
       "    })\n",
       "    holdout: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 33702\n",
       "    })\n",
       "    extended: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 224737\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': Value(dtype='string', id=None),\n",
       " 'batch_id': Value(dtype='int64', id=None),\n",
       " 'domain': Value(dtype='string', id=None),\n",
       " 'view_url': Value(dtype='string', id=None),\n",
       " 'lang': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'text_length': Value(dtype='int64', id=None),\n",
       " 'word_count': Value(dtype='int64', id=None),\n",
       " 'topic': Value(dtype='string', id=None),\n",
       " 'category': Value(dtype='string', id=None),\n",
       " 'good_for_training': Value(dtype='string', id=None),\n",
       " 'good_for_augmentation': Value(dtype='string', id=None),\n",
       " 'annotation_type': Value(dtype='string', id=None),\n",
       " 'is_topic': Value(dtype='bool', id=None),\n",
       " 'label': Value(dtype='int64', id=None),\n",
       " 'token_count': Value(dtype='int64', id=None),\n",
       " 'chunk_id': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': Value(dtype='string', id=None),\n",
       " 'batch_id': Value(dtype='int64', id=None),\n",
       " 'domain': Value(dtype='string', id=None),\n",
       " 'view_url': Value(dtype='string', id=None),\n",
       " 'lang': Value(dtype='string', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'text_length': Value(dtype='int64', id=None),\n",
       " 'word_count': Value(dtype='int64', id=None),\n",
       " 'topic': Value(dtype='string', id=None),\n",
       " 'category': Value(dtype='string', id=None),\n",
       " 'good_for_training': Value(dtype='string', id=None),\n",
       " 'good_for_augmentation': Value(dtype='string', id=None),\n",
       " 'annotation_type': Value(dtype='string', id=None),\n",
       " 'is_topic': Value(dtype='bool', id=None),\n",
       " 'token_count': Value(dtype='int64', id=None),\n",
       " 'chunk_id': Value(dtype='int64', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None),\n",
       " 'label': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_urls.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_columns(dataset: Dataset, columns_to_keep: list) -> Dataset:\n",
    "    \"\"\"Returns a new dataset containing only the specified columns.\"\"\"\n",
    "    # Directly compute columns to remove and apply removal\n",
    "    return dataset.remove_columns([col for col in dataset.column_names if col not in columns_to_keep])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = set(dataset[\"train\"].features.keys())\n",
    "dataset_urls = dataset_urls.select_columns(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset:   0%|          | 0/88922 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 88922/88922 [00:00<00:00, 820731.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Access the schema (features) of the existing dataset\n",
    "schema = dataset[\"train\"].features\n",
    "dataset_urls = dataset_urls.cast(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "    num_rows: 88922\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add URLs as Chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine URLs to add:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique view_urls: 48147\n",
      "Some examples of unique view_urls: ['www.quoka.de/wellness-gesundheit/massage/duesseldorf/sc_15_ct_119205_page_2.html', 'https://www.oberpfalzecho.de/beitrag/cannabislegalisierung-no-go-fuer-polizei-im-landkreis-new', 'https://www1.wdr.de/nachrichten/cannabis-freigabe-lauterbach-legalisierung-100.html']\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the view_url columns from all splits\n",
    "view_urls = pd.concat([\n",
    "    pd.Series(dataset['train']['view_url']),\n",
    "    pd.Series(dataset['test']['view_url']),\n",
    "    pd.Series(dataset['holdout']['view_url']),\n",
    "    pd.Series(dataset['extended']['view_url'])\n",
    "])\n",
    "\n",
    "# Get unique view_urls\n",
    "unique_view_urls = view_urls.unique()\n",
    "unique_view_urls_list = unique_view_urls.tolist()\n",
    "\n",
    "print(f\"Number of unique view_urls: {len(unique_view_urls_list)}\")\n",
    "print(\"Some examples of unique view_urls:\", unique_view_urls_list[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 88922/88922 [00:56<00:00, 1568.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages after filtering out pages already in the dataset:  40518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_urls = dataset_urls.filter(lambda example: example['view_url'] not in unique_view_urls_list)\n",
    "print(\"Number of pages after filtering out pages already in the dataset: \", len(dataset_urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extend Dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 3815\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 507\n",
       "    })\n",
       "    holdout: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 33702\n",
       "    })\n",
       "    extended: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 224737\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/40518 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 40518/40518 [00:01<00:00, 20485.55 examples/s]\n",
      "Filter: 100%|██████████| 40518/40518 [00:02<00:00, 19592.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_urls_holdout = dataset_urls.filter(lambda example: example['good_for_training'] == \"True\")\n",
    "\n",
    "dataset_urls_extended = dataset_urls.filter(lambda example: example['good_for_training'] == \"False\")\n",
    "\n",
    "dataset[\"holdout_url\"] = dataset_urls_holdout\n",
    "dataset[\"extended_url\"] = dataset_urls_extended\n",
    "\n",
    "#dataset_combined_tmp = concatenate_datasets([dataset[\"train\"], dataset_urls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 3815\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 507\n",
       "    })\n",
       "    holdout: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 33702\n",
       "    })\n",
       "    extended: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 224737\n",
       "    })\n",
       "    holdout_url: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 1094\n",
       "    })\n",
       "    extended_url: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 39424\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save extended Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3815/3815 [00:00<00:00, 300837.95 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 507/507 [00:00<00:00, 48571.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33702/33702 [00:00<00:00, 517209.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 224737/224737 [00:00<00:00, 633894.66 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1094/1094 [00:00<00:00, 38358.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 39424/39424 [00:00<00:00, 57530.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(f\"../../data_ccu/tmp/processed_dataset_{TOPIC}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_with_urls\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
