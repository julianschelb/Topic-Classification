{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set a seed for numpy module\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set a seed for torch module\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "SPLITS = ['train', 'test', 'holdout', 'extended', 'holdout_url', 'extended_url']\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url_and_content\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPIC = \"cannabis\" # \"cannabis\", \"kinder\", \"energie\"\n",
    "MODEL = \"deepset/gbert-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_metrics_table(eval_results):\n",
    "    \"\"\"\n",
    "    Display evaluation metrics as an HTML table.\n",
    "\n",
    "    Parameters:\n",
    "    eval_results (dict): A dictionary where keys are split names (e.g., 'train', 'test')\n",
    "                         and values are dictionaries containing 'metrics' and 'count'.\n",
    "    \"\"\"\n",
    "    # Prepare headers for the table\n",
    "    headers = [\"Split\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"Count\"]\n",
    "\n",
    "    # Prepare rows\n",
    "    rows = []\n",
    "    for split, data in eval_results.items():\n",
    "        metrics = data.get(\"metrics\", {})\n",
    "        count = data.get(\"count\", 0)\n",
    "        row = [\n",
    "            split,\n",
    "            metrics.get('accuracy', 0.0),\n",
    "            metrics.get('precision', 0.0),\n",
    "            metrics.get('recall', 0.0),\n",
    "            metrics.get('f1', 0.0),\n",
    "            count\n",
    "        ]\n",
    "        rows.append(row)\n",
    "\n",
    "    # Generate the HTML table\n",
    "    table_html = tabulate(rows, headers=headers, tablefmt=\"html\", floatfmt=\".3f\")\n",
    "\n",
    "    # Display the HTML table\n",
    "    display(HTML(table_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy, precision, recall, and F1 score for the given labels and predictions and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary'),\n",
    "        'recall': recall_score(labels, preds, average='binary'),\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(f\"../../data_ccu/tmp/processed_dataset_{TOPIC}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_with_urls_{MODEL.split('/')[1]}_{FEATURES}_with_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
       "        num_rows: 3815\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
       "        num_rows: 507\n",
       "    })\n",
       "    holdout: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
       "        num_rows: 33702\n",
       "    })\n",
       "    extended: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
       "        num_rows: 224737\n",
       "    })\n",
       "    holdout_url: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
       "        num_rows: 1094\n",
       "    })\n",
       "    extended_url: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
       "        num_rows: 39424\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '64a09474749484eec86957cf',\n",
       " 'batch_id': 16,\n",
       " 'domain': 'cheezburger.com',\n",
       " 'view_url': 'amp.cheezburger.com/21165573/with-her-own-card-manager-makes-hotel-desk-worker-pay-for-a-guests-room-when-he-refuses-to',\n",
       " 'lang': 'en',\n",
       " 'text': \"Log In Cheezburger Search Submit FAIL Blog Channels FAIL Blog After 12 Autocowrecks Dating Fails FAIL Nation Failbook Monday Thru Friday Music Parenting Poorly Dressed School of Fail There, I Fixed It Ugliest Tattoos WIN! Cheezburger Channels I Can Has FAIL Blog Memebase Animal Comedy Geek Universe CheezCake Loquillo FAIL Blog search email community favorite this article chev-right latest posts article list comments tags video article login twitter facebook menu pinterest whatsapp 'With HER own card': Manager makes hotel desk worker pay for a guest's room when he refuses to Businesses and their owners take on more of the risk and, in return, take more of the reward, paying workers a small portion of the value of their production and taking the rest for providing the means. This is the basis of a capitalist system and the argument used by your boss for why he's not giving you a raise, despite the fact that he himself just bought another new Porsche and vacations in Monaco every summer. It's true; when you're working for a business instead of starting your own, there's a lot less capital risk involved (not that most of us have the capital to risk in the first place)—that is, until your manager suddenly asks you to put down your own personal card against a guest's room when they refuse to put their own card against the room. There's good customer service, there's keeping customers happy—and then there's this\",\n",
       " 'text_length': 1428,\n",
       " 'word_count': 241,\n",
       " 'topic': 'cannabis',\n",
       " 'category': 'other',\n",
       " 'good_for_training': 'True',\n",
       " 'good_for_augmentation': 'True',\n",
       " 'annotation_type': '04.urls-with-title',\n",
       " 'is_topic': False,\n",
       " 'label': 0,\n",
       " 'token_count': 331,\n",
       " 'chunk_id': 0,\n",
       " 'url_path': '21165573/with-her-own-card-manager-makes-hotel-desk-worker-pay-for-a-guests-room-when-he-refuses-to',\n",
       " 'preds': 0,\n",
       " 'probas': 0.0005884783458895981}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Chunk Level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for split train:\n",
      " {'accuracy': 0.9955439056356488, 'precision': 0.9925373134328358, 'recall': 0.99899849774662, 'f1': 0.9957574245071126}\n",
      "\n",
      "Metrics for deepset/gbert-large on cannabis for split test:\n",
      " {'accuracy': 0.9842209072978304, 'precision': 0.9763313609467456, 'recall': 1.0, 'f1': 0.9880239520958084}\n",
      "\n",
      "Metrics for deepset/gbert-large on cannabis for split holdout:\n",
      " {'accuracy': 0.9857871936383598, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for split extended:\n",
      " {'accuracy': 0.9970187374575615, 'precision': 0.2591362126245847, 'recall': 0.9957446808510638, 'f1': 0.4112478031634446}\n",
      "\n",
      "Metrics for deepset/gbert-large on cannabis for split holdout_url:\n",
      " {'accuracy': 0.9990859232175503, 'precision': 0.8, 'recall': 1.0, 'f1': 0.8888888888888888}\n",
      "\n",
      "Metrics for deepset/gbert-large on cannabis for split extended_url:\n",
      " {'accuracy': 0.999518060064935, 'precision': 0.9348837209302325, 'recall': 0.9757281553398058, 'f1': 0.9548693586698337}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries\n",
    "preds_and_labels_per_chunk = {}\n",
    "labels_per_chunk = []\n",
    "preds_per_chunk = []\n",
    "\n",
    "# Iterate over the splits\n",
    "for split in SPLITS:\n",
    "    \n",
    "    # Get the labels and predictions for the current split\n",
    "    labels = dataset[split][\"label\"]\n",
    "    preds = dataset[split][\"preds\"]\n",
    "    preds_and_labels_per_chunk[split] = {\"preds\": preds, \"labels\": labels, \"count\": len(labels)}\n",
    "    labels_per_chunk.extend(labels)\n",
    "    preds_per_chunk.extend(preds)\n",
    "\n",
    "    # Calculate the metrics for the current split\n",
    "    metrics = calc_metrics(labels, preds)\n",
    "    preds_and_labels_per_chunk[split][\"metrics\"] = metrics\n",
    "    print(f\"Metrics for {MODEL} on {TOPIC} for split {split}:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for all splits:\n",
      " {'accuracy': 0.9960630310704005, 'precision': 0.699746835443038, 'recall': 0.9971139971139971, 'f1': 0.82237429336507}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = calc_metrics(labels_per_chunk, preds_per_chunk)\n",
    "print(f\"Metrics for {MODEL} on {TOPIC} for all splits:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Split       </th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train       </td><td style=\"text-align: right;\">     0.996</td><td style=\"text-align: right;\">      0.993</td><td style=\"text-align: right;\">   0.999</td><td style=\"text-align: right;\">     0.996</td><td style=\"text-align: right;\">   3815</td></tr>\n",
       "<tr><td>test        </td><td style=\"text-align: right;\">     0.984</td><td style=\"text-align: right;\">      0.976</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.988</td><td style=\"text-align: right;\">    507</td></tr>\n",
       "<tr><td>holdout     </td><td style=\"text-align: right;\">     0.986</td><td style=\"text-align: right;\">      0.000</td><td style=\"text-align: right;\">   0.000</td><td style=\"text-align: right;\">     0.000</td><td style=\"text-align: right;\">  33702</td></tr>\n",
       "<tr><td>extended    </td><td style=\"text-align: right;\">     0.997</td><td style=\"text-align: right;\">      0.259</td><td style=\"text-align: right;\">   0.996</td><td style=\"text-align: right;\">     0.411</td><td style=\"text-align: right;\"> 224737</td></tr>\n",
       "<tr><td>holdout_url </td><td style=\"text-align: right;\">     0.999</td><td style=\"text-align: right;\">      0.800</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.889</td><td style=\"text-align: right;\">   1094</td></tr>\n",
       "<tr><td>extended_url</td><td style=\"text-align: right;\">     1.000</td><td style=\"text-align: right;\">      0.935</td><td style=\"text-align: right;\">   0.976</td><td style=\"text-align: right;\">     0.955</td><td style=\"text-align: right;\">  39424</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metrics_table(preds_and_labels_per_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for all splits:\n",
      " {'accuracy': 0.9960696444313841, 'precision': 0.39639175257731957, 'recall': 0.9922580645161291, 'f1': 0.5664825046040516}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the labels for all splits except train\n",
    "labels_per_chunk_except_train = [\n",
    "    label for split in SPLITS if split != 'train' for label in preds_and_labels_per_chunk[split].get(\"labels\", [])\n",
    "]\n",
    "\n",
    "# Concatenate the predictions for all splits except train\n",
    "preds_per_chunk_except_train = [\n",
    "    prediction for split in SPLITS if split != 'train' for prediction in preds_and_labels_per_chunk[split].get(\"preds\", [])\n",
    "]\n",
    "\n",
    "# Calculate the metrics for all splits except train\n",
    "metrics = calc_metrics(labels_per_chunk_except_train, preds_per_chunk_except_train)\n",
    "print(f\"Metrics for {MODEL} on {TOPIC} for all splits:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'holdout', 'extended', 'holdout_url', 'extended_url'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_and_labels_per_chunk.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'holdout', 'extended'])\n"
     ]
    }
   ],
   "source": [
    "preds_and_labels_per_chunk_merged = preds_and_labels_per_chunk.copy()\n",
    "\n",
    "#### Create unbalanced test set ####\n",
    "preds_and_labels_per_chunk_merged[\"holdout\"][\"labels\"] = preds_and_labels_per_chunk[\"holdout_url\"][\"labels\"] + preds_and_labels_per_chunk[\"holdout\"][\"labels\"] + preds_and_labels_per_chunk[\"test\"][\"labels\"]\n",
    "\n",
    "preds_and_labels_per_chunk_merged[\"holdout\"][\"preds\"] = preds_and_labels_per_chunk[\"holdout_url\"][\"preds\"] + preds_and_labels_per_chunk[\"holdout\"][\"preds\"] + preds_and_labels_per_chunk[\"test\"][\"preds\"]\n",
    "\n",
    "#### Create extended test set ####\n",
    "preds_and_labels_per_chunk_merged[\"extended\"][\"labels\"] = preds_and_labels_per_chunk[\"extended_url\"][\"labels\"] + preds_and_labels_per_chunk[\"extended\"][\"labels\"]\n",
    "\n",
    "preds_and_labels_per_chunk_merged[\"extended\"][\"preds\"] = preds_and_labels_per_chunk[\"extended_url\"][\"preds\"] + preds_and_labels_per_chunk[\"extended\"][\"preds\"]\n",
    "\n",
    "\n",
    "#### Remove unnecessary splits ####\n",
    "preds_and_labels_per_chunk_merged.pop(\"holdout_url\")\n",
    "preds_and_labels_per_chunk_merged.pop(\"extended_url\")\n",
    "preds_and_labels_per_chunk_merged.pop(\"train\")\n",
    "print(preds_and_labels_per_chunk_merged.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for split test:\n",
      " {'accuracy': 0.9842209072978304, 'precision': 0.9763313609467456, 'recall': 1.0, 'f1': 0.9880239520958084}\n",
      "\n",
      "Metrics for deepset/gbert-large on cannabis for split holdout:\n",
      " {'accuracy': 0.9861768121689375, 'precision': 0.40632603406326034, 'recall': 1.0, 'f1': 0.5778546712802768}\n",
      "\n",
      "Metrics for deepset/gbert-large on cannabis for split extended:\n",
      " {'accuracy': 0.997391742157245, 'precision': 0.389087656529517, 'recall': 0.9863945578231292, 'f1': 0.5580500320718409}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_per_chunk = []\n",
    "preds_per_chunk = []\n",
    "\n",
    "# Iterate over the splits\n",
    "for split in preds_and_labels_per_chunk_merged.keys():\n",
    "    \n",
    "    # Get the labels and predictions for the current split\n",
    "    labels = preds_and_labels_per_chunk_merged[split][\"labels\"]\n",
    "    preds = preds_and_labels_per_chunk_merged[split][\"preds\"]\n",
    "    labels_per_chunk.extend(labels)\n",
    "    preds_per_chunk.extend(preds)\n",
    "\n",
    "    # Calculate the metrics for the current split\n",
    "    metrics = calc_metrics(labels, preds)\n",
    "    preds_and_labels_per_chunk_merged[split][\"metrics\"] = metrics\n",
    "    preds_and_labels_per_chunk_merged[split][\"count\"] = len(labels)\n",
    "    print(f\"Metrics for {MODEL} on {TOPIC} for split {split}:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for all splits:\n",
      " {'accuracy': 0.9960496181297526, 'precision': 0.48244073748902544, 'recall': 0.9945701357466064, 'f1': 0.6497191841560744}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concatenate the labels for all splits \n",
    "labels_per_chunk_except_merged = [\n",
    "    label for split in preds_and_labels_per_chunk_merged.keys() if split != 'train' for label in preds_and_labels_per_chunk[split].get(\"labels\", [])\n",
    "]\n",
    "\n",
    "# Concatenate the predictions for all splits\n",
    "preds_per_chunk_except_merged = [\n",
    "    prediction for split in preds_and_labels_per_chunk_merged.keys() if split != 'train' for prediction in preds_and_labels_per_chunk[split].get(\"preds\", [])\n",
    "]\n",
    "\n",
    "# Calculate the metrics for all splits\n",
    "metrics = calc_metrics(labels_per_chunk_except_merged, preds_per_chunk_except_merged)\n",
    "print(f\"Metrics for {MODEL} on {TOPIC} for all splits:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Split   </th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>test    </td><td style=\"text-align: right;\">     0.984</td><td style=\"text-align: right;\">      0.976</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.988</td><td style=\"text-align: right;\">    507</td></tr>\n",
       "<tr><td>holdout </td><td style=\"text-align: right;\">     0.986</td><td style=\"text-align: right;\">      0.406</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.578</td><td style=\"text-align: right;\">  35303</td></tr>\n",
       "<tr><td>extended</td><td style=\"text-align: right;\">     0.997</td><td style=\"text-align: right;\">      0.389</td><td style=\"text-align: right;\">   0.986</td><td style=\"text-align: right;\">     0.558</td><td style=\"text-align: right;\"> 264161</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metrics_table(preds_and_labels_per_chunk_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify all topics (assuming all models are evaluated on the same topics)\n",
    "# topics = list(next(iter(eval_results.values())).keys())\n",
    "\n",
    "# # Prepare headers for the table: each topic will have four metrics\n",
    "# headers = [\"Model\"] + [f\"{topic} {metric}\" for topic in topics for metric in [\"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# # Prepare rows: one row per model, containing metrics for each topic\n",
    "# rows = []\n",
    "# for model, topics_metrics in eval_results.items():\n",
    "#     row = [model]  # Start with the model name\n",
    "#     for topic in topics:\n",
    "#         metrics = topics_metrics.get(topic, {})\n",
    "#         row.extend([metrics.get('accuracy',0.0), metrics.get('precision',0.0), metrics.get('recall',0.0), metrics.get('f1',0.0)])\n",
    "#     rows.append(row)\n",
    "\n",
    "# # Generate the HTML table\n",
    "# table_html = tabulate(rows, headers=headers, tablefmt=\"html\", showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, HTML\n",
    "# display(HTML(table_html))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Page Level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majority_voting(answers):\n",
    "    \"\"\"Apply majority voting to a list of arbitrary classification answers.\"\"\"\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common()  # Get all common answers sorted by frequency\n",
    "\n",
    "    if not most_common:\n",
    "        return 0 # Handle empty input scenario\n",
    "\n",
    "    # Check for ties at the highest count\n",
    "    max_votes = most_common[0][1]\n",
    "    tied_classes = [cls for cls, votes in most_common if votes == max_votes]\n",
    "\n",
    "    if len(tied_classes) > 1:\n",
    "        return max(tied_classes)  # Return the maximum class label in case of a tie\n",
    "    return tied_classes[0]  # Return the class with the most votes\n",
    "\n",
    "majority_voting([1, 1, 2, 2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3815/3815 [00:00<00:00, 6240.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis: {'accuracy': 0.9796954314720813, 'precision': 0.9593908629441624, 'recall': 1.0, 'f1': 0.9792746113989638}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:00<00:00, 6526.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis: {'accuracy': 0.9534883720930233, 'precision': 0.9090909090909091, 'recall': 1.0, 'f1': 0.9523809523809523}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33702/33702 [00:05<00:00, 6366.52it/s]\n",
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis: {'accuracy': 0.9514675966288869, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 224737/224737 [00:36<00:00, 6209.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis: {'accuracy': 0.9932232487745375, 'precision': 0.0856269113149847, 'recall': 0.9655172413793104, 'f1': 0.15730337078651685}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1094/1094 [00:00<00:00, 5688.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis: {'accuracy': 0.9990850869167429, 'precision': 0.8, 'recall': 1.0, 'f1': 0.8888888888888888}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39424/39424 [00:07<00:00, 5374.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis: {'accuracy': 0.9995144762732221, 'precision': 0.9342723004694836, 'recall': 0.9754901960784313, 'f1': 0.9544364508393285}\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries\n",
    "preds_and_labels_per_page = {}\n",
    "labels_per_page = []\n",
    "preds_per_page = []\n",
    "\n",
    "# Iterate over the splits\n",
    "for split in SPLITS:\n",
    "            \n",
    "    # Group dataset examples by URL, with a fallback to domain\n",
    "    grouped_dataset = {}\n",
    "    for example in tqdm(dataset[split]):\n",
    "        url = example.get(\"view_url\") or example.get(\"domain\")\n",
    "        example_filtered = {k: example[k] for k in [\"text\", \"domain\", \"preds\", \"label\", \"category\", \"annotation_type\", \"lang\"]}\n",
    "        grouped_dataset.setdefault(url, []).append(example_filtered)\n",
    "        \n",
    "    # Extract labels\n",
    "    labels = []\n",
    "    for url, chunks in grouped_dataset.items():\n",
    "        label = max([chunk[\"label\"] for chunk in chunks])\n",
    "        labels.append(label)\n",
    "        \n",
    "    # Merge chunk level predictions\n",
    "    predictions = []\n",
    "    for url, chunks in grouped_dataset.items():\n",
    "        preds = [chunk[\"preds\"] for chunk in chunks]\n",
    "        pred = majority_voting([pred for pred in preds if pred > 0]) if max(preds) > 0 else 0\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Store the predictions and labels for the current split\n",
    "    preds_and_labels_per_page[split] = {\"preds\": predictions, \"labels\": labels, \"count\": len(labels)}\n",
    "    labels_per_page.extend(labels)\n",
    "    preds_per_page.extend(predictions)\n",
    "\n",
    "    # Use the trained model to make predictions on the test set\n",
    "    metrics = calc_metrics(labels, predictions)\n",
    "    preds_and_labels_per_page[split][\"metrics\"] = metrics\n",
    "    print(f\"Metrics for {MODEL} on {TOPIC}: {metrics}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for all splits:\n",
      " {'accuracy': 0.9943761103504464, 'precision': 0.47261009667024706, 'recall': 0.9865470852017937, 'f1': 0.6390704429920117}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = calc_metrics(labels_per_page, preds_per_page)\n",
    "print(f\"Metrics for {MODEL} on {TOPIC} for all splits:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Split       </th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train       </td><td style=\"text-align: right;\">     0.980</td><td style=\"text-align: right;\">      0.959</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.979</td><td style=\"text-align: right;\">    394</td></tr>\n",
       "<tr><td>test        </td><td style=\"text-align: right;\">     0.953</td><td style=\"text-align: right;\">      0.909</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.952</td><td style=\"text-align: right;\">     43</td></tr>\n",
       "<tr><td>holdout     </td><td style=\"text-align: right;\">     0.951</td><td style=\"text-align: right;\">      0.000</td><td style=\"text-align: right;\">   0.000</td><td style=\"text-align: right;\">     0.000</td><td style=\"text-align: right;\">   3441</td></tr>\n",
       "<tr><td>extended    </td><td style=\"text-align: right;\">     0.993</td><td style=\"text-align: right;\">      0.086</td><td style=\"text-align: right;\">   0.966</td><td style=\"text-align: right;\">     0.157</td><td style=\"text-align: right;\">  44269</td></tr>\n",
       "<tr><td>holdout_url </td><td style=\"text-align: right;\">     0.999</td><td style=\"text-align: right;\">      0.800</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.889</td><td style=\"text-align: right;\">   1093</td></tr>\n",
       "<tr><td>extended_url</td><td style=\"text-align: right;\">     1.000</td><td style=\"text-align: right;\">      0.934</td><td style=\"text-align: right;\">   0.975</td><td style=\"text-align: right;\">     0.954</td><td style=\"text-align: right;\">  39133</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metrics_table(preds_and_labels_per_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'holdout', 'extended', 'holdout_url', 'extended_url'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_and_labels_per_page.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for all splits:\n",
      " {'accuracy': 0.9944418554427761, 'precision': 0.3419618528610354, 'recall': 0.9766536964980544, 'f1': 0.5065590312815338}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the labels for all splits except train\n",
    "labels_per_page_except_train = [\n",
    "    label for split in SPLITS if split != 'train' for label in preds_and_labels_per_page[split].get(\"labels\", [])\n",
    "]\n",
    "\n",
    "# Concatenate the predictions for all splits except train\n",
    "preds_per_page_except_train = [\n",
    "    prediction for split in SPLITS if split != 'train' for prediction in preds_and_labels_per_page[split].get(\"preds\", [])\n",
    "]\n",
    "\n",
    "# Calculate the metrics for all splits except train\n",
    "metrics = calc_metrics(labels_per_page_except_train, preds_per_page_except_train)\n",
    "print(f\"Metrics for {MODEL} on {TOPIC} for all splits:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['test', 'holdout', 'extended'])\n"
     ]
    }
   ],
   "source": [
    "preds_and_labels_per_page_merged = preds_and_labels_per_page.copy()\n",
    "\n",
    "#### Create unbalanced test set ####\n",
    "preds_and_labels_per_page_merged[\"holdout\"][\"labels\"] = preds_and_labels_per_page[\"holdout_url\"][\"labels\"] + preds_and_labels_per_page[\"holdout\"][\"labels\"] + preds_and_labels_per_page[\"test\"][\"labels\"]\n",
    "\n",
    "preds_and_labels_per_page_merged[\"holdout\"][\"preds\"] = preds_and_labels_per_page[\"holdout_url\"][\"preds\"] + preds_and_labels_per_page[\"holdout\"][\"preds\"] + preds_and_labels_per_page[\"test\"][\"preds\"]\n",
    "\n",
    "#### Create extended test set ####\n",
    "preds_and_labels_per_page_merged[\"extended\"][\"labels\"] = preds_and_labels_per_page[\"extended_url\"][\"labels\"] + preds_and_labels_per_page[\"extended\"][\"labels\"]\n",
    "\n",
    "preds_and_labels_per_page_merged[\"extended\"][\"preds\"] = preds_and_labels_per_page[\"extended_url\"][\"preds\"] + preds_and_labels_per_page[\"extended\"][\"preds\"]\n",
    "\n",
    "\n",
    "#### Remove unnecessary splits ####\n",
    "preds_and_labels_per_page_merged.pop(\"holdout_url\")\n",
    "preds_and_labels_per_page_merged.pop(\"extended_url\")\n",
    "preds_and_labels_per_page_merged.pop(\"train\")\n",
    "print(preds_and_labels_per_page_merged.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for split test:\n",
      " {'accuracy': 0.9534883720930233, 'precision': 0.9090909090909091, 'recall': 1.0, 'f1': 0.9523809523809523}\n",
      "\n",
      "Metrics for deepset/gbert-large on cannabis for split holdout:\n",
      " {'accuracy': 0.9628577670963513, 'precision': 0.12371134020618557, 'recall': 1.0, 'f1': 0.22018348623853212}\n",
      "\n",
      "Metrics for deepset/gbert-large on cannabis for split extended:\n",
      " {'accuracy': 0.9961751516750198, 'precision': 0.4203703703703704, 'recall': 0.9742489270386266, 'f1': 0.5873221216041398}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_per_page = []\n",
    "preds_per_page = []\n",
    "\n",
    "# Iterate over the splits\n",
    "for split in preds_and_labels_per_page_merged.keys():\n",
    "    \n",
    "    # Get the labels and predictions for the current split\n",
    "    labels = preds_and_labels_per_page_merged[split][\"labels\"]\n",
    "    preds = preds_and_labels_per_page_merged[split][\"preds\"]\n",
    "    labels_per_page.extend(labels)\n",
    "    preds_per_page.extend(preds)\n",
    "\n",
    "    # Calculate the metrics for the current split\n",
    "    metrics = calc_metrics(labels, preds)\n",
    "    preds_and_labels_per_page_merged[split][\"metrics\"] = metrics\n",
    "    preds_and_labels_per_page_merged[split][\"count\"] = len(labels)\n",
    "    print(f\"Metrics for {MODEL} on {TOPIC} for split {split}:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gbert-large on cannabis for all splits:\n",
      " {'accuracy': 0.9944218490831838, 'precision': 0.3584656084656085, 'recall': 0.9783393501805054, 'f1': 0.5246853823814134}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concatenate the labels for all splits \n",
    "labels_per_page_merged = [\n",
    "    label for split in preds_and_labels_per_page_merged.keys() if split != 'train' for label in preds_and_labels_per_page[split].get(\"labels\", [])\n",
    "]\n",
    "\n",
    "# Concatenate the predictions for all splits\n",
    "preds_per_page_merged = [\n",
    "    prediction for split in preds_and_labels_per_page_merged.keys() if split != 'train' for prediction in preds_and_labels_per_page[split].get(\"preds\", [])\n",
    "]\n",
    "\n",
    "# Calculate the metrics for all splits\n",
    "metrics = calc_metrics(labels_per_page_merged, preds_per_page_merged)\n",
    "print(f\"Metrics for {MODEL} on {TOPIC} for all splits:\\n {metrics}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Split   </th><th style=\"text-align: right;\">  Accuracy</th><th style=\"text-align: right;\">  Precision</th><th style=\"text-align: right;\">  Recall</th><th style=\"text-align: right;\">  F1 Score</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>test    </td><td style=\"text-align: right;\">     0.953</td><td style=\"text-align: right;\">      0.909</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.952</td><td style=\"text-align: right;\">     43</td></tr>\n",
       "<tr><td>holdout </td><td style=\"text-align: right;\">     0.963</td><td style=\"text-align: right;\">      0.124</td><td style=\"text-align: right;\">   1.000</td><td style=\"text-align: right;\">     0.220</td><td style=\"text-align: right;\">   4577</td></tr>\n",
       "<tr><td>extended</td><td style=\"text-align: right;\">     0.996</td><td style=\"text-align: right;\">      0.420</td><td style=\"text-align: right;\">   0.974</td><td style=\"text-align: right;\">     0.587</td><td style=\"text-align: right;\">  83402</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_metrics_table(preds_and_labels_per_page_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify all topics (assuming all models are evaluated on the same topics)\n",
    "# topics = list(next(iter(eval_results_pages.values())).keys())\n",
    "\n",
    "# # Prepare headers for the table: each topic will have four metrics\n",
    "# headers = [\"Model\"] + [f\"{topic} {metric}\" for topic in topics for metric in [\"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# # Prepare rows: one row per model, containing metrics for each topic\n",
    "# rows = []\n",
    "# for model, topics_metrics in eval_results_pages.items():\n",
    "#     row = [model]  # Start with the model name\n",
    "#     for topic in topics:\n",
    "#         metrics = topics_metrics.get(topic, {})\n",
    "#         row.extend([metrics.get('accuracy',0.0), metrics.get('precision',0.0), metrics.get('recall',0.0), metrics.get('f1',0.0)])\n",
    "#     rows.append(row)\n",
    "\n",
    "# # Generate the HTML table\n",
    "# table_html = tabulate(rows, headers=headers, tablefmt=\"html\", showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(HTML(table_html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
