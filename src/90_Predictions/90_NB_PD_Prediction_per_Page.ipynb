{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify Webpages: Iterate over Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.database import *\n",
    "from utils.files import *\n",
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import BertTokenizer\n",
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.model import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "CONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n",
    "DATABASE_NAME = os.getenv(\"DATABASE_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connect to database:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs, db = getConnection(CONNECTION_STRING, DATABASE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics and Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {\n",
    "\t'kinder': ['kinder', 'kindergr', 'paus', 'familie', 'bundestag.de', 'arbeitsagentur.de', 'kindergrundsicherung',  'kindergeld', 'kindersicherung', 'kinderzuschlag', 'gesetz'],\n",
    "\t'energie': ['energie', 'eeg','grün','gruen','habeck', 'climate', 'strom','Waerme','wende','frderung', 'förderung', 'windkraft', 'windrad', 'photovoltaik',\n",
    "            \t'photovoltaic', 'solar', 'heizung', 'heiz', 'gesetz', 'erneuer', 'geothermie', 'pv', 'geg'],\n",
    "\t'cannabis': ['cannabis', 'canabis', 'cannabic', 'gras', 'cbd' , 'droge', 'hanf', 'thc', 'canbe', 'legal', 'legalisierung', 'gesetz', 'verein', 'entkriminali']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchPages(\n",
    "    db,\n",
    "    limit: int = 0,\n",
    "    skip: int = 0,\n",
    "    query={},\n",
    "    fields: dict = {},\n",
    "    collection = 'pages.content.extracted'\n",
    "):\n",
    "    tasks = db[collection].find(\n",
    "        query, fields).limit(limit).skip(skip)\n",
    "    return list(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [15, 16, 17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {\"_id\": 1, \"batch_id\": 1, \"view_url\": 1, \"lang\": 1, \"domain\": 1, \"text\": 1}\n",
    "query = {\"batch_id\": { \"$in\": batches }, \"classes\": {\"$exists\": False}, \n",
    "        \"text\": {\"$exists\": True}, \"word_count\": {\"$lt\": 100_000}, \"lang\": \"de\"}\n",
    "pages = fetchPages(db, limit = 10, skip = 0, query = query, fields = fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example page:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('648c2ad88e8cadbd29004e04'), 'batch_id': 15, 'domain': 'insektenstop.net', 'lang': 'de', 'text': ' Um Insektenstop in vollem Umfang nutzen zu können, empfehlen wir Ihnen Javascript in Ihrem Browser zu aktiveren. 4,6 / 5 Menü Suchen Suchen Infocenter Warenkorb Fenster Dachfenster Türen Maßanfertigung Lichtschachtabdeckungen Gewebe Zubehör Zur Kategorie Fenster Spannrahmen Klettband Set Dachfenster Rollladen mit Insektenschutz Rollosystem Fertig montiert/Zuschnitt Zur Kategorie Dachfenster Zur Kategorie Türen Drehtüren Schiebetüren Vorhang Rollladen mit Insektenschutz Fertig montiert/Zuschnitt Zur Kategorie Maßanfertigung Zur Kategorie Lichtschachtabdeckungen Zur Kategorie Gewebe Zur Kategorie Zubehör Ersatzteile mobiler Insektenschutz Fenster Spannrahmen Fliegengitter Fenster \"START\", Zuschnitt - Alurahmen Zurück Vor Menü schließen Kategorien Fenster Spannrahmen Klettband Set Dachfenster Rollladen mit Insektenschutz Rollosystem Fertig montiert/Zuschnitt Dachfenster Türen Maßanfertigung Lichtschachtabdeckungen Gewebe Zubehör Infocenter Blog Fliegengitter Fenster \"START\", Zuschnitt - Alurahmen ( 529 ) Zuschnitt nach deinem Wunschmaß - ganz bequem über unseren Produktkonfigurator! 13,95\\xa0€ * inkl. MwSt. zzgl. Versand Lieferzeit ca. 1 - 3 Werktage Farbe: weiß braun anthrazit Groesse: 60 x 80 80 x 100 100 x 120 120 x 140 130 x 150 Zuschnitt/Aufbau: Auf Maß geschnitten Selbstbausatz 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 In den Warenkorb Merken Bewerten Für \" Auf Maß geschnitten \" und \" komplett aufgebaut \" beachte bitte unsere Hinweise in der Artikelbeschreibung. Artikel-Nr.: 76207 EAN: 4061749341065 Im Lager: > 10 Stück verfügbar Produktdetails: Produktbeschreibung Bewertungen Fragen zum Produkt: Eine Frage zum Produkt stellen Wie kann ich bezahlen? Kurz & knapp: günstiges Einstiegsmodell schnell montiert & eingesetzt abgerundetes Profil geringe Einbautiefe von nur 10 mm Aluminium Profile schwarzes Gewebe Anleitungen: Fliegengitter_Fenster_Spannrahmen_START_V.pdf Videos: Video ansehen Produktinformationen \"Fliegengitter Fenster \"START\", Zuschnitt - Alurahmen\" Günstig in der Anschaffung, gute Sicht nach draußen und hält die lästigen Insekten fern. Was will man mehr? Das Fliegengitter Fenster \"START\" überzeugt mit unschlagbarem Preis-Leistungs-Verhältnis , einfachem Anbringen mittels Steckverbindern - ohne Bohren oder Schrauben - und dem zuverlässigen Schutz vor Fliegen und anderen Insekten. Dank der geringen Einbautiefe kann das Fliegengitterfenster mit stabilem Alu-Rahmen problemlos zwischen Fenster und Rollladen angebracht werden. Du kannst dich für einen einfachen Selbstbausatz entscheiden, oder dein Fliegengitter nach Wunschmaß von uns zugeschneiden lassen. Highlights top Preis-Leistungs-Verhältnis schnell montiert und eingesetzt stabile Alu-Profile gute Durchsicht durch schwarzes Gewebe geringe Einbautiefe ideal für Mietwohnungen passt zwischen Fenster & Rollo Zuschnitt-Service wählbar Produktdetails Montage ohne Bohren schwarzes Filatec® Gewebe Einbautiefe: 10 mm abgerundetes Profil passt zwischen Fenster und Rollo besonders für Mietswohnungen geeignet passend für eine Blendrahmenstärke von 5 - 35 mm Profile: Aluminium Auflagefläche: 20 mm Eckverbinder: Kunststoff Wählbare Größen & Farben Der Bausatz ist für ein maximales lichtes Maß (gemessen zwischen den inneren Rahmenkanten) geeignet: 60 x 80 cm = 56 x 76 cm 80 x 100 cm = 76 x 96 cm 100 x 120 cm = 96 x 116 cm 120 x 140 cm = 116 x 136 cm 130 x 150 cm = 126 x 146 cm Signalweiß (RAL 9003) (Eckverbinder weiß) Schokoladenbraun (RAL 8017) (Eckverbinder schwarz) Anthrazitgrau (RAL 7016) (Eckverbinder schwarz) Komplett vormontiert! schnell & komplett aufgebaut zu dir nach Hause Entdecke unser \"MASTER READY\" Premium Fliegengitter Fenster! zum Produkt Entdecke unsere Fliegengitter Spannrahmen! Start Empfehlung! Master Slim Basic Flex Einbautiefe 10 mm 11 mm 11 mm 3 mm Auflagefläche 20 mm 12 mm 12 mm 15 mm Montage geschraubt / gesteckt gesteckt geschraubt gesteckt gesteckt Umlaufende Bürstendichtung ✗ √ ✗ √ Besonderheit günstiges Einstiegsmodell hohe Stabilität einfache Montage für flächenversetzte Fenster zum Produkt zum Produkt zum Produkt zum Produkt Du benötigst ein Fliegengitter für besonders große Fenster? Dann entdecke unseren Fenster Spannrahmen \"MASTER SLIM XL\" - auch horizontal montierbar! zum Produkt Bitte beachten Der Insektenschutzrahmen benötigt eine umlaufende Auflagefläche von ca. 20 mm am Fensterrahmen, damit dieser einen sicheren Halt hat. Bei abgeschrägten Rahmenkanten am Fenster bzw. der Türe weicht das Abzugsmaß ab! Für Fenster mit Wetterschenkel / Wasserablaufschiene nur bedingt geeignet. Das Gewebe und die Kederleisten werden bei der Variante „Auf Maß geschnitten“ nicht zugeschnitten geliefert und erst im Laufe der Montage gekürzt. Diese können ganz einfach mit einem handelsüblichen Seitenschneider, Cuttermesser oder Schere abgeschnitten werden. Unsere Erfahrung hat gezeigt, dass bei einer vorab gekürzten Kederleiste Missverständnisse während des Zusammenbaus auftreten können. Tipps Das Fliegenschutzgitter bitte gemäß Anleitung montieren Das Filatec® Gewebe beim Einlegen in den Fliegengitterrahmen bitte nicht spannen. Ansonsten könnte sich der Rahmen verziehen. Hier geht es zu unserer Anleitung Wie messe ich richtig? Zugeschnittene oder vormontierte Produkte sind vom gesetzlichen Widerrufsrecht ausgeschlossen. Video für \"Fliegengitter Fenster \"START\", Zuschnitt - Alurahmen\" Kundenbewertungen \"Fliegengitter Fenster \"START\", Zuschnitt - Alurahmen\" 4.8 /5 529 Bewertungen 5 Sterne 430 4 Sterne 88 3 Sterne 9 2 Sterne 2 1 Stern 0 Auswahl zurücksetzen Bewertung schreiben Schreibe eine Bewertung und hilf dadurch anderen Kunden. Das wäre toll! Bitte beachte, dass es einige Zeit dauern kann, bis Deine Bewertung angezeigt wird. bitte Sterne vergeben Bitte gebe die Zahlenfolge in das nachfolgende Textfeld ein. Die mit einem * markierten Felder sind Pflichtfelder. Ich habe die Datenschutzbestimmungen zur Kenntnis genommen. Bewertung senden Von: Helmut R. Am: 16.06.2023 Passgenau und einfacher Zusammenbau Qualität und Verarbeitung gut Preis- Leistungsverhältnis gut. verifizierter Kauf Von: Shopkunde Am: 15.06.2023 schnelle Lieferung, gutes Preis-Leistungsverhältnis schnelle Lieferung, gutes Preis-Leistungsverhältnis verifizierter Kauf Von: Michael J. Am: 14.06.2023 Gutes Produkt - günstiger Preis - einfach Gutes Produkt - günstiger Preis - einfach perfekt :-) Jederzeit wieder! verifizierter Kauf Von: René F. Am: 14.06.2023 Sehr gutes Produkt,gut zu bearbeiten. Sehr gutes Produkt,gut zu bearbeiten. verifizierter Kauf Von: Walter H. Am: 13.06.2023 War alles OK, werde diesen Artikel wieder War alles OK, werde diesen Artikel wieder Kaufen. verifizierter Kauf 524 weitere anzeigen Überprüfung von Bewertungen im Shop Eine Überprüfung der Bewertungen hat wie folgt stattgefunden: Kunden erhalten einen Link zum Bewerten der Produkte. Es handelt sich dabei um einen individualisierten Link, den nur Verbraucher erhalten, die die Waren oder Dienstleistungen tatsächlich bei uns erworben haben. Diese Bewertungen werden dementsprechend als \"verifiziert\" gekennzeichnet. Wir prüfen Bewertungen zu unseren Produkten vor der Veröffentlichung. Jede Bewertung wird individuell darauf geprüft, ob diese ein Verbraucher vorgenommen hat, der die Waren oder Dienstleistungen tatsächlich bei uns erworben hat. Kunden kauften auch Fliegengittertür \"START\", Alurahmen - Zuschnitt ab 29,95\\xa0€ * TOPSELLER Fliegengittertür \"MASTER SLIM PLUS\", Aufgebaut - Zuschnitt - Alurahmen ab 41,95\\xa0€ * Insektenschutz Dachfenster Plissee \"MASTER\", Zuschnitt - Alurahmen ab 34,95\\xa0€ * Fliegengitter Fenster \"MASTER SLIM XL\", Aufgebaut - Zuschnitt - 130 x 220 cm ab 59,95\\xa0€ * Fliegengitter Fenster \"BASIC\", Zuschnitt - Alurahmen ab 13,95\\xa0€ * Zuletzt angesehen SCHNELLER VERSAND INDIVIDUELL & NACH MASS ÜBER 10 JAHRE ERFAHRUNG TOP PREIS-LEISTUNG BEI HOHER QUALITÄT BLOG | FLIEGENGITTER-FENSTER | FLIEGENGITTER-TÜREN | INSEKTENSCHUTZROLLOS EMPASA GMBH Telefon: 09805-8219999 Mo. - Do. 09:00 - 17:00 Uhr Fr. 09:00 - 12:30 Uhr Im Herrmannshof 10 91595 Burgoberbach Deutschland ZAHLUNGSARTEN VERSAND CO2 KLIMANEUTRAL NEWSLETTER ANMELDUNG Deine Vorteile: 5 € Willkommens-Gutschein**** Aktuelle Angebote Rabatte & Sonderaktionen Jetzt anmelden SERVICE Impressum Datenschutz Über uns Versandkosten Kontakt AGB Häufige Fragen / FAQ Bestellstatus Widerrufsrecht Rücksendung OS-Plattform Streitschlichtung Affiliate Programm Jobs Privatsphäre-Einstellungen * Alle Preise inkl. gesetzl. Mehrwertsteuer zzgl. Versandkosten , falls nicht anders beschrieben. ** Zahlungseingang vollständig; ausgenommen Freitag/Samstag/Sonntag/Feiertage, Maßanfertigungen (Zuschnitt/Fertig montiert) und Speditionsware *** auf alle Aluminiumprofile **** Der 5€-Gutschein ist ab einem Einkaufswert von 50€ gültig und nur einmal pro Kunde einlösbar. BLOG | FLIEGENGITTER-FENSTER | FLIEGENGITTER-TÜREN | INSEKTENSCHUTZROLLOS Copyright © 2023 Insektenstop.net - empasa GmbH Kundenbewertungen 4,6 / 5 ', 'view_url': 'insektenstop.net/Hausschutz/Insektenschutz/Fenster/Spannrahmen/1747/Fliegengitter-Fenster-START-Zuschnitt-Alurahmen'}\n"
     ]
    }
   ],
   "source": [
    "example_page = pages[0]\n",
    "print(example_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Pages into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepset/gbert-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitText(text, n_tokens, tokenizer, overlap=100):\n",
    "    \"\"\"\n",
    "    Splits the input text into chunks with n_tokens tokens using HuggingFace tokenizer, with an overlap of overlap tokens.\n",
    "    Each chunk includes the special tokens (e.g., [CLS], [SEP]) required by the tokenizer.\n",
    "    \"\"\"\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text, truncation=False)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Adjust the end index of the chunk to account for the special tokens\n",
    "        end_index = min(i + n_tokens - 4, len(tokens)) # -3 for [CLS] and [SEP]\n",
    "        \n",
    "        # Extract the chunk and add special tokens\n",
    "        chunk = tokens[i:end_index]\n",
    "        chunk_str = tokenizer.convert_tokens_to_string(chunk)\n",
    "        \n",
    "        chunks.append(chunk_str)\n",
    "        i += n_tokens - overlap - 4 # -3 for [CLS] and [SEP] in overlap\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2063 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Um Insektenstop in vollem Umfang nutzen zu können, empfehlen wir Ihnen Javasc', 'Umfang nutzen zu können, empfehlen wir Ihnen Javascript in Ihrem Browser zu aktiver']\n"
     ]
    }
   ],
   "source": [
    "example_page_chunks = splitText(example_page.get(\"text\", \"\"), n_tokens = 20, tokenizer = tokenizer, overlap=10)\n",
    "print(example_page_chunks[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  607, 14287,  3403, 30903,   153, 21830,  7661,  4964,   205,   618,\n",
      "          818, 10558,   268,  1609, 19204,  3753,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0])\n",
      "tensor([[  607, 14287,  3403,  ...,     0,     0,     0],\n",
      "        [ 7661,  4964,   205,  ...,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "def tokenizeInputs(examples, tokenizer, padding=\"max_length\", truncation=True, return_tensors=\"pt\"):\n",
    "    \"\"\"Tokenizes the input examples using the given tokenizer.\"\"\"\n",
    "    return tokenizer(examples, padding=padding, truncation=truncation, add_special_tokens=False, return_tensors=return_tensors)\n",
    "\n",
    "# Example of tokenizing the chunks\n",
    "example_page_chunks_tokenized = tokenizeInputs(example_page_chunks, tokenizer)\n",
    "print(example_page_chunks_tokenized.get(\"input_ids\")[0])\n",
    "print(example_page_chunks_tokenized.get(\"input_ids\")[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeClassProbas(model, examples, tokenizer, device=None):\n",
    "    \"\"\"Computes the classification for the input examples using the given model and tokenizer.\"\"\"\n",
    "    # Tokenize the input examples\n",
    "    inputs = tokenizeInputs(examples, tokenizer)\n",
    "    \n",
    "    # Move the inputs to the appropriate device\n",
    "    if device is not None:\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Compute the classification\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Compute the predicted class probabilities\n",
    "    #probas = outputs.softmax(dim=1)\n",
    "    probas = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../models/deepset_gbert-large_kinder_model\" # Path to the model e.g. \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9969, 0.0031],\n",
      "        [0.9880, 0.0120]])\n"
     ]
    }
   ],
   "source": [
    "# Example of computing the classifications\n",
    "example_page_chunks_classifications = computeClassProbas(model, example_page_chunks[:2], tokenizer)\n",
    "print(example_page_chunks_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Predicted classes\n",
    "example_page_chunks_classes = example_page_chunks_classifications.argmax(dim=1)\n",
    "print(example_page_chunks_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 1\n",
      "['Eltern mit Kindern sollen mehr Geld bekommen. Das fordert die Partei Die Linke. Kindergeld leistet einen wichtigen Beitrag zur Bekämpfung von Kinderarmut. Die Linke will das Kindergeld um 75 Euro monatlich erhöhen.']\n",
      "tensor([[8.1902e-05, 9.9992e-01]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "# Predict class for positive example\n",
    "example_page_kinder = \"\"\"\n",
    "Eltern mit Kindern sollen mehr Geld bekommen. Das fordert die Partei Die Linke. Kindergeld leistet einen wichtigen Beitrag zur Bekämpfung von Kinderarmut. Die Linke will das Kindergeld um 75 Euro monatlich erhöhen.\n",
    "\"\"\"\n",
    "\n",
    "# Split into chunks\n",
    "example_page_kinder_chunks = splitText(example_page_kinder, n_tokens=100, tokenizer=tokenizer, overlap=10)\n",
    "print(\"Number of chunks:\", len(example_page_kinder_chunks))\n",
    "print(example_page_kinder_chunks[:2])\n",
    "\n",
    "# Tokenize the input\n",
    "#example_page_kinder_tokenized = tokenizeInputs(example_page_kinder_chunks, tokenizer)\n",
    "\n",
    "# Compute the classification\n",
    "example_page_kinder_classification = computeClassProbas(model, example_page_kinder_chunks, tokenizer)\n",
    "print(example_page_kinder_classification)\n",
    "\n",
    "# Predicted class\n",
    "example_page_kinder_class = example_page_kinder_classification.argmax(dim=1)\n",
    "print(example_page_kinder_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Pages in Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePage(db, id: str, values: dict = {}, collection=\"pages.content.extracted\"):\n",
    "    \"\"\"Updates an article in the database.\"\"\"\n",
    "    # Ensure the _id is formatted correctly for MongoDB\n",
    "    filter = {\"_id\": ObjectId(id)}\n",
    "    # Prepare the update values\n",
    "    values = {\"$set\": {**values}}\n",
    "    # Execute the update operation\n",
    "    r = db[collection].update_one(filter, values)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def updatePages(db, pages, field_name, collection=\"pages.content.extracted\"):\n",
    "    \"\"\"Updates the articles in the database with prediction results.\"\"\"\n",
    "    print(\"Field name:\", field_name)\n",
    "    for page in tqdm(pages, desc=\"Uploading results\"):\n",
    "        id = page.get(\"_id\")  # Retrieve the article ID\n",
    "        # Prepare the values to be updated with prediction results\n",
    "        values = {\n",
    "            field_name: page.get(\"prediction\", {}),  # Includes chunks info, avg. class probas, overall predicted class\n",
    "            # Add any other fields you wish to update here\n",
    "        }\n",
    "        # Call updateArticle to update each article in the database\n",
    "        #updatePage(db, id, values, collection) # TODO: Uncomment this line to update the articles in the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Pages in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processBatch(\n",
    "    articles, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    chunk_size=512, \n",
    "    overlap=64, \n",
    "    show_progress=False,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a batch of articles, tokenizes them, computes class probabilities, and updates each article with a predicted class.\n",
    "    \"\"\"\n",
    "    \n",
    "    runtimes = []  # List to store processing times for each article\n",
    "    \n",
    "    # Iterate over articles with optional progress bar\n",
    "    for article in tqdm(articles, disable=not show_progress, desc=\"Processing articles\"):\n",
    "        start_time = time.time()  # Start timer for processing this article\n",
    "        \n",
    "        # Extract text from the article\n",
    "        article_text = article.get(\"text\", \"\")\n",
    "        \n",
    "        # Split the article text into chunks\n",
    "        chunks = splitText(article_text, n_tokens=chunk_size, tokenizer=tokenizer, overlap=overlap)\n",
    "        \n",
    "        # Compute the classification probabilities for the chunks\n",
    "        probas = computeClassProbas(model, chunks, tokenizer, device=device)\n",
    "        \n",
    "        # Determine the predicted class for each chunk\n",
    "        predicted_classes = probas.argmax(dim=1)\n",
    "        \n",
    "        # Prepare the chunk, probabilities, and classes for storage\n",
    "        chunks_info = [{\n",
    "            \"chunk\": chunk,\n",
    "            \"class_probas\": proba.tolist(),\n",
    "            \"predicted_class\": predicted_class.item()\n",
    "        } for chunk, proba, predicted_class in zip(chunks, probas, predicted_classes)]\n",
    "        \n",
    "        # Calculate the average class probabilities over all chunks\n",
    "        # Ensure 'probas' is moved to CPU and converted to a NumPy array if necessary\n",
    "        if torch.is_tensor(probas) and probas.is_cuda:\n",
    "            average_class_probas = np.mean(probas.cpu().numpy(), axis=0).tolist()\n",
    "        else:\n",
    "            # Handle the case where 'probas' is already a CPU tensor or a NumPy array\n",
    "            average_class_probas = np.mean(probas, axis=0).tolist()\n",
    "        \n",
    "        # Convert each probability to a Python float\n",
    "        average_class_probas = [float(proba) for proba in average_class_probas]\n",
    "\n",
    "        # Determine the overall predicted class based on the average class probabilities\n",
    "        overall_predicted_class = np.argmax(average_class_probas)\n",
    "        \n",
    "        # Convert to Python int\n",
    "        overall_predicted_class = int(overall_predicted_class)\n",
    "        \n",
    "        # Organize the prediction data\n",
    "        prediction = {\n",
    "            \"chunks\": chunks_info,\n",
    "            \"average_class_probas\": average_class_probas,\n",
    "            \"overall_predicted_class\": overall_predicted_class\n",
    "        }\n",
    "        \n",
    "        # Update the article with the chunks, their probabilities, and predicted classes\n",
    "        article[\"prediction\"] = prediction\n",
    "\n",
    "        end_time = time.time()  # End timer for processing this article\n",
    "        runtimes.append(end_time - start_time)  # Calculate and store processing time\n",
    "\n",
    "    return articles, runtimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"../models/bert_kinder_model_buff\" # Path to the model e.g. \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BertForSequenceClassification(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(31102, 1024, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 1024)\n",
       "        (token_type_embeddings): Embedding(2, 1024)\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-23): 24 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using device:', device)\n",
    "# print()\n",
    "\n",
    "# #Additional Info when using cuda\n",
    "# if device.type == 'cuda':\n",
    "#     print(torch.cuda.get_device_name(0))\n",
    "#     print('Memory Usage:')\n",
    "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#     print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "#     model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = 10 # Number of articles to process in each batch\n",
    "SKIP = 0 # Number of articles to skip in each batch\n",
    "CHUNK_SIZE = 512 # Number of tokens in each chunk\n",
    "OVERLAP = 64 # Number of overlapping tokens between chunks\n",
    "TOPIC = \"kinder\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = \"pages.content.extracted\"\n",
    "fields = {\"_id\": 1, \"batch_id\": 1, \"view_url\": 1, \"lang\": 1, \"domain\": 1, \"text\": 1}\n",
    "query = {\"batch_id\": { \"$in\": batches }, f\"prediction_{TOPIC}\": {\"$exists\": False}, \n",
    "        \"text\": {\"$exists\": True}, \"word_count\": {\"$lt\": 100_000}, \"lang\": \"de\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Batch 0 ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles:   0%|          | 0/10 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1714 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing articles: 100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Field name: prediction_kinder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading results: 100%|██████████| 10/10 [00:00<00:00, 195995.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average processing time: 1.14 seconds\n",
      "Standard deviation: 1.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_id = 0\n",
    "\n",
    "while True:\n",
    "    print(f\"------ Batch {batch_id} ------\")\n",
    "\n",
    "    # Fetch the next batch of articles\n",
    "    pages = fetchPages(db, limit = LIMIT, skip = SKIP, query = query, fields = fields)\n",
    "    \n",
    "    # Stop if no more articles are available\n",
    "    if not pages:\n",
    "        break\n",
    "    \n",
    "    # Process the batch of articles\n",
    "    pages, runtimes = processBatch(pages, model, tokenizer, chunk_size=CHUNK_SIZE, overlap=OVERLAP, show_progress=True, device=device)\n",
    "\n",
    "    # Update the articles in the database\n",
    "    updatePages(db, pages, field_name = f\"prediction_{TOPIC}\", collection=collection)\n",
    "    #print(f\"Updated {len(articles)} articles\", end=\"\\n\\n\")\n",
    "    \n",
    "    #print(runtimes)\n",
    "    \n",
    "    # Print average processing time for the batch\n",
    "    print(f\"Average processing time: {statistics.mean(runtimes):.2f} seconds\")\n",
    "    print(f\"Standard deviation: {statistics.stdev(runtimes):.2f} seconds\")\n",
    "\n",
    "    batch_id += 1\n",
    "    break # TODO: Remove to process all batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Runtime Napkin Math:**\n",
    ">21136872 pages times 0.08 seconds = 1.690.949\n",
    ">1.690.949 / 60 / 60 / 24 / 2 = 10 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Classification Aggregation Strategies\n",
    "\n",
    "For document classification tasks where documents are split into chunks and each chunk is classified separately, several strategies can be employed to aggregate these chunk-level predictions to determine a single label for the entire document:\n",
    "\n",
    "**Majority Voting:**\n",
    "The label that occurs most frequently among all chunk predictions is assigned to the entire document.\n",
    "\n",
    "**Weighted Voting:**\n",
    "Similar to majority voting, but each vote (chunk prediction) is weighted based on confidence, chunk length, or position in the document.\n",
    "\n",
    "**Threshold-Based Aggregation:**\n",
    "Aggregate the prediction probabilities across chunks and choose the label with the highest cumulative probability, or set a threshold for label validity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
