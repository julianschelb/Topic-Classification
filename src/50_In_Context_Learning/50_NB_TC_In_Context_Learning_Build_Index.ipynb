{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: In Context Learing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pop529700/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"energie\", \"kinder\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding dimension 768\n"
     ]
    }
   ],
   "source": [
    "# Load the transformer-based model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "encoder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "print(\"Sentence embedding dimension\", encoder.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Index:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode texts to embeddings\n",
    "def encode_to_embedding(example):\n",
    "    example['embeddings'] = encoder.encode(example['text'])\n",
    "    return example\n",
    "\n",
    "# Encode all texts in the dataset\n",
    "def build_document_index(encoder, dataset, topic, num_trees=10):\n",
    "    \"\"\"Builds an AnnoyIndex for articles in the dataset using embeddings generated by the encoder.\"\"\"\n",
    "    \n",
    "    # Initiate index\n",
    "    dim = encoder.get_sentence_embedding_dimension()\n",
    "    article_index = AnnoyIndex(dim, 'angular')\n",
    "\n",
    "    # Add articles to index\n",
    "    for page_id, page in tqdm(enumerate(dataset), desc=\"Indexing articles\"):\n",
    "        article_index.add_item(page_id, page[\"embeddings\"])\n",
    "\n",
    "    # Build and save index\n",
    "    article_index.build(num_trees)\n",
    "    article_index.save(f'../../data/indices/page_index_{topic}.ann')\n",
    "    print(f\"Article index for topic '{topic}' saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for cannabis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Indexing articles: 3815it [00:01, 2956.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article index for topic 'cannabis' saved successfully.\n",
      "Loading dataset for energie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4227/4227 [00:08<00:00, 502.94 examples/s]\n",
      "Indexing articles: 4227it [00:01, 2916.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article index for topic 'energie' saved successfully.\n",
      "Loading dataset for kinder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3628/3628 [00:07<00:00, 516.80 examples/s]\n",
      "Indexing articles: 3628it [00:01, 2921.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article index for topic 'kinder' saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for topic in TOPICS:\n",
    "    \n",
    "    print(f\"Loading dataset for {topic}\")\n",
    "    dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "    dataset = dataset[\"train\"].map(encode_to_embedding, batched=True)\n",
    "    build_document_index(encoder, dataset, topic, num_trees=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for cannabis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 3815\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 507\n",
       "    })\n",
       "    holdout: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 33702\n",
       "    })\n",
       "    extended: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id'],\n",
       "        num_rows: 224737\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = \"cannabis\"\n",
    "\n",
    "# Load dataset\n",
    "print(f\"Loading dataset for {topic}\")\n",
    "dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_index = AnnoyIndex(encoder.get_sentence_embedding_dimension(), \"angular\")\n",
    "article_index.load(f'../../data/indices/page_index_{topic}.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dein sicherer Marktplatz Kleinanzeigen durchsuchen Kostenlos inserieren ✔ über 1.3 Mio Anzeigen ✔ über 210.000 Besucher pro Tag ✔ über 75.000 Anfragen pro Tag Was suchst Du? Wellness & Gesundheit Rubrik Bitte warte bis alle Daten geladen sind. PLZ oder Ort Dein Standort +25 km Umkreis im Ort +5 km +10 km +25 km +50 km +100 km +150 km +250 km maximal Finden Gemerkt Kleinanzeigen Düsseldorf Wellness & Gesundheit Massage in Düsseldorf Meine Suche massage Wellness & Gesundheit Düsseldorf Suche speichern Übergabe Abholung 79 Versand 15 Rubriken Alle Rubriken Wellness & Gesundheit 84 Kosmetik und Schönheit 63 Natürlich Leben 18 Medizinische Hilfsmittel, Rollstühle 2 Esoterik 1 Preis eingrenzen Preis von - Preis bis abschicken zu verschenken Anbieter nur Private 18 nur Gewerbliche 66 Angebotstyp nur Angebote 83 nur Gesuche 1 Ort Alle Städte Düsseldorf 38 Krefeld 16 Mönchengladbach 5 Mülheim an der Ruhr 5 Haan 3 Jüchen 3 weitere Städte Neuss 3 Solingen 3 Duisburg 2 Leverkusen 2 Hilden 1 Langenfeld 1 Leichlingen 1 Meerbusch 1 weniger Suche speichern In dieser Rubrik verkaufen Massage in Düsseldorf - 84 Anzeigen neueste Anzeigen neueste Anzeigen Preis aufsteigend Preis absteigend nach Entfernung chinesische Wellness Massage ViTa -TCM Massage Gönnen Sie sich einen Kurzurlaub bei uns! Massagen nach traditioneller chinesischer Technik in unserer Massagestudio. Sie fühlen sich erschöpft ..'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 2763], [0.0, 0.3817378580570221])\n"
     ]
    }
   ],
   "source": [
    "# Encode query and search for similar articles\n",
    "inferred_vector = encoder.encode(dataset[\"train\"][0][\"text\"], convert_to_tensor=True, show_progress_bar = False)\n",
    "sims = article_index.get_nns_by_vector(inferred_vector, 2, search_k=-1, include_distances=True)\n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
