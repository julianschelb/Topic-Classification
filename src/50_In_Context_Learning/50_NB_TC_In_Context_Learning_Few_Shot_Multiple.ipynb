{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: In Context Learing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Limit visibility to only GPU 0\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# # Set the device to GPU 0 if available\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "SPLIT = \"test\" # \"train\", \"test\", \"holdout\", \"extende\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url_and_content\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMONSTR_SAMPLING = \"random_balanced\"  # , \"random_balanced\", \"knn\", \"expert\"\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"energie\", \"kinder\"]\n",
    "MODELS = [\n",
    "    {\n",
    "        \"name\": \"aya-101\",\n",
    "        \"model\": \"CohereForAI/aya-101\",\n",
    "        \"tokenizer_class\": \"AutoTokenizer\",\n",
    "        \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"vicuna-13b\",\n",
    "        \"model\": \"lmsys/vicuna-13b-v1.5\",\n",
    "        \"tokenizer_class\": \"LlamaTokenizer\",\n",
    "        \"model_class\": \"LlamaForCausalLM\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"vicuna-7b\",\n",
    "        \"model\": \"lmsys/vicuna-7b-v1.5\",\n",
    "        \"tokenizer_class\": \"LlamaTokenizer\",\n",
    "        \"model_class\": \"LlamaForCausalLM\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"FLAN-t5-base\",\n",
    "    #     \"model\": \"google/flan-t5-base\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"FLAN-t5-large\",\n",
    "    #     \"model\": \"google/flan-t5-large\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"FLAN-t5-xxl\",\n",
    "    #     \"model\": \"google/flan-t5-xxl\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"leo-hessianai-13b\",\n",
    "    #     \"model\": \"LeoLM/leo-hessianai-13b\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForCausalLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"leo-hessianai-7b\",\n",
    "    #     \"model\": \"LeoLM/leo-hessianai-7b\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForCausalLM\"\n",
    "    # },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Desciptions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_desciptions =  {\n",
    "    \"kinder\": {\n",
    "        \"name\": \"Child Support Act\",\n",
    "        \"description\": \"The Kindergrundsicherung (basic child support) policy aims to combat child poverty by providing a fixed amount, income-dependent supplement, and educational benefits.\",\n",
    "        \"keywords\": ['kinder', 'kindergr', 'paus', 'familie', 'bundestag.de', 'arbeitsagentur.de', 'kindergrundsicherung',  'kindergeld', 'kindersicherung', 'kinderzuschlag', 'gesetz']\n",
    "    },\n",
    "    \"cannabis\": {\n",
    "        \"name\": \"Cannabis Control Act)\",\n",
    "        \"description\": \"The CanG 2023 (Cannabisgesetz, Cannabis Control Act) will legalize the private cultivation of cannabis by adults for personal use and collective non-commercial cultivation\",\n",
    "        \"keywords\": ['cannabis', 'canabis', 'cannabic', 'gras', 'cbd' , 'droge', 'hanf', 'thc', 'canbe', 'legal', 'legalisierung', 'gesetz', 'verein', 'entkriminali']\n",
    "    },\n",
    "    \"energie\":  {\n",
    "        \"name\": \"Renewable Energy Sources Act\",\n",
    "        \"description\": \"The EEG 2023 (Erneuerbare-Energien-Gesetz, Renewable Energy Sources Act) aims to increase the share of renewable energies in gross electricity consumption to at least 80% by 2030\",\n",
    "        \"keywords\": ['energie', 'eeg','grün','gruen','habeck', 'climate', 'strom','Waerme','wende','frderung', 'förderung', 'windkraft', 'windrad', 'photovoltaik',\n",
    "            \t'photovoltaic', 'solar', 'heizung', 'heiz', 'gesetz', 'erneuer', 'geothermie', 'pv', 'geg']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Templates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### ZERO SHOT ###############################\n",
    "\n",
    "PROMPT_TEMPLATE_ZERO_SHOT = \"\"\"Classify the following webpage text in {lang} as topic releated or unrelated. Does it contain information about '{topic}'? Please answer with 'Yes' or 'No' only.\n",
    "\n",
    "Topic description: {topic_desc}\n",
    "Topic keywords: {topic_keyw}\n",
    "\n",
    "URL: '''{url}'''\n",
    "Text: '''{webpage_text}'''\n",
    "Answer:\"\"\"\n",
    "\n",
    "############################### FEW SHOT ###############################\n",
    "\n",
    "PROMPT_TEMPLATE_FEW_SHOT = \"\"\"Classify the following webpage text in {lang} as topic releated or unrelated. Does it contain information about '{topic}'? Please answer with 'Yes' or 'No' only.\n",
    "\n",
    "Topic description: {topic_desc}\n",
    "Topic keywords: {topic_keyw}\n",
    "\n",
    "Examples:\n",
    "{examples}\n",
    "\n",
    "Webpage:\n",
    "URL: '''{url}'''\n",
    "Text: '''{webpage_text}'''\n",
    "Answer:\"\"\"\n",
    "\n",
    "############################### DEMONSTRATOR ###############################\n",
    "\n",
    "PROMPT_TEMPLATE_EXAMPLES = \"\"\"\n",
    "URL: '''{url}'''\n",
    "Text: '''{text}'''\n",
    "Answer: '{label}'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test prompt templates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following webpage text in German as topic releated or unrelated. Does it contain information about 'Cannabis Control Act)'? Please answer with 'Yes' or 'No' only.\n",
      "\n",
      "Topic description: The CanG 2023 (Cannabisgesetz, Cannabis Control Act) will legalize the private cultivation of cannabis by adults for personal use and collective non-commercial cultivation\n",
      "Topic keywords: cannabis, canabis, cannabic, gras, cbd, droge, hanf, thc, canbe, legal, legalisierung, gesetz, verein, entkriminali\n",
      "\n",
      "Examples:\n",
      "\n",
      "URL: '''google.de'''\n",
      "Text: '''Cannabis ist eine Droge.'''\n",
      "Answer: 'Yes'\n",
      "\n",
      "URL: '''example.com'''\n",
      "Text: '''Katzen sind Tiere.'''\n",
      "Answer: 'No'\n",
      "\n",
      "\n",
      "Webpage:\n",
      "URL: '''example.com'''\n",
      "Text: '''Lorem ipsum dolor sit amet, consectetur adipiscing elit.'''\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example text\n",
    "example_list = [\n",
    "    {\n",
    "        \"text\": \"Cannabis ist eine Droge.\",\n",
    "        \"view_url\": \"google.de\",\n",
    "        \"label\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Katzen sind Tiere.\",\n",
    "        \"view_url\": \"example.com\",\n",
    "        \"label\": \"No\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt_list = [PROMPT_TEMPLATE_EXAMPLES.format(text=example[\"text\"], url=example[\"view_url\"],  label=example[\"label\"]) for example in example_list]\n",
    "\n",
    "# print(\"Example prompt list:\")\n",
    "# print(\"\\n\".join(example_prompt_list))\n",
    "\n",
    "topic_desciption = topic_desciptions[\"cannabis\"]\n",
    "topic_name = topic_desciption.get(\"name\")\n",
    "topic_desc = topic_desciption.get(\"description\")\n",
    "topic_keyw = topic_desciption.get(\"keywords\")\n",
    "\n",
    "\n",
    "# Test the template with a dummy text\n",
    "prompt_test = PROMPT_TEMPLATE_FEW_SHOT.format(topic = topic_name, lang = 'German', url = \"example.com\",  topic_desc = topic_desc, topic_keyw = \", \".join(topic_keyw),  webpage_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.', examples=\"\".join(example_prompt_list))\n",
    "print(prompt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter influences the text generation in a specific way. Below are the parameters along with a brief explanation:\n",
    "\n",
    "**`max_length`**:\n",
    "* Sets the maximum number of tokens in the generated text (default is 50).\n",
    "* Generation stops if the maximum length is reached before the model produces an EOS token.\n",
    "* A higher `max_length` allows for longer generated texts but may increase the time and computational resources required.\n",
    "\n",
    "**`min_length`**:\n",
    "* Sets the minimum number of tokens in the generated text (default is 10).\n",
    "* Generation continues until this minimum length is reached even if an EOS token is produced.\n",
    "\n",
    "**`num_beams`**:\n",
    "* In beam search, sets the number of \"beams\" or hypotheses to keep at each step (default is 4).\n",
    "* A higher number of beams increases the chances of finding a good output but also increases the computational cost.\n",
    "\n",
    "**`num_return_sequences`**:\n",
    "* Specifies the number of independently computed sequences to return (default is 3).\n",
    "* When using sampling, multiple different sequences are generated independently from each other.\n",
    "\n",
    "**`early_stopping`**:\n",
    "* Stops generation if the model produces the EOS (End Of Sentence) token, even if the predefined maximum length is not reached (default is True).\n",
    "* Useful when an EOS token signifies the logical end of a text (often represented as `</s>`).\n",
    "\n",
    "**`do_sample`**:\n",
    "* Tokens are selected probabilistically based on their likelihood scores (default is True).\n",
    "* Introduces randomness into the generation process for diverse outputs.\n",
    "* The level of randomness is controlled by the 'temperature' parameter.\n",
    "\n",
    "**`temperature`**:\n",
    "* Adjusts the probability distribution used for sampling the next token (default is 0.7).\n",
    "* Higher values make the generation more random, while lower values make it more deterministic.\n",
    "\n",
    "**`top_k`**:\n",
    "* Limits the number of tokens considered for sampling at each step to the top K most likely tokens (default is 50).\n",
    "* Can make the generation process faster and more focused.\n",
    "\n",
    "**`top_p`**:\n",
    "* Also known as nucleus sampling, sets a cumulative probability threshold (default is 0.95).\n",
    "* Tokens are sampled only from the smallest set whose cumulative probability exceeds this threshold.\n",
    "\n",
    "**`repetition_penalty`**:\n",
    "* Discourages the model from repeating the same token by modifying the token's score (default is 1.5).\n",
    "* Values greater than 1.0 penalize repetitions, and values less than 1.0 encourage repetitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "          'early_stopping': True,\n",
    "          #'num_beams': 5,\n",
    "          #'num_return_sequences': 5,\n",
    "          'max_new_tokens': 128,\n",
    "          'min_new_tokens': 1,\n",
    "          #'output_scores': True,\n",
    "          # 'repetition_penalty': 1.0,\n",
    "          #'max_length': 8192,\n",
    "          'temperature': 0.3,\n",
    "          'top_k': 50,\n",
    "          'top_p': 0.95\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_prompt(article, template, template_example, topic, topic_desciptions, examples, lang='German'):\n",
    "    \"\"\" Compiles the prompt for the given article and model.\"\"\"\n",
    "\n",
    "    # Get the topic description and keywords\n",
    "    topic_desciption = topic_desciptions[topic]\n",
    "    topic_name = topic_desciption.get(\"name\")\n",
    "    topic_desc = topic_desciption.get(\"description\")\n",
    "    topic_keyw = topic_desciption.get(\"keywords\")\n",
    "    \n",
    "    # Get the text of the article\n",
    "    article_text = article.get(\"text\")\n",
    "    article_lang = article.get(\"lang\")\n",
    "    article_url = article.get(\"view_url\")\n",
    "    example_prompts = [template_example.format(text=example[\"text\"], url=example[\"view_url\"], label= \"Yes\" if example[\"label\"] == 1 else \"No\") for example in examples]\n",
    "    prompt = template.format(topic = topic_name, url = article_url,  topic_desc = topic_desc, topic_keyw = topic_keyw,\n",
    "                             lang = article_lang, webpage_text=article_text, examples=\"\".join(example_prompts))\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classify the following webpage text in de as topic releated or unrelated. Does it contain information about 'Cannabis Control Act)'? Please answer with 'Yes' or 'No' only.\n",
      "\n",
      "Topic description: The CanG 2023 (Cannabisgesetz, Cannabis Control Act) will legalize the private cultivation of cannabis by adults for personal use and collective non-commercial cultivation\n",
      "Topic keywords: ['cannabis', 'canabis', 'cannabic', 'gras', 'cbd', 'droge', 'hanf', 'thc', 'canbe', 'legal', 'legalisierung', 'gesetz', 'verein', 'entkriminali']\n",
      "\n",
      "Examples:\n",
      "\n",
      "URL: '''google.de'''\n",
      "Text: '''Cannabis ist eine Droge.'''\n",
      "Answer: 'No'\n",
      "\n",
      "URL: '''example.com'''\n",
      "Text: '''Katzen sind Tiere.'''\n",
      "Answer: 'No'\n",
      "\n",
      "\n",
      "Webpage:\n",
      "URL: '''test.com'''\n",
      "Text: '''Lorem Ipsum'''\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "article = {\"view_url\": \"test.com\", \"text\": \"Lorem Ipsum\", \"lang\": \"de\"}\n",
    "exmaple_prompt = compile_prompt(article, PROMPT_TEMPLATE_FEW_SHOT, PROMPT_TEMPLATE_EXAMPLES, \"cannabis\", topic_desciptions, example_list, lang='German')\n",
    "print(exmaple_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_input_length(prompt):\n",
    "    \"\"\" Calculates the length of the input sequence for the model. \"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", add_special_tokens=False, truncation=False, padding=False)\n",
    "\n",
    "    # Calculate the length of the input sequence\n",
    "    input_length = tokenized_prompt.input_ids.size(1)\n",
    "\n",
    "    return input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model, tokenizer, prompt, params, remove_input=True):\n",
    "    \"\"\"Generates answers from a language model for a given prompt.\"\"\"\n",
    "\n",
    "    # Encode the prompt and generate the answers\n",
    "    encoded_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    if encoded_input.size()[1] > tokenizer.model_max_length:\n",
    "        print(\"Input too long, truncating.\")\n",
    "        # encoded_input = encoded_input[:, :tokenizer.model_max_length]\n",
    "\n",
    "    generated_outputs = model.generate(encoded_input, **params)\n",
    "\n",
    "    # Decode and clean outputs\n",
    "    outputs = []\n",
    "    input_text_wo_st = tokenizer.decode(\n",
    "        encoded_input[0], skip_special_tokens=True)\n",
    "    for output in generated_outputs:\n",
    "        decoded_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        cleaned_text = decoded_text.replace(input_text_wo_st, \"\").strip()\n",
    "        outputs.append(cleaned_text if remove_input else decoded_text)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(output_text):\n",
    "    \"\"\"Determines if the model's output signifies \"Yes\" (1) or \"No\" (0).\"\"\"\n",
    "    text = output_text.lower()\n",
    "    return 1 if \"yes\" in text else 0 if \"no\" in text else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy, precision, recall, and F1 score for the given labels and predictions and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary'),\n",
    "        'recall': recall_score(labels, preds, average='binary'),\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_from_dataset(dataset, n=5, subset='test'):\n",
    "    \"\"\"\n",
    "    Samples n random examples from a specified subset of the dataset.\n",
    "    \"\"\"\n",
    "    n = min(n, len(dataset[subset]))\n",
    "    random_indices = random.sample(range(len(dataset[subset])), n)\n",
    "    sampled_dataset = dataset[subset].select(random_indices)\n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_details):\n",
    "    \"\"\"\n",
    "    Loads a model and its corresponding tokenizer based on the provided model details.\n",
    "    \"\"\"\n",
    "    model_name = model_details['model']\n",
    "    tokenizer_class = model_details['tokenizer_class']\n",
    "    model_class = model_details['model_class']\n",
    "    \n",
    "    # Cohere models and FLAN models\n",
    "    if tokenizer_class == \"AutoTokenizer\" and model_class == \"AutoModelForSeq2SeqLM\":\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "        \n",
    "    # Vicuna models\n",
    "    elif tokenizer_class == \"LlamaTokenizer\" and model_class == \"LlamaForCausalLM\":\n",
    "        from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "        \n",
    "    #  LeoLM models  \n",
    "    elif tokenizer_class == \"AutoTokenizer\" and model_class == \"AutoModelForCausalLM\":\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True, trust_remote_code=False)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Model class not supported.\")\n",
    "        \n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Sample Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "################### Random Sampling ############################\n",
    "\n",
    "def sample_examples_random(dataset, k=2):\n",
    "    \"\"\"Samples k pairs of examples completely at random.\"\"\"\n",
    "    dataset_sampled = dataset.shuffle().select(range(k))\n",
    "    return [example for example in dataset_sampled]\n",
    "\n",
    "\n",
    "################### Random Sampling balanced ###################\n",
    "\n",
    "def sample_examples_random_balanced(dataset, k=2):\n",
    "    \"\"\"Samples k pairs of examples, each pair containing one positive and one negative example.\"\"\"\n",
    "    # Separate the dataset into positive and negative examples\n",
    "    positive_examples = [example for example in dataset if example['label'] == 1]\n",
    "    negative_examples = [example for example in dataset if example['label'] == 0]\n",
    "    \n",
    "    # Sample k examples from each subset\n",
    "    sampled_positive = sample(positive_examples, k)\n",
    "    sampled_negative = sample(negative_examples, k)\n",
    "    \n",
    "    # Alternate between positive and negative examples to create pairs\n",
    "    examples = []\n",
    "    for idx in range(k):\n",
    "        if idx % 2 == 0:\n",
    "            examples.append(sampled_positive[idx])\n",
    "        else:\n",
    "            examples.append(sampled_negative[idx])\n",
    "    \n",
    "    return examples\n",
    "\n",
    "\n",
    "################### KNN Sampling ############################\n",
    "\n",
    "def sample_examples_knn(model, index, query, dataset, k=2):\n",
    "    inferred_vector = model.encode(query, convert_to_tensor=True, show_progress_bar = False)\n",
    "    sims = index.get_nns_by_vector(inferred_vector, k, search_k=-1, include_distances=False)\n",
    "    return [dataset[idx] for idx in sims]\n",
    "\n",
    "################### Expert Sampling ############################\n",
    "\n",
    "# def sample_from_expert(curated_examples, topic, k=2):\n",
    "    \n",
    "#     curated_examples_topic = curated_examples[topic]\n",
    "#     sampled_positive = sample(curated_examples_topic['positive_examples'], k)\n",
    "#     sampled_negative = sample(curated_examples_topic['negative_examples'], k)\n",
    "    \n",
    "#     # Alternate between positive and negative examples to create pairs\n",
    "#     examples = []\n",
    "#     for idx in range(k):\n",
    "#         if idx % 2 == 0:\n",
    "#             examples.append(sampled_positive[idx])\n",
    "#         else:\n",
    "#             examples.append(sampled_negative[idx])\n",
    "    \n",
    "#     return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Encoder for KNN Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transformer-based model\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "encoder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode texts to embeddings\n",
    "def encode_to_embedding(example):\n",
    "    example['embeddings'] = encoder.encode(example['text'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority vote is: Apple\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def majority_voting(answers):\n",
    "    \"\"\"Apply majority voting to a list of arbitrary classification answers.\"\"\"\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common(2)  # Get the two most common answers\n",
    "    \n",
    "    if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:\n",
    "        return \"Tie\"\n",
    "    return most_common[0][0]\n",
    "\n",
    "# Example usage with arbitrary labels\n",
    "answers = [\"Apple\", \"Banana\", \"Apple\", \"Orange\"]\n",
    "print(f\"The majority vote is: {majority_voting(answers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = 2\n",
    "# examples = sample_examples_random(dataset['train'], k) \n",
    "# examples = sample_examples_random_balanced(dataset['train'], k)\n",
    "# examples = sample_examples_knn(dataset['train'], k)\n",
    "# examples = sample_from_expert(curated_examples, topic, k)\n",
    "# sample_examples_knn(model, article_index, dataset[\"train\"][0][\"text\"], dataset[\"train\"], k)\n",
    "#print(\"Examples: \", examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model lmsys/vicuna-13b-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pytorch_model-00001-of-00003.bin: 100%|██████████| 9.95G/9.95G [01:31<00:00, 109MB/s]\n",
      "pytorch_model-00002-of-00003.bin: 100%|██████████| 9.90G/9.90G [01:29<00:00, 110MB/s]\n",
      "pytorch_model-00003-of-00003.bin: 100%|██████████| 6.18G/6.18G [00:57<00:00, 108MB/s]\n",
      "Downloading shards: 100%|██████████| 3/3 [03:59<00:00, 79.72s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:10<00:00,  3.66s/it]\n",
      "generation_config.json: 100%|██████████| 192/192 [00:00<00:00, 1.22MB/s]\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/pop529700/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for cannabis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/pop529700/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "100%|██████████| 10/10 [00:55<00:00,  5.51s/it]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 2007.23 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3815/3815 [00:00<00:00, 490580.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2505.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33702/33702 [00:00<00:00, 803558.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 224737/224737 [00:00<00:00, 907748.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Yes'\", \"'No'\", \"'Yes'\", \"'No'\", \"'No'\", \"'No'\", \"'Yes'\", \"'No'\", \"'No'\", \"'No'\"]\n",
      "Metrics for lmsys/vicuna-13b-v1.5: {'accuracy': 0.9, 'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571}\n",
      "Loading dataset for energie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:04<00:00,  6.44s/it]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 2082.37 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4227/4227 [00:00<00:00, 507407.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2545.24 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 39782/39782 [00:00<00:00, 815563.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 229661/229661 [00:00<00:00, 896364.31 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Yes'\", \"'Yes'\", \"'No'\", \"'Yes'\", \"'No'\", \"'No'\", \"'Yes'\", \"'No'\", \"'Yes'\", \"'Yes'\"]\n",
      "Metrics for lmsys/vicuna-13b-v1.5: {'accuracy': 0.8, 'precision': 1.0, 'recall': 0.75, 'f1': 0.8571428571428571}\n",
      "Loading dataset for kinder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:08<00:00,  6.89s/it]\n",
      "Flattening the indices: 100%|██████████| 10/10 [00:00<00:00, 2063.62 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3628/3628 [00:00<00:00, 468848.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 2524.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33730/33730 [00:00<00:00, 814819.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 266322/266322 [00:00<00:00, 913016.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'Yes'\", \"'Yes'\", \"'Yes'\", \"'No'\", \"'Yes'\", \"'No'\", \"'Yes'\", \"'No'\", \"'Yes'\", \"'Yes'\"]\n",
      "Metrics for lmsys/vicuna-13b-v1.5: {'accuracy': 0.8, 'precision': 1.0, 'recall': 0.7777777777777778, 'f1': 0.875}\n"
     ]
    }
   ],
   "source": [
    "eval_results = defaultdict(dict)\n",
    "\n",
    "for model_details in MODELS: #-------------------------------------------------------------\n",
    "\n",
    "    # Load model\n",
    "    model_name = model_details['model']\n",
    "    print(f\"Loading model {model_name}\")\n",
    "    tokenizer, model = load_model_and_tokenizer(model_details)\n",
    "    \n",
    "    for topic in TOPICS:  # ---------------------------------------------------------------\n",
    "\n",
    "        # Load dataset\n",
    "        print(f\"Loading dataset for {topic}\")\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "        #dataset['test'] = dataset['test'].filter(lambda x: x['token_count'] > 200)\n",
    "        #dataset[SPLIT] = sample_random_from_dataset(dataset, n=10, subset=SPLIT)\n",
    "        #print(dataset['test'][0])\n",
    "\n",
    "        # Few-Shot ------------------------------------------------------------------------\n",
    "\n",
    "        # Random Sampling\n",
    "        if DEMONSTR_SAMPLING == \"random\":\n",
    "            examples = sample_examples_random(dataset['train'], k=K) \n",
    "            #print(\"Examples: \", len(examples))  \n",
    "\n",
    "        # Random Sampling (Balanced by classes)\n",
    "        elif DEMONSTR_SAMPLING ==  \"random_balanced\":\n",
    "            examples = sample_examples_random_balanced(dataset['train'], k=K)\n",
    "\n",
    "        # KNN Cluster-based sampling\n",
    "        elif DEMONSTR_SAMPLING == \"knn\":\n",
    "            article_index = AnnoyIndex(encoder.get_sentence_embedding_dimension(), \"angular\")\n",
    "            article_index.load(f'../../data/indices/page_index_{topic}.ann')\n",
    "            \n",
    "        # # Use curated examples\n",
    "        # elif DEMONSTR_SAMPLING == \"expert\":\n",
    "        #     examples = sample_from_expert(curated_examples, topic, k)\n",
    "\n",
    "        # Zero-Shot ------------------------------------------------------------------------\n",
    "        \n",
    "        elif K == 0:\n",
    "            examples = []\n",
    "        else: \n",
    "            examples = []\n",
    "    \n",
    "        # Iterate over pages in test split\n",
    "        answers = [] \n",
    "        for row in tqdm(dataset[SPLIT]): # ---------------------------------------------------\n",
    "    \n",
    "            # Dynamic Sampling \n",
    "            if DEMONSTR_SAMPLING == \"knn\" and K > 0:\n",
    "                examples = sample_examples_knn(encoder, article_index, row[\"text\"], dataset[\"train\"], K)\n",
    "    \n",
    "            # Generate answers\n",
    "            prompt_template = PROMPT_TEMPLATE_FEW_SHOT if K > 0 else PROMPT_TEMPLATE_ZERO_SHOT\n",
    "            prompt = compile_prompt(row, prompt_template, PROMPT_TEMPLATE_EXAMPLES, topic, topic_desciptions, examples)\n",
    "            answers.append(generate_answers(model, tokenizer, prompt, params))\n",
    "            \n",
    "    \n",
    "        # Add answers to the dataset\n",
    "        dataset[SPLIT] = dataset[SPLIT].add_column(\"answers\", answers)\n",
    "        dataset.save_to_disk(f\"../../data/tmp/processed_dataset_{topic}_answers_{K}s_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_{model_name.split('/')[1]}_{FEATURES}_{SPLIT}_{DEMONSTR_SAMPLING}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        answers_after_voting = [majority_voting(ans) for ans in answers]\n",
    "        print(answers_after_voting)\n",
    "        answers_parsed = [parse_response(ans) for ans in answers_after_voting]\n",
    "        metrics = calc_metrics(dataset['test']['label'], answers_parsed)\n",
    "        eval_results[model_name][topic] = metrics\n",
    "        print(f\"Metrics for {model_name}: {metrics}\")\n",
    "        \n",
    "    # Clear GPU memory to avoid memory errors\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    del model, tokenizer\n",
    "    gc.collect()  # Explicitly invoking garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear cache again after garbage collection\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '648c2adb8e8cadbd29095617',\n",
       " 'batch_id': 15,\n",
       " 'domain': 'schule.at',\n",
       " 'view_url': 'www.schule.at/gewinnspiel-buch-tori-twister-stuermisch-unterwegs',\n",
       " 'lang': 'de',\n",
       " 'text': 'Your browser does not support JavaScript! Webmail Bildungs-News Schul-Themen Unterrichts-Portale Schulführer Termine Tools & Apps Webmail schule.at Gewinnspiel: Buch \"Tori Twister - Stürmisch unterwegs\" © KOSMOS Verlag Tori Twister - Stürmisch unterwegs Die Autorin von \"Tori Twister - Stürmisch unterwegs\" verpackt Naturwissen zu Wetterphänomenen und Umweltschutz in eine spannende und lustige Geschichte. Wir haben bei Autorin Marikka Pfeiffer nachgefragt, was Ihre Beweggründe waren, dieses Buch zu schreiben. Und mit etwas Glück können Sie bei unserem Gewinnspiel ein Buchexemplar gewinnen . Als Geschichtenerzählerin liegt es mir sehr am Herzen, mit meinen Büchern die Freude am Lesen zu wecken, denn die Vorstellungskraft und das Lesen sind für mich zwei der wichtigsten Kulturgüter, die wir haben. Daher ist die Geschichte von TORI TWISTER vor allem ein spannendes Abenteuer voller Rätsel und einer Prise Magie. Das Wetter als Thema kam auf ganz simple Weise zu mir: Bei einem Post auf Social Media las ich das Wort - Wetterfabrik - und sah in meiner Vorstellung sofort eine geheime Wetterküche vor mir. Ich wusste auch prompt, wer die vor über hundert Jahren aus welchem Grund errichtet hat. Und während ich mich in dieses Bild hinein geträumt habe, tauchte Familie Twister vor meinem geistigen Auge auf und erzählte mir ihre Geschichte - die ich dann aufschrieb',\n",
       " 'text_length': 1370,\n",
       " 'word_count': 209,\n",
       " 'topic': 'kinder',\n",
       " 'category': 'other',\n",
       " 'good_for_training': 'True',\n",
       " 'good_for_augmentation': 'True',\n",
       " 'annotation_type': '04.urls-with-title',\n",
       " 'is_topic': True,\n",
       " 'label': 1,\n",
       " 'token_count': 246,\n",
       " 'chunk_id': 12,\n",
       " 'answers': [\"'Yes'\", \"'Yes'\", \"'Yes'\", \"'Yes'\", \"'Yes'\"]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get chunk level predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path = f\"eval_results_icl_{K}s_{DEMONSTR_SAMPLING}_{SPLIT}_chunks.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the dictionary from the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    eval_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all topics (assuming all models are evaluated on the same topics)\n",
    "topics = list(next(iter(eval_results.values())).keys())\n",
    "\n",
    "# Prepare headers for the table: each topic will have four metrics\n",
    "headers = [\"Model\"] + \\\n",
    "    [f\"{topic} {metric}\" for topic in topics for metric in [\n",
    "        \"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# Prepare rows: one row per model, containing metrics for each topic\n",
    "rows = []\n",
    "for model_name_t, topics_metrics in eval_results.items():\n",
    "    row = [model_name_t]  # Start with the model name\n",
    "    for topic in topics:\n",
    "        metrics = topics_metrics.get(topic, {})\n",
    "        row.extend([metrics.get('accuracy', 0.0), metrics.get(\n",
    "            'precision', 0.0), metrics.get('recall', 0.0), metrics.get('f1', 0.0)])\n",
    "    rows.append(row)\n",
    "\n",
    "# Generate the HTML table\n",
    "table_html = tabulate(rows, headers=headers, tablefmt=\"html\",\n",
    "                      showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model                </th><th style=\"text-align: right;\">  cannabis Acc.</th><th style=\"text-align: right;\">  cannabis Prec.</th><th style=\"text-align: right;\">  cannabis Rec.</th><th style=\"text-align: right;\">  cannabis F1</th><th style=\"text-align: right;\">  energie Acc.</th><th style=\"text-align: right;\">  energie Prec.</th><th style=\"text-align: right;\">  energie Rec.</th><th style=\"text-align: right;\">  energie F1</th><th style=\"text-align: right;\">  kinder Acc.</th><th style=\"text-align: right;\">  kinder Prec.</th><th style=\"text-align: right;\">  kinder Rec.</th><th style=\"text-align: right;\">  kinder F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lmsys/vicuna-13b-v1.5</td><td style=\"text-align: right;\">          0.900</td><td style=\"text-align: right;\">           1.000</td><td style=\"text-align: right;\">          0.750</td><td style=\"text-align: right;\">        0.857</td><td style=\"text-align: right;\">         0.800</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">         0.750</td><td style=\"text-align: right;\">       0.857</td><td style=\"text-align: right;\">        0.800</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        0.778</td><td style=\"text-align: right;\">      0.875</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get page level predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majority_voting(answers):\n",
    "    \"\"\"Apply majority voting to a list of arbitrary classification answers.\"\"\"\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common()  # Get all common answers sorted by frequency\n",
    "\n",
    "    if not most_common:\n",
    "        return 0 # Handle empty input scenario\n",
    "\n",
    "    # Check for ties at the highest count\n",
    "    max_votes = most_common[0][1]\n",
    "    tied_classes = [cls for cls, votes in most_common if votes == max_votes]\n",
    "\n",
    "    if len(tied_classes) > 1:\n",
    "        return max(tied_classes)  # Return the maximum class label in case of a tie\n",
    "    return tied_classes[0]  # Return the class with the most votes\n",
    "\n",
    "majority_voting([1, 1, 2, 2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model CohereForAI/aya-101 on cannabis ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/507 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:00<00:00, 6785.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for CohereForAI/aya-101 on cannabis: {'accuracy': 0.5581395348837209, 'precision': 0.5128205128205128, 'recall': 1.0, 'f1': 0.6779661016949152}\n",
      "\n",
      "\n",
      "###### Evaluating model lmsys/vicuna-13b-v1.5 on cannabis ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:00<00:00, 8818.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for lmsys/vicuna-13b-v1.5 on cannabis: {'accuracy': 0.9534883720930233, 'precision': 0.95, 'recall': 0.95, 'f1': 0.95}\n",
      "\n",
      "\n",
      "###### Evaluating model lmsys/vicuna-7b-v1.5 on cannabis ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 507/507 [00:00<00:00, 8837.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for lmsys/vicuna-7b-v1.5 on cannabis: {'accuracy': 0.9767441860465116, 'precision': 0.9523809523809523, 'recall': 1.0, 'f1': 0.975609756097561}\n",
      "\n",
      "\n",
      "###### Evaluating model CohereForAI/aya-101 on energie ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:00<00:00, 8856.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for CohereForAI/aya-101 on energie: {'accuracy': 0.8913043478260869, 'precision': 0.875, 'recall': 0.9130434782608695, 'f1': 0.8936170212765957}\n",
      "\n",
      "\n",
      "###### Evaluating model lmsys/vicuna-13b-v1.5 on energie ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:00<00:00, 8824.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for lmsys/vicuna-13b-v1.5 on energie: {'accuracy': 0.9130434782608695, 'precision': 1.0, 'recall': 0.8260869565217391, 'f1': 0.9047619047619048}\n",
      "\n",
      "\n",
      "###### Evaluating model lmsys/vicuna-7b-v1.5 on energie ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:00<00:00, 8936.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for lmsys/vicuna-7b-v1.5 on energie: {'accuracy': 0.7608695652173914, 'precision': 0.6875, 'recall': 0.9565217391304348, 'f1': 0.8}\n",
      "\n",
      "\n",
      "###### Evaluating model CohereForAI/aya-101 on kinder ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 8793.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for CohereForAI/aya-101 on kinder: {'accuracy': 0.8837209302325582, 'precision': 1.0, 'recall': 0.7619047619047619, 'f1': 0.8648648648648649}\n",
      "\n",
      "\n",
      "###### Evaluating model lmsys/vicuna-13b-v1.5 on kinder ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 8861.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for lmsys/vicuna-13b-v1.5 on kinder: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "\n",
      "\n",
      "###### Evaluating model lmsys/vicuna-7b-v1.5 on kinder ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 8827.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for lmsys/vicuna-7b-v1.5 on kinder: {'accuracy': 0.813953488372093, 'precision': 0.7241379310344828, 'recall': 1.0, 'f1': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results_pages = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "    for model_details in MODELS: # -------------------------------------------------------------\n",
    "        \n",
    "        model_name = model_details['model']\n",
    "            \n",
    "        print(f\"\\n\\n###### Evaluating model {model_name} on {topic} ###### \\n\\n\")\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_answers_{K}s_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_{model_name.split('/')[1]}_{FEATURES}_{SPLIT}_{DEMONSTR_SAMPLING}\")\n",
    "        \n",
    "        #print(dataset)\n",
    "        \n",
    "        # Group dataset examples by URL, with a fallback to domain\n",
    "        grouped_dataset = {}\n",
    "        for example in tqdm(dataset[SPLIT]):\n",
    "            url = example.get(\"view_url\") or example.get(\"domain\")\n",
    "            example_filtered = {k: example[k] for k in [\"text\", \"domain\", \"answers\", \"label\", \"category\", \"annotation_type\", \"lang\"]}\n",
    "            grouped_dataset.setdefault(url, []).append(example_filtered)\n",
    "            \n",
    "        # Extract labels\n",
    "        labels = []\n",
    "        for url, chunks in grouped_dataset.items():\n",
    "            label = [chunk[\"label\"] for chunk in chunks]\n",
    "            labels.append(max(label))\n",
    "            \n",
    "        # Merge chunk level predictions\n",
    "        predictions = []\n",
    "        for url, chunks in grouped_dataset.items():\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                answers = [parse_response(ans) for ans in chunk[\"answers\"]]\n",
    "                chunk[\"pred\"] = majority_voting(answers)\n",
    "                \n",
    "            preds = [chunk[\"pred\"] for chunk in chunks]\n",
    "            pred = majority_voting([pred for pred in preds if pred > 0]) if max(preds) > 0 else 0\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # Use the trained model to make predictions on the test set\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "        metrics = calc_metrics(labels, predictions)\n",
    "        print(f\"Metrics for {model_name} on {topic}: {metrics}\")\n",
    "        \n",
    "        # Update the eval_results dictionary\n",
    "        eval_results_pages[model_name][topic] = metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['www.schule.at/gewinnspiel-buch-tori-twister-stuermisch-unterwegs', 'www.betexplorer.com/football/italy/serie-c-promotion-play-offs/', 'www1.wdr.de/radio/player/radioplayer104~_layout-popupVersion.html', 'swoodoo.com/flights/HAM,BRE-BCN,AYT/2023-07-17/2023-07-24', 'www.primeraportal.de/primera/paidbanner/click-4508.html', 'web.de/magazine/sport/fussball/bundesliga/fc-bayern/aktuellen-transfergeruechte-fc-bayern-muenchen-38267340', 'gutefrage.net/frage/serien-und-filme-aufm-handy-schauen', 'alexas-koestlichkeiten.blogspot.com/2013/07/turkische-frikadellen-spiee.html?m=1', 'forum.digitalfernsehen.de/threads/rundfunkbeitrag-ard-will-um-erh%C3%B6hung-%E2%80%9Ek%C3%A4mpfen%E2%80%9C.438741/page-13', 'de.m.wikipedia.org/wiki/Die_unheimlichen_Drei', 'www.ikk-gesundplus.de/die-ikk/', 'www.hz.de/meinort/gerstetten/landwirte-ueben-kritik-anhauser-strasse-in-dettingen-wird-fuer-neuen-radweg-verschmaelert-70691257.html', 'gmx.net/magazine/politik/russland-krieg-ukraine/ukraine-vermeldet-tote-angriff-wohnhaus-38313694', 'm.bild.de/sport/american-football/nfl/nfl-insta-patzer-verraet-es-sie-datet-heimlich-diesen-superstar-84295678.bildMobile.html', 'web.de/magazine/unterhaltung/tv-shows/fail-zdf-fernsehgarten-sven-hannawalds-outfit-irritiert-38320186#.logout.inactivityDialog_4_inactivityDialog_treatSimple.Outfit-Fail im %22ZDF-Fernsehgarten%22: Sven Hannawald sorgt für Irritationen.1', 'rtl.lu/tele', 'www.stern.de/lifestyle/special/content-7149632.html?an=s:spezial_779019_wfb_fc-a:4-t:n', 'www.klamm.de/forum/threads/was-trinkt-ihr-gerade.15564/page-3996#post-8662732', 'change.org/p/0-0-mehrwertsteuer-auf-bio-lebensmittel/psf/share?source_location=combo_psf&psf_variant=combo&cbd_s=eyJleHBlcmltZW50TmFtZSI6InBzZl9jb21iby0zNDQxNzQ0MiIsInZhcmlhbnQiOnsidmFyaWFudE5hbWUiOiJhMyIsImRhdGEiOnsiYW1vdW50Ijo0MDQsImFtb3VudF9pZCI6ImEzIn0sInB1bGxzIjoyMDgxOSwicmV3YXJkcyI6MzA1Nzk1fSwidmFyaWFudE5hbWUiOiJhMyIsImNvbWJvQmFuZGl0QW1vdW50Ijo3LCJhbW91bnRJZCI6ImEzIn0%3D&share_intent=1', 'www.doccheck.com/de/detail/videos/5982-thoraxschmerz-kennt-ihr-die-5-gruende?utm_source=DC-Newsletter&utm_medium=email&utm_campaign=DocCheck-News_2023-06-12&utm_content=asset&utm_term=video&dcuid=e554dd2f7bf4d5d5af3b3b02b671767d&sc_src=email_3802896&sc_lid=384617484&sc_uid=WVnPaxdhVB&sc_llid=206412&sc_eh=71858855e44dc4491', 'express.de/sport/fussball/1-fc-koeln', 'nord24.de/landkreis-cuxhaven/notfallpatient-in-cuxhaven-abgelehnt-helios-klinik-feuert-arzt-111942.html', 'www.tagesschau.de/inland/kinderarmut-155.html', 'savethechildren.de/news/kindergrundsicherung-was-ist-das-eigentlich-und-worauf-kommt-es-jetzt-an?gclid=CjwKCAjwyqWkBhBMEiwAp2yUFpmuMwfsaHvPfr4Q53CCwXYAjxXua53g0oL8wei3wZ6ceCjTo4SM7xoCL1QQAvD_BwE', 'morgenpost.de/ratgeber/article237749599/kindergrundsicherung-2025-eckpunkte-reform-familien-kinder.html', 'www.t-online.de/nachrichten/deutschland/gesellschaft/id_100137536/kindergrundsicherung-statt-kindergeld-wie-viel-geld-bekommen-familien-.html', 'www.arbeitsagentur.de/familie-und-kinder/kinderzuschlag-verstehen/kiz-lotse', 'www.bmfsfj.de/bmfsfj/aktuelles/reden-und-interviews/lisa-paus-kindergrundsicherung-soll-alle-familienkonstellationen-erreichen--214628', 'www.mdr.de/brisant/ratgeber/kindergrundsicherung-132.html#:~:text=Grund%2D%20und%20Zusatzbetrag%3A%20Mit%20wieviel,alle%20zwei%20Jahre%20angepasst%20wird.', 'www.kinder-grund-sicherung.de/', 'zdf.de/nachrichten/politik/kindergrundsicherung-eckpunkte-paus-100.html', 'www.vdk.de/deutschland/pages/themen/soziale_gerechtigkeit/86234/kampf_gegen_kinderarmut_verena_bentele_sprecherin_buendnis_kindergrundsicherung', 'www.tichyseinblick.de/gastbeitrag/kindergrundsicherung-migration-kinderarmut/', 'www.deutschlandfunkkultur.de/kindergrundsicherung-100.html#wieviel', 'www.dkhw.de/presse/schlagzeilen-archiv/schlagzeilen-details/aufbruch-in-ein-kindgerechtes-deutschland-mit-kindergrundsicherung-die-kinderarmut-in-deutschland-b/?gclid=CjwKCAjwhJukBhBPEiwAniIcNQS49LY21JtMfGoSBKOBf1iTqnO9uxAIJ6F5iQdHVGQJPljpq-jMzRoCGx8QAvD_BwE', 'www.augsburger-allgemeine.de/politik/kindergrundsicherung-2023-ab-wann-hoehe-zeitplan-id65632151.html', 'savethechildren.de/news/kindergrundsicherung-was-ist-das-eigentlich-und-worauf-kommt-es-jetzt-an/?gclid=CjwKCAjwhJukBhBPEiwAniIcNWaAFqtH_j5jYmkP5aajiqDQ-pDxpA0przjwgGzLE0SAAhpB19wl9xoC9vsQAvD_BwE', 'www.tagesschau.de/inland/innenpolitik/kindergrundsicherung-105.html', 'www.zeit.de/zustimmung?url=https%3A%2F%2Fwww.zeit.de%2Fpolitik%2Fdeutschland%2F2023-03%2Flisa-paus-kindergrundsicherung-haushalt-armut', 'www.arbeitsagentur.de/eservices?pk_vid=6a58ed12b997ec94168675482402f2cd', 'www.apuntateuna.es/wann/wann-kommt-kindergeld-bonus-2022.html', 'merkur.de/leben/geld/ausbildung-antrag-kindergrundsicherung-kindergeld-2025-zukunft-hoehe-alter-einkommen-zr-92242682.html#:~:text=Wer%20jetzt%20Kindergeld%20bekommt%20und,Geld%20bis%20zu%20seinem%2025.', 'einfach-elterngeld.de/'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path = f\"eval_results_icl_{K}s_{DEMONSTR_SAMPLING}_{SPLIT}_pages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results_pages, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    eval_results_pages = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all topics (assuming all models are evaluated on the same topics)\n",
    "topics = list(next(iter(eval_results_pages.values())).keys())\n",
    "\n",
    "# Prepare headers for the table: each topic will have four metrics\n",
    "headers = [\"Model\"] + [f\"{topic} {metric}\" for topic in topics for metric in [\"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# Prepare rows: one row per model, containing metrics for each topic\n",
    "rows = []\n",
    "for model, topics_metrics in eval_results_pages.items():\n",
    "    row = [model]  # Start with the model name\n",
    "    for topic in topics:\n",
    "        metrics = topics_metrics.get(topic, {})\n",
    "        row.extend([metrics.get('accuracy',0.0), metrics.get('precision',0.0), metrics.get('recall',0.0), metrics.get('f1',0.0)])\n",
    "    rows.append(row)\n",
    "\n",
    "# Generate the HTML table\n",
    "table_html = tabulate(rows, headers=headers, tablefmt=\"html\", showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model                </th><th style=\"text-align: right;\">  cannabis Acc.</th><th style=\"text-align: right;\">  cannabis Prec.</th><th style=\"text-align: right;\">  cannabis Rec.</th><th style=\"text-align: right;\">  cannabis F1</th><th style=\"text-align: right;\">  energie Acc.</th><th style=\"text-align: right;\">  energie Prec.</th><th style=\"text-align: right;\">  energie Rec.</th><th style=\"text-align: right;\">  energie F1</th><th style=\"text-align: right;\">  kinder Acc.</th><th style=\"text-align: right;\">  kinder Prec.</th><th style=\"text-align: right;\">  kinder Rec.</th><th style=\"text-align: right;\">  kinder F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>lmsys/vicuna-13b-v1.5</td><td style=\"text-align: right;\">          0.600</td><td style=\"text-align: right;\">           0.000</td><td style=\"text-align: right;\">          0.000</td><td style=\"text-align: right;\">        0.000</td><td style=\"text-align: right;\">         0.250</td><td style=\"text-align: right;\">          0.000</td><td style=\"text-align: right;\">         0.000</td><td style=\"text-align: right;\">       0.000</td><td style=\"text-align: right;\">        0.125</td><td style=\"text-align: right;\">         0.000</td><td style=\"text-align: right;\">        0.000</td><td style=\"text-align: right;\">      0.000</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
