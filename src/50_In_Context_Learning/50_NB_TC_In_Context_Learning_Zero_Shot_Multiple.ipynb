{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: In Context Learing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pop529700/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit visibility to only GPU 0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Set the device to GPU 0 if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "SPLIT = \"test\" # \"train\", \"test\", \"holdout\", \"extende\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url_and_content\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"cannabis\", \"energie\", \"kinder\"]\n",
    "models = [\n",
    "    {\n",
    "        \"name\": \"aya-101\",\n",
    "        \"model\": \"CohereForAI/aya-101\",\n",
    "        \"tokenizer_class\": \"AutoTokenizer\",\n",
    "        \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"vicuna-13b\",\n",
    "    #     \"model\": \"lmsys/vicuna-13b-v1.5\",\n",
    "    #     \"tokenizer_class\": \"LlamaTokenizer\",\n",
    "    #     \"model_class\": \"LlamaForCausalLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"vicuna-7b\",\n",
    "    #     \"model\": \"lmsys/vicuna-7b-v1.5\",\n",
    "    #     \"tokenizer_class\": \"LlamaTokenizer\",\n",
    "    #     \"model_class\": \"LlamaForCausalLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"FLAN-t5-base\",\n",
    "    #     \"model\": \"google/flan-t5-base\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"FLAN-t5-large\",\n",
    "    #     \"model\": \"google/flan-t5-large\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"FLAN-t5-xxl\",\n",
    "    #     \"model\": \"google/flan-t5-xxl\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"leo-hessianai-13b\",\n",
    "    #     \"model\": \"LeoLM/leo-hessianai-13b\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForCausalLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"leo-hessianai-7b\",\n",
    "    #     \"model\": \"LeoLM/leo-hessianai-7b\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForCausalLM\"\n",
    "    # },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"LeoLM/leo-hessianai-13b\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"LeoLM/leo-hessianai-13b\", trust_remote_code=True, device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# model_name = \"CohereForAI/aya-101\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "#     model_name, device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"lmsys/vicuna-13b-v1.5\" #\"lmsys/vicuna-13b-v1.5\"\n",
    "\n",
    "# from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "# model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following text in German, does it contain information about 'Cannabis'? Please answer with 'Yes' or 'No' only.\n",
      "\n",
      "Text: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Given the following text in {lang}, does it contain information about '{topic}'? Please answer with 'Yes' or 'No' only.\n",
    "\n",
    "Text: \"{webpage_text}\"\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "prompt_test = PROMPT_TEMPLATE.format(\n",
    "    topic=\"Cannabis\", lang='German', webpage_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.')\n",
    "print(prompt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter influences the text generation in a specific way. Below are the parameters along with a brief explanation:\n",
    "\n",
    "**`max_length`**:\n",
    "* Sets the maximum number of tokens in the generated text (default is 50).\n",
    "* Generation stops if the maximum length is reached before the model produces an EOS token.\n",
    "* A higher `max_length` allows for longer generated texts but may increase the time and computational resources required.\n",
    "\n",
    "**`min_length`**:\n",
    "* Sets the minimum number of tokens in the generated text (default is 10).\n",
    "* Generation continues until this minimum length is reached even if an EOS token is produced.\n",
    "\n",
    "**`num_beams`**:\n",
    "* In beam search, sets the number of \"beams\" or hypotheses to keep at each step (default is 4).\n",
    "* A higher number of beams increases the chances of finding a good output but also increases the computational cost.\n",
    "\n",
    "**`num_return_sequences`**:\n",
    "* Specifies the number of independently computed sequences to return (default is 3).\n",
    "* When using sampling, multiple different sequences are generated independently from each other.\n",
    "\n",
    "**`early_stopping`**:\n",
    "* Stops generation if the model produces the EOS (End Of Sentence) token, even if the predefined maximum length is not reached (default is True).\n",
    "* Useful when an EOS token signifies the logical end of a text (often represented as `</s>`).\n",
    "\n",
    "**`do_sample`**:\n",
    "* Tokens are selected probabilistically based on their likelihood scores (default is True).\n",
    "* Introduces randomness into the generation process for diverse outputs.\n",
    "* The level of randomness is controlled by the 'temperature' parameter.\n",
    "\n",
    "**`temperature`**:\n",
    "* Adjusts the probability distribution used for sampling the next token (default is 0.7).\n",
    "* Higher values make the generation more random, while lower values make it more deterministic.\n",
    "\n",
    "**`top_k`**:\n",
    "* Limits the number of tokens considered for sampling at each step to the top K most likely tokens (default is 50).\n",
    "* Can make the generation process faster and more focused.\n",
    "\n",
    "**`top_p`**:\n",
    "* Also known as nucleus sampling, sets a cumulative probability threshold (default is 0.95).\n",
    "* Tokens are sampled only from the smallest set whose cumulative probability exceeds this threshold.\n",
    "\n",
    "**`repetition_penalty`**:\n",
    "* Discourages the model from repeating the same token by modifying the token's score (default is 1.5).\n",
    "* Values greater than 1.0 penalize repetitions, and values less than 1.0 encourage repetitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "          'early_stopping': True,\n",
    "          # 'max_length': 100,\n",
    "          # 'min_length': 1,\n",
    "          # 'logprobs': 1,\n",
    "          # 'n': 1,\n",
    "          # 'best_of': 1,\n",
    "\n",
    "          # 'num_beam_groups': 2,\n",
    "          'num_beams': 2,\n",
    "          'num_return_sequences': 1,\n",
    "          'max_new_tokens': 1024,\n",
    "          'min_new_tokens': 1,\n",
    "          'output_scores': True,\n",
    "          # 'repetition_penalty': 1.0,\n",
    "          'temperature': 0.6,\n",
    "          'top_k': 50,\n",
    "          'top_p': 1.0\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_prompt(article, template, topic, lang='German'):\n",
    "    \"\"\" Compiles the prompt for the given article and model.\"\"\"\n",
    "\n",
    "    # Extract the article headline and text\n",
    "    article_text = article.get(\"text\")\n",
    "    prompt = template.format(topic=topic, lang=lang, webpage_text=article_text)\n",
    "    # prompt = template.format(topic = \"Cannabis\", lang = 'German', webpage_text=article_text, positive_example=positive_example, negative_example=negative_example)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_input_length(prompt):\n",
    "    \"\"\" Calculates the length of the input sequence for the model. \"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", add_special_tokens=False, truncation=False, padding=False)\n",
    "\n",
    "    # Calculate the length of the input sequence\n",
    "    input_length = tokenized_prompt.input_ids.size(1)\n",
    "\n",
    "    return input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model, tokenizer, prompt, params, remove_input=True):\n",
    "    \"\"\"Generates answers from a language model for a given prompt.\"\"\"\n",
    "\n",
    "    # Encode the prompt and generate the answers\n",
    "    encoded_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    if encoded_input.size()[1] > tokenizer.model_max_length:\n",
    "        print(\"Input too long, truncating.\")\n",
    "        # encoded_input = encoded_input[:, :tokenizer.model_max_length]\n",
    "\n",
    "    generated_outputs = model.generate(encoded_input, **params)\n",
    "\n",
    "    # Decode and clean outputs\n",
    "    outputs = []\n",
    "    input_text_wo_st = tokenizer.decode(\n",
    "        encoded_input[0], skip_special_tokens=True)\n",
    "    for output in generated_outputs:\n",
    "        decoded_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        cleaned_text = decoded_text.replace(input_text_wo_st, \"\").strip()\n",
    "        outputs.append(cleaned_text if remove_input else decoded_text)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(output_text):\n",
    "    \"\"\"Determines if the model's output signifies \"Yes\" (1) or \"No\" (0).\"\"\"\n",
    "    text = output_text.lower()\n",
    "    return 1 if \"yes\" in text else 0 if \"no\" in text else ValueError(\"Ambiguous response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy, precision, recall, and F1 score for the given labels and predictions and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary'),\n",
    "        'recall': recall_score(labels, preds, average='binary'),\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_from_dataset(dataset, n=5, subset='test'):\n",
    "    \"\"\"\n",
    "    Samples n random examples from a specified subset of the dataset.\n",
    "    \"\"\"\n",
    "    n = min(n, len(dataset[subset]))\n",
    "    random_indices = random.sample(range(len(dataset[subset])), n)\n",
    "    sampled_dataset = dataset[subset].select(random_indices)\n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "def load_model_and_tokenizer(model_details):\n",
    "    \"\"\"\n",
    "    Loads a model and its corresponding tokenizer based on the provided model details.\n",
    "    \"\"\"\n",
    "    model_name = model_details['model']\n",
    "    tokenizer_class = model_details['tokenizer_class']\n",
    "    model_class = model_details['model_class']\n",
    "    \n",
    "    # Cohere models and FLAN models\n",
    "    if tokenizer_class == \"AutoTokenizer\" and model_class == \"AutoModelForSeq2SeqLM\":\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "        \n",
    "    # Vicuna models\n",
    "    elif tokenizer_class == \"LlamaTokenizer\" and model_class == \"LlamaForCausalLM\":\n",
    "        from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "        \n",
    "    #  LeoLM models  \n",
    "    elif tokenizer_class == \"AutoTokenizer\" and model_class == \"AutoModelForCausalLM\":\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, device_map=\"auto\", load_in_8bit=True)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Model class not supported.\")\n",
    "        \n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating topic cannabis\n",
      "Loading dataset for cannabis\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory ../data/tmp/processed_dataset_cannabis_buffed_chunkified_random_192 not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_details \u001b[38;5;129;01min\u001b[39;00m models: \u001b[38;5;66;03m#-------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading dataset for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/tmp/processed_dataset_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtopic\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_buffed_chunkified_random_192\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_random_from_dataset(dataset, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Load model\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/datasets/load.py:2233\u001b[0m, in \u001b[0;36mload_from_disk\u001b[0;34m(dataset_path, fs, keep_in_memory, storage_options)\u001b[0m\n\u001b[1;32m   2231\u001b[0m fs, _, _ \u001b[38;5;241m=\u001b[39m fsspec\u001b[38;5;241m.\u001b[39mget_fs_token_paths(dataset_path, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[1;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mexists(dataset_path):\n\u001b[0;32m-> 2233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_INFO_FILENAME)) \u001b[38;5;129;01mand\u001b[39;00m fs\u001b[38;5;241m.\u001b[39misfile(\n\u001b[1;32m   2235\u001b[0m     posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_path, config\u001b[38;5;241m.\u001b[39mDATASET_STATE_JSON_FILENAME)\n\u001b[1;32m   2236\u001b[0m ):\n\u001b[1;32m   2237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39mload_from_disk(dataset_path, keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Directory ../data/tmp/processed_dataset_cannabis_buffed_chunkified_random_192 not found"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results = defaultdict(dict)\n",
    "\n",
    "for topic in topics:  # ----------------------------------------------------------------------\n",
    "    print(f\"Evaluating topic {topic}\")\n",
    "    for model_details in models: #-------------------------------------------------------------\n",
    "        \n",
    "        # Load dataset\n",
    "        print(f\"Loading dataset for {topic}\")\n",
    "        dataset = load_from_disk(f\"../data/tmp/processed_dataset_{topic}_buffed_chunkified_random_192\")\n",
    "        dataset['test'] = sample_random_from_dataset(dataset, n=5, subset='test')\n",
    "        \n",
    "        # Load model\n",
    "        model_name = model_details['model']\n",
    "        print(f\"Loading model {model_name}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        tokenizer, model = load_model_and_tokenizer(model_details)\n",
    "        \n",
    "        # Generate answers\n",
    "        answers = [] \n",
    "        for row in tqdm(dataset['test']): # ---------------------------------------------------\n",
    "            prompt = compile_prompt(row, PROMPT_TEMPLATE, topic)\n",
    "            answers.append(generate_answers(model, tokenizer, prompt, params)[0])\n",
    "\n",
    "        # Add answers to the dataset\n",
    "        dataset['test'] = dataset['test'].add_column(\"answers\", answers)\n",
    "        dataset.save_to_disk(f\"../data/tmp/processed_dataset_{topic}_answers_0s_{model_name.split('/')[1]}\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = calc_metrics(dataset['test']['label'], [parse_response(ans) for ans in answers])\n",
    "        eval_results[model_name][topic] = metrics\n",
    "        \n",
    "        # Clear GPU memory to avoid memory errors\n",
    "        model.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "        del model, tokenizer\n",
    "        gc.collect()  # Explicitly invoking garbage collection\n",
    "        torch.cuda.empty_cache()  # Clear cache again after garbage collection\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '648c2ad98e8cadbd29055709',\n",
       " 'batch_id': 15,\n",
       " 'domain': 'kinder-grund-sicherung.de',\n",
       " 'view_url': 'www.kinder-grund-sicherung.de/',\n",
       " 'lang': 'de',\n",
       " 'text': 'beantragen zu können führt zu einem einfachen Zugang zur Leistung. Auch das Bewilligungsverfahren ist übersichtlich. 4. Wenig Bürokratie Bisherige Sozialleistungen sind in der einen Kindergrundsicherung zusammengefasst. Die Einkommensprüfung ist einfach. 5. Sozialer Einkommensbegriff Durch Verwendung des sozialrechtlichen Einkommensbegriffs im Rahmen der Kindergrundsicherung wird die Existenzsicherung in den Vordergrund gestellt. 6. Vorrang von Unterhaltsleistungen Der Kindergrundsicherung gehen Unterhaltsleistungen und anderer zur Sicherung des Unterhalts bestimmte Sozialleistungen, wie Unterhaltsvorschuss vor. 7. Nachrrang von Bürgergeld Die Kindergrundsicherung geht dem Bürgergeld vor. Es gibt keine doppelte Zuständigkeit von Jobcenter und Familienkasse als Kindergrundsicherungsbehörde. 8. Förderung von Ausbildung und Arbeit Junge Menschen ab 15 Jahre sollen durch Angebote und Förderleistungen zur Eingliederung in Arbeit oder Ausbildung nach dem SGB III oder SGB II begleitet und gefördert werden. Zwei Komponenten der Kindergrundsicherung Die Kindergrundsicherung besteht aus zwei Elementen. Zum einen aus dem einkommensunabhängigen Garantiebetrag, der dem Kindergeld nachfolgt. Zum anderen aus dem altersgestaffelten Zusatzbetrag, der vom Einkommen des Kindes und der Eltern abhängt. Die Kindergrundsicherung ersetzt somit folgende Leistungen : – Kindergeld, – steuerliche Kinderfreibeträge, – Leistungen des Bürgergeldes für Kinder im SGB II, – Leistungen nach dem SGB XVII für Kinder – Leistungen des Asylbewerberleistungsgesetzes für Kinder – Kinderzuschlag nach § 6a Bundeskindergeldgesetz ( BKGG ) – Teile des Bildungs - und Teilhabepaketes nach § 6b BKGG bzw. § 28 SGB II bzw. § 34 SGB XII. Garantiebetrag der Kindergrundsicherung Im Rahmen der Kindergrundsicherung gibt es den einkommensunabhängigen Garantiebetrag für alle Kinder und Jugendliche in gleiche Höhe. Er ist mit dem bisher gezahltem Kindergeld vergleichbar, dass an alle Eltern unabhängig von ihrem Einkommen und Vermögen gezahlt wird. Zusatzbetrag der Kindergrundsicherung Der Zusatzbetrag der Kindergrundsicherung hängt vom Einkommen der Familie ab, in der das Kind lebt. Die Höhe des Zusatzbetrages muss so bemessen sein, dass sie zusammen mit dem Garantiebetrag das kindliche Existenzminimum absichert. Der Zusatzbetrag der Kindergrundsicherung hängt vom Einkommen der Familie ab, in der das Kind lebt. Die Höhe des Zusatzbetrages muss so bemessen sein, dass sie zusammen mit dem Garantiebetrag das kindliche Existenzminimum absichert. Antrag auf Kindergrundsicherung Kinder',\n",
       " 'text_length': 15946,\n",
       " 'word_count': 1989,\n",
       " 'is_topic': True,\n",
       " 'label': 1,\n",
       " 'chunk_id': 3,\n",
       " 'answers': 'Yes'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.834\n",
    "Accuracy: 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path to save the dictionary\n",
    "file_path = \"eval_results_icl_zero_shot.json\"\n",
    "\n",
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path where the JSON data is saved\n",
    "file_path = \"eval_results_icl_zero_shot.json\"\n",
    "\n",
    "# Load the dictionary from the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    eval_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all topics (assuming all models are evaluated on the same topics)\n",
    "topics = list(next(iter(eval_results.values())).keys())\n",
    "\n",
    "# Prepare headers for the table: each topic will have four metrics\n",
    "headers = [\"Model\"] + \\\n",
    "    [f\"{topic} {metric}\" for topic in topics for metric in [\n",
    "        \"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# Prepare rows: one row per model, containing metrics for each topic\n",
    "rows = []\n",
    "for model, topics_metrics in eval_results.items():\n",
    "    row = [model]  # Start with the model name\n",
    "    for topic in topics:\n",
    "        metrics = topics_metrics.get(topic, {})\n",
    "        row.extend([metrics.get('accuracy', 0.0), metrics.get(\n",
    "            'precision', 0.0), metrics.get('recall', 0.0), metrics.get('f1', 0.0)])\n",
    "    rows.append(row)\n",
    "\n",
    "# Generate the HTML table\n",
    "table_html = tabulate(rows, headers=headers, tablefmt=\"html\",\n",
    "                      showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model                </th><th style=\"text-align: right;\">  cannabis Acc.</th><th style=\"text-align: right;\">  cannabis Prec.</th><th style=\"text-align: right;\">  cannabis Rec.</th><th style=\"text-align: right;\">  cannabis F1</th><th style=\"text-align: right;\">  energie Acc.</th><th style=\"text-align: right;\">  energie Prec.</th><th style=\"text-align: right;\">  energie Rec.</th><th style=\"text-align: right;\">  energie F1</th><th style=\"text-align: right;\">  kinder Acc.</th><th style=\"text-align: right;\">  kinder Prec.</th><th style=\"text-align: right;\">  kinder Rec.</th><th style=\"text-align: right;\">  kinder F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>CohereForAI/aya-101  </td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">           1.000</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       1.000</td><td style=\"text-align: right;\">        0.600</td><td style=\"text-align: right;\">         0.000</td><td style=\"text-align: right;\">        0.000</td><td style=\"text-align: right;\">      0.000</td></tr>\n",
       "<tr><td>lmsys/vicuna-13b-v1.5</td><td style=\"text-align: right;\">          0.800</td><td style=\"text-align: right;\">           1.000</td><td style=\"text-align: right;\">          0.500</td><td style=\"text-align: right;\">        0.667</td><td style=\"text-align: right;\">         0.800</td><td style=\"text-align: right;\">          0.667</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       0.800</td><td style=\"text-align: right;\">        0.800</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        0.667</td><td style=\"text-align: right;\">      0.800</td></tr>\n",
       "<tr><td>lmsys/vicuna-7b-v1.5 </td><td style=\"text-align: right;\">          0.400</td><td style=\"text-align: right;\">           0.000</td><td style=\"text-align: right;\">          0.000</td><td style=\"text-align: right;\">        0.000</td><td style=\"text-align: right;\">         0.400</td><td style=\"text-align: right;\">          0.400</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       0.571</td><td style=\"text-align: right;\">        0.800</td><td style=\"text-align: right;\">         0.800</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">      0.889</td></tr>\n",
       "<tr><td>google/flan-t5-base  </td><td style=\"text-align: right;\">          0.800</td><td style=\"text-align: right;\">           1.000</td><td style=\"text-align: right;\">          0.667</td><td style=\"text-align: right;\">        0.800</td><td style=\"text-align: right;\">         0.600</td><td style=\"text-align: right;\">          0.500</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       0.667</td><td style=\"text-align: right;\">        0.800</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        0.667</td><td style=\"text-align: right;\">      0.800</td></tr>\n",
       "<tr><td>google/flan-t5-large </td><td style=\"text-align: right;\">          0.800</td><td style=\"text-align: right;\">           1.000</td><td style=\"text-align: right;\">          0.500</td><td style=\"text-align: right;\">        0.667</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">          0.000</td><td style=\"text-align: right;\">         0.000</td><td style=\"text-align: right;\">       0.000</td><td style=\"text-align: right;\">        0.600</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        0.333</td><td style=\"text-align: right;\">      0.500</td></tr>\n",
       "<tr><td>google/flan-t5-xxl   </td><td style=\"text-align: right;\">          0.800</td><td style=\"text-align: right;\">           0.000</td><td style=\"text-align: right;\">          0.000</td><td style=\"text-align: right;\">        0.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       1.000</td><td style=\"text-align: right;\">        0.800</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        0.750</td><td style=\"text-align: right;\">      0.857</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
