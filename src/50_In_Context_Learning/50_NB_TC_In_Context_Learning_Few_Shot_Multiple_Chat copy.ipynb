{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier: In Context Learing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pop529700/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Limit visibility to only GPU 0\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# # Set the device to GPU 0 if available\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "SPLIT = \"test\" # \"train\", \"test\", \"holdout\", \"extende\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url_and_content\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMONSTR_SAMPLING = \"random\" \n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"energie\", \"kinder\"]\n",
    "MODELS = [\n",
    "    # {\n",
    "    #     \"name\": \"aya-101\",\n",
    "    #     \"model\": \"CohereForAI/aya-101\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"vicuna-13b\",\n",
    "    #     \"model\": \"lmsys/vicuna-13b-v1.5\",\n",
    "    #     \"tokenizer_class\": \"LlamaTokenizer\",\n",
    "    #     \"model_class\": \"LlamaForCausalLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #      \"name\": \"vicuna-7b\",\n",
    "    #      \"model\": \"lmsys/vicuna-7b-v1.5\",\n",
    "    #      \"tokenizer_class\": \"LlamaTokenizer\",\n",
    "    #      \"model_class\": \"LlamaForCausalLM\"\n",
    "    #  },\n",
    "    #  {\n",
    "    #      \"name\": \"FLAN-t5-base\",\n",
    "    #      \"model\": \"google/flan-t5-base\",\n",
    "    #      \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #      \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    #  },\n",
    "    #  {\n",
    "    #      \"name\": \"FLAN-t5-large\",\n",
    "    #      \"model\": \"google/flan-t5-large\",\n",
    "    #      \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #      \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    #  },\n",
    "    #  {\n",
    "    #      \"name\": \"FLAN-t5-xxl\",\n",
    "    #         \"model\": \"google/flan-t5-xxl\",\n",
    "    #      \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #      \"model_class\": \"AutoModelForSeq2SeqLM\"\n",
    "    #  },\n",
    "    # {\n",
    "    #     \"name\": \"leo-hessianai-13b-chat\",\n",
    "    #     \"model\": \"LeoLM/leo-hessianai-13b-chat\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForCausalLM\"\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"leo-hessianai-7b-chat\",\n",
    "    #     \"model\": \"LeoLM/leo-hessianai-7b-chat\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForCausalLM\"\n",
    "    # },\n",
    "    # {\n",
    "    # \"name\": \"leo-hessianai-7b-chat-bilingual\",\n",
    "    # \"model\": \"LeoLM/leo-hessianai-7b-chat-bilingual\",\n",
    "    # \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    # \"model_class\": \"AutoModelForCausalLM\"\n",
    "    # },\n",
    "    {\n",
    "    \"name\": \"leo-mistral-hessianai-7b-chat\",\n",
    "    \"model\": \"LeoLM/leo-mistral-hessianai-7b-chat\",\n",
    "    \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    \"model_class\": \"AutoModelForCausalLM\"\n",
    "    },\n",
    "    # {\n",
    "    #     \"name\": \"aya-23-8B\",\n",
    "    #     \"model\": \"CohereForAI/aya-23-8B\",\n",
    "    #     \"tokenizer_class\": \"AutoTokenizer\",\n",
    "    #     \"model_class\": \"AutoModelForCausalLM\"\n",
    "    # },\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Prompt Template:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topic Desciptions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_desciptions = {\n",
    "    \"kinder\": {\n",
    "        \"name\": \"Kindergrundsicherung\",\n",
    "        \"description\": \"Die Kindergrundsicherung zielt darauf ab, Kinderarmut zu bekämpfen, indem sie einen festen Betrag, einkommensabhängige Zuschläge und Bildungsleistungen bereitstellt.\",\n",
    "        \"keywords\": ['kinder', 'kindergr', 'paus', 'familie', 'bundestag.de', 'arbeitsagentur.de', 'kindergrundsicherung',  'kindergeld', 'kindersicherung', 'kinderzuschlag', 'gesetz']\n",
    "    },\n",
    "    \"cannabis\": {\n",
    "        \"name\": \"die Legalisierung von Cannabis\",\n",
    "        \"description\": \"Das CanG 2023 (Cannabisgesetz) wird den privaten Anbau von Cannabis durch Erwachsene für den persönlichen Gebrauch und den kollektiven nicht-kommerziellen Anbau legalisieren.\",\n",
    "        \"keywords\": ['cannabis', 'canabis', 'cannabic', 'gras', 'cbd' , 'droge', 'hanf', 'thc', 'canbe', 'legal', 'legalisierung', 'gesetz', 'verein', 'entkriminali']\n",
    "    },\n",
    "    \"energie\": {\n",
    "        \"name\": \"erneuerbare Energiequellen\",\n",
    "        \"description\": \"Das EEG 2023 (Erneuerbare-Energien-Gesetz) zielt darauf ab, den Anteil erneuerbarer Energien am Bruttostromverbrauch bis 2030 auf mindestens 80% zu erhöhen.\",\n",
    "        \"keywords\": ['energie', 'eeg','grün','gruen','habeck', 'climate', 'strom','Waerme','wende','frderung', 'förderung', 'windkraft', 'windrad', 'photovoltaik',\n",
    "                     'photovoltaic', 'solar', 'heizung', 'heiz', 'gesetz', 'erneuer', 'geothermie', 'pv', 'geg']\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt Templates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### ZERO SHOT ###############################\n",
    "\n",
    "PROMPT_TEMPLATE_ZERO_SHOT = \"\"\"Klassifiziere den folgende Webseite auf {lang} als themenbezogen oder nicht themenbezogen. Enthält er Informationen über '{topic}'? Bitte antworte nur mit 'Ja' oder 'Nein'.\n",
    "\n",
    "Themenbeschreibung: {topic_desc}\n",
    "Themen-Schlüsselwörter: {topic_keyw}\n",
    "\n",
    "Webseite:\n",
    "URL: '''{url}'''\n",
    "Text: '''{webpage_text}'''\n",
    "\n",
    "Enthält dieser Text relevante Informationen über '{topic}'?\n",
    "\"\"\"\n",
    "\n",
    "############################### FEW SHOT ###############################\n",
    "\n",
    "PROMPT_TEMPLATE_FEW_SHOT = \"\"\"Klassifiziere die folgende Webseite auf {lang} als themenbezogen oder nicht themenbezogen. Enthält er Informationen über '{topic}'? Bitte antworte nur mit 'Ja' oder 'Nein'.\n",
    "\n",
    "Themenbeschreibung: {topic_desc}\n",
    "Themen-Schlüsselwörter: {topic_keyw}\n",
    "\n",
    "Beispiele:\n",
    "{examples}\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "Webseite :\n",
    "URL: '''{url}'''\n",
    "Text: '''{webpage_text}'''\n",
    "\n",
    "Enthält diese Webseite relevante Informationen über '{topic}'?\n",
    "\"\"\"\n",
    "\n",
    "############################### DEMONSTRATOR ###############################\n",
    "\n",
    "PROMPT_TEMPLATE_EXAMPLES = \"\"\"\n",
    "URL: '''{url}'''\n",
    "Text: '''{text}'''\n",
    "Antwort: '{label}'\n",
    "\"\"\"\n",
    "\n",
    "############################### System Prompt ###############################\n",
    "\n",
    "SYTEM_PROMPT_TEMPLATE = \"\"\"Du bist ein fortschrittliches Sprachmodell, das darauf ausgelegt ist, Texte entweder als \"themenbezogen\" oder \"nicht themenbezogen\" zu klassifizieren.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test prompt templates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klassifiziere die folgende Webseite auf German als themenbezogen oder nicht themenbezogen. Enthält er Informationen über 'die Legalisierung von Cannabis'? Bitte antworte nur mit 'Ja' oder 'Nein'.\n",
      "\n",
      "Themenbeschreibung: Das CanG 2023 (Cannabisgesetz) wird den privaten Anbau von Cannabis durch Erwachsene für den persönlichen Gebrauch und den kollektiven nicht-kommerziellen Anbau legalisieren.\n",
      "Themen-Schlüsselwörter: cannabis, canabis, cannabic, gras, cbd, droge, hanf, thc, canbe, legal, legalisierung, gesetz, verein, entkriminali\n",
      "\n",
      "Beispiele:\n",
      "\n",
      "URL: '''google.de'''\n",
      "Text: '''Cannabis ist eine Droge.'''\n",
      "Antwort: 'Yes'\n",
      "\n",
      "URL: '''example.com'''\n",
      "Text: '''Katzen sind Tiere.'''\n",
      "Antwort: 'No'\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n",
      "Webseite :\n",
      "URL: '''example.com'''\n",
      "Text: '''Lorem ipsum dolor sit amet, consectetur adipiscing elit.'''\n",
      "\n",
      "Enthält diese Webseite relevante Informationen über 'die Legalisierung von Cannabis'?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example text\n",
    "example_list = [\n",
    "    {\n",
    "        \"text\": \"Cannabis ist eine Droge.\",\n",
    "        \"view_url\": \"google.de\",\n",
    "        \"label\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Katzen sind Tiere.\",\n",
    "        \"view_url\": \"example.com\",\n",
    "        \"label\": \"No\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_prompt_list = [PROMPT_TEMPLATE_EXAMPLES.format(text=example[\"text\"], url=example[\"view_url\"],  label=example[\"label\"]) for example in example_list]\n",
    "\n",
    "# print(\"Example prompt list:\")\n",
    "# print(\"\\n\".join(example_prompt_list))\n",
    "\n",
    "topic_desciption = topic_desciptions[\"cannabis\"]\n",
    "topic_name = topic_desciption.get(\"name\")\n",
    "topic_desc = topic_desciption.get(\"description\")\n",
    "topic_keyw = topic_desciption.get(\"keywords\")\n",
    "\n",
    "\n",
    "# Test the template with a dummy text\n",
    "prompt_test = PROMPT_TEMPLATE_FEW_SHOT.format(topic = topic_name, lang = 'German', url = \"example.com\",  topic_desc = topic_desc, topic_keyw = \", \".join(topic_keyw),  webpage_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.', examples=\"\".join(example_prompt_list))\n",
    "print(prompt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LeoLM/leo-hessianai-7b-chat\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32005,  1788,    13, 29928, 29884,   289,   391,  1011,  5162,   816,\n",
      "         12123, 20048,  1706, 10221,  1545,   514, 29892,  1697, 19118, 17691,\n",
      "         16260,  1752, 29892,  8490,   371,   875, 29893,  2447,  1620,   376,\n",
      "           386, 12398, 15325,  6352, 29908,  4461,   376, 29876,  1428,   963,\n",
      "           264, 15325,  6352, 29908,  1729, 22902, 21722,  7884, 29889, 32006,\n",
      "         29871,    13, 32005,  1404,    13, 29968,   605,   361, 10077,   406,\n",
      "           762,   900, 11693,  2563, 11485,  1622,  5332,  1620,   963,   264,\n",
      "         15325,  6352,  4461,  3072,   963,   264, 15325,  6352, 29889,  1174,\n",
      "           386, 10905,   604, 10343,   264,  2939,   525, 16217,  5682,  5711,\n",
      "          7263,  1005,   315,   812,   370,   275, 29915, 29973, 19064,  3677,\n",
      "         29893, 10069,  5595,  1380,   525, 29967, 29874, 29915,  4461,   525,\n",
      "          8139,   262,  4286,    13,    13,  1349, 12398, 29890,  1968, 20989,\n",
      "         29901,  3713,  1815, 29954, 29871, 29906, 29900, 29906, 29941,   313,\n",
      "         29907,   812,   370,   275,  2710,  6618, 29897,  4296,   972,  5999,\n",
      "          2579,   530,  9738,  1005,   315,   812,   370,   275,  3494,  1425,\n",
      "         21686, 29879,  1600,  1865,   972,  3736,  3042,  4057,  6882,   336,\n",
      "           987,   563,   972, 14276,   280,  1193,  5428,  3072, 29899,  7218,\n",
      "          1050,  2526,  4919,   530,  9738,  2814,  5711,  7884, 29889,    13,\n",
      "          1349, 12398, 29899,  4504, 29880, 10673,   295, 29893,  1340,   357,\n",
      "         29901,   508,  7183,   275, 29892,   508,   370,   275, 29892,   508,\n",
      "          7183,   293, 29892,   867,   294, 29892,   274,  6448, 29892,  4441,\n",
      "           479, 29892,  5905, 29888, 29892,   266, 29883, 29892,   508,   915,\n",
      "         29892, 11706, 29892,  2814,  5711,  7263, 29892,  6300,  6618, 29892,\n",
      "         24269,   262, 29892,   875, 29895, 28479, 29875,    13,    13,  3629,\n",
      "         11936,  9152, 29901,    13,    13,  4219, 29901, 14550,  3608, 29889,\n",
      "           311, 12008,    13,  1626, 29901, 14550, 29907,   812,   370,   275,\n",
      "          1752,  2128, 22938,   479, 29889, 12008,    13, 13448, 17572, 29901,\n",
      "           525,  8241, 29915,    13,    13,  4219, 29901, 14550,  4773, 29889,\n",
      "           510, 12008,    13,  1626, 29901, 14550, 29968,   271,  2256,  3937,\n",
      "           323, 10883, 29889, 12008,    13, 13448, 17572, 29901,   525,  3782,\n",
      "         29915,    13,    13,    13, 26589,    13,    13,    13,  3609, 11485,\n",
      "           584,    13,  4219, 29901, 14550,  4773, 29889,   510, 12008,    13,\n",
      "          1626, 29901, 14550, 29931,  3668, 23421, 22224,  7845, 27315, 29892,\n",
      "          5178,   312,   300,   332,   594,   666,   275,  3277,   560,   277,\n",
      "         29889, 12008,    13,    13,  2369,   386, 10905, 10009,  2563, 11485,\n",
      "         29527,  1647, 10343,   264,  2939,   525, 16217,  5682,  5711,  7263,\n",
      "          1005,   315,   812,   370,   275, 29915, 29973,    13, 32006, 29871,\n",
      "            13, 32005, 20255,    13]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_prompt_to_chat(tokenizer, SYTEM_PROMPT_TEMPLATE, prompt):\n",
    "    \"\"\"\n",
    "    Convert a prompt to a chat format\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYTEM_PROMPT_TEMPLATE},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "    #return tokenizer.decode(input_ids[0])\n",
    "\n",
    "chat_prompt = convert_prompt_to_chat(tokenizer, SYTEM_PROMPT_TEMPLATE, prompt_test)\n",
    "print(chat_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameter for Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each parameter influences the text generation in a specific way. Below are the parameters along with a brief explanation:\n",
    "\n",
    "**`max_length`**:\n",
    "* Sets the maximum number of tokens in the generated text (default is 50).\n",
    "* Generation stops if the maximum length is reached before the model produces an EOS token.\n",
    "* A higher `max_length` allows for longer generated texts but may increase the time and computational resources required.\n",
    "\n",
    "**`min_length`**:\n",
    "* Sets the minimum number of tokens in the generated text (default is 10).\n",
    "* Generation continues until this minimum length is reached even if an EOS token is produced.\n",
    "\n",
    "**`num_beams`**:\n",
    "* In beam search, sets the number of \"beams\" or hypotheses to keep at each step (default is 4).\n",
    "* A higher number of beams increases the chances of finding a good output but also increases the computational cost.\n",
    "\n",
    "**`num_return_sequences`**:\n",
    "* Specifies the number of independently computed sequences to return (default is 3).\n",
    "* When using sampling, multiple different sequences are generated independently from each other.\n",
    "\n",
    "**`early_stopping`**:\n",
    "* Stops generation if the model produces the EOS (End Of Sentence) token, even if the predefined maximum length is not reached (default is True).\n",
    "* Useful when an EOS token signifies the logical end of a text (often represented as `</s>`).\n",
    "\n",
    "**`do_sample`**:\n",
    "* Tokens are selected probabilistically based on their likelihood scores (default is True).\n",
    "* Introduces randomness into the generation process for diverse outputs.\n",
    "* The level of randomness is controlled by the 'temperature' parameter.\n",
    "\n",
    "**`temperature`**:\n",
    "* Adjusts the probability distribution used for sampling the next token (default is 0.7).\n",
    "* Higher values make the generation more random, while lower values make it more deterministic.\n",
    "\n",
    "**`top_k`**:\n",
    "* Limits the number of tokens considered for sampling at each step to the top K most likely tokens (default is 50).\n",
    "* Can make the generation process faster and more focused.\n",
    "\n",
    "**`top_p`**:\n",
    "* Also known as nucleus sampling, sets a cumulative probability threshold (default is 0.95).\n",
    "* Tokens are sampled only from the smallest set whose cumulative probability exceeds this threshold.\n",
    "\n",
    "**`repetition_penalty`**:\n",
    "* Discourages the model from repeating the same token by modifying the token's score (default is 1.5).\n",
    "* Values greater than 1.0 penalize repetitions, and values less than 1.0 encourage repetitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "          #'early_stopping': True,\n",
    "          #'num_beams': 5,\n",
    "          #'num_return_sequences': 5,\n",
    "          #'max_new_tokens': 128,\n",
    "          #'min_new_tokens': 1,\n",
    "          #'output_scores': True,\n",
    "          # 'repetition_penalty': 1.0,\n",
    "          'max_length': 8192,\n",
    "          'temperature': 0.3,\n",
    "          #'top_k': 50,\n",
    "          'top_p': 0.95\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_prompt(article, template, template_example, topic, topic_desciptions, examples, lang='German'):\n",
    "    \"\"\" Compiles the prompt for the given article and model.\"\"\"\n",
    "\n",
    "    # Get the topic description and keywords\n",
    "    topic_desciption = topic_desciptions[topic]\n",
    "    topic_name = topic_desciption.get(\"name\")\n",
    "    topic_desc = topic_desciption.get(\"description\")\n",
    "    topic_keyw = topic_desciption.get(\"keywords\")\n",
    "    \n",
    "    # Get the text of the article\n",
    "    article_text = article.get(\"text\")\n",
    "    article_lang = article.get(\"lang\")\n",
    "    article_url = article.get(\"view_url\")\n",
    "    example_prompts = [template_example.format(text=example[\"text\"], url=example[\"view_url\"], label= \"Ja\" if example[\"label\"] == 1 else \"Nein\") for example in examples]\n",
    "    prompt = template.format(topic = topic_name, url = article_url,  topic_desc = topic_desc, topic_keyw = topic_keyw,\n",
    "                             lang = article_lang, webpage_text=article_text, examples=\"\".join(example_prompts))\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klassifiziere die folgende Webseite auf de als themenbezogen oder nicht themenbezogen. Enthält er Informationen über 'die Legalisierung von Cannabis'? Bitte antworte nur mit 'Ja' oder 'Nein'.\n",
      "\n",
      "Themenbeschreibung: Das CanG 2023 (Cannabisgesetz) wird den privaten Anbau von Cannabis durch Erwachsene für den persönlichen Gebrauch und den kollektiven nicht-kommerziellen Anbau legalisieren.\n",
      "Themen-Schlüsselwörter: ['cannabis', 'canabis', 'cannabic', 'gras', 'cbd', 'droge', 'hanf', 'thc', 'canbe', 'legal', 'legalisierung', 'gesetz', 'verein', 'entkriminali']\n",
      "\n",
      "Beispiele:\n",
      "\n",
      "URL: '''google.de'''\n",
      "Text: '''Cannabis ist eine Droge.'''\n",
      "Antwort: 'Nein'\n",
      "\n",
      "URL: '''example.com'''\n",
      "Text: '''Katzen sind Tiere.'''\n",
      "Antwort: 'Nein'\n",
      "\n",
      "\n",
      "-------\n",
      "\n",
      "\n",
      "Webseite :\n",
      "URL: '''test.com'''\n",
      "Text: '''Lorem Ipsum'''\n",
      "\n",
      "Enthält diese Webseite relevante Informationen über 'die Legalisierung von Cannabis'?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "article = {\"view_url\": \"test.com\", \"text\": \"Lorem Ipsum\", \"lang\": \"de\"}\n",
    "exmaple_prompt = compile_prompt(article, PROMPT_TEMPLATE_FEW_SHOT, PROMPT_TEMPLATE_EXAMPLES, \"cannabis\", topic_desciptions, example_list, lang='German')\n",
    "print(exmaple_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_input_length(prompt):\n",
    "    \"\"\" Calculates the length of the input sequence for the model. \"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokenized_prompt = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", add_special_tokens=False, truncation=False, padding=False)\n",
    "\n",
    "    # Calculate the length of the input sequence\n",
    "    input_length = tokenized_prompt.input_ids.size(1)\n",
    "\n",
    "    return input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model, tokenizer, prompt, params, remove_input=True):\n",
    "    \"\"\"Generates answers from a language model for a given prompt.\"\"\"\n",
    "\n",
    "    # Encode the prompt and generate the answers\n",
    "    # prompt = convert_prompt_to_chat(tokenizer, SYTEM_PROMPT_TEMPLATE, prompt)\n",
    "    # encoded_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # messages = [\n",
    "    #     {\"role\": \"system\", \"content\": SYTEM_PROMPT_TEMPLATE},\n",
    "    #     {\"role\": \"user\", \"content\": prompt}\n",
    "    # ]\n",
    "    # encoded_input = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    prompt_template = \"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{user_prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(system_prompt=SYTEM_PROMPT_TEMPLATE, user_prompt=prompt)\n",
    "    encoded_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    if encoded_input.size()[1] > tokenizer.model_max_length:\n",
    "        print(\"Input too long, truncating.\")\n",
    "        # encoded_input = encoded_input[:, :tokenizer.model_max_length]\n",
    "\n",
    "    generated_outputs = model.generate(encoded_input, pad_token_id=tokenizer.eos_token_id, **params)\n",
    "    \n",
    "    #print(\"Generated outputs:\", tokenizer.decode(generated_outputs[0], skip_special_tokens=True))\n",
    "\n",
    "    # Decode and clean outputs\n",
    "    outputs = []\n",
    "    input_ids = encoded_input[0].tolist()\n",
    "\n",
    "    for output in generated_outputs:\n",
    "        output_ids = output.tolist()\n",
    "\n",
    "        # Find the position where the input ends in the output\n",
    "        input_length = len(input_ids)\n",
    "        output_length = len(output_ids)\n",
    "        \n",
    "        # Ensure we don't go out of bounds\n",
    "        if input_length < output_length and output_ids[:input_length] == input_ids:\n",
    "            cleaned_text_ids = output_ids[input_length:]\n",
    "        else:\n",
    "            cleaned_text_ids = output_ids\n",
    "        \n",
    "        # Decode the cleaned output\n",
    "        cleaned_text = tokenizer.decode(cleaned_text_ids, skip_special_tokens=True).strip()\n",
    "        outputs.append(cleaned_text)\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(output_text):\n",
    "    \"\"\"Determines if the model's output signifies \"Yes\" (1) or \"No\" (0).\"\"\"\n",
    "    text = output_text.lower()\n",
    "    return 1 if \"ja\" in text else 0 if \"nein\" in text else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy, precision, recall, and F1 score for the given labels and predictions and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary'),\n",
    "        'recall': recall_score(labels, preds, average='binary'),\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_from_dataset(dataset, n=5, subset='test'):\n",
    "    \"\"\"\n",
    "    Samples n random examples from a specified subset of the dataset.\n",
    "    \"\"\"\n",
    "    n = min(n, len(dataset[subset]))\n",
    "    random_indices = random.sample(range(len(dataset[subset])), n)\n",
    "    sampled_dataset = dataset[subset].select(random_indices)\n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_details):\n",
    "    \"\"\"\n",
    "    Loads a model and its corresponding tokenizer based on the provided model details.\n",
    "    \"\"\"\n",
    "    model_name = model_details['model']\n",
    "    tokenizer_class = model_details['tokenizer_class']\n",
    "    model_class = model_details['model_class']\n",
    "    \n",
    "    # Cohere models and FLAN models\n",
    "    if tokenizer_class == \"AutoTokenizer\" and model_class == \"AutoModelForSeq2SeqLM\":\n",
    "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "        \n",
    "    # Vicuna models\n",
    "    elif tokenizer_class == \"LlamaTokenizer\" and model_class == \"LlamaForCausalLM\":\n",
    "        from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "        model = LlamaForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)\n",
    "        \n",
    "    #  LeoLM models  \n",
    "    elif tokenizer_class == \"AutoTokenizer\" and model_class == \"AutoModelForCausalLM\":\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True, trust_remote_code=False)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Model class not supported.\")\n",
    "        \n",
    "    return tokenizer, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Different Sample Methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "\n",
    "################### Random Sampling ############################\n",
    "\n",
    "def sample_examples_random(dataset, k=2):\n",
    "    \"\"\"Samples k pairs of examples completely at random.\"\"\"\n",
    "    dataset_sampled = dataset.shuffle().select(range(k))\n",
    "    return [example for example in dataset_sampled]\n",
    "\n",
    "\n",
    "################### Random Sampling balanced ###################\n",
    "\n",
    "def sample_examples_random_balanced(dataset, k=2):\n",
    "    \"\"\"Samples k pairs of examples, each pair containing one positive and one negative example.\"\"\"\n",
    "    # Separate the dataset into positive and negative examples\n",
    "    positive_examples = [example for example in dataset if example['label'] == 1]\n",
    "    negative_examples = [example for example in dataset if example['label'] == 0]\n",
    "    \n",
    "    # Sample k examples from each subset\n",
    "    sampled_positive = sample(positive_examples, k)\n",
    "    sampled_negative = sample(negative_examples, k)\n",
    "    \n",
    "    # Alternate between positive and negative examples to create pairs\n",
    "    examples = []\n",
    "    for idx in range(k):\n",
    "        if idx % 2 == 0:\n",
    "            examples.append(sampled_positive[idx])\n",
    "        else:\n",
    "            examples.append(sampled_negative[idx])\n",
    "    \n",
    "    return examples\n",
    "\n",
    "\n",
    "################### KNN Sampling ############################\n",
    "\n",
    "def sample_examples_knn(model, index, query, dataset, k=2):\n",
    "    inferred_vector = model.encode(query, convert_to_tensor=True, show_progress_bar = False)\n",
    "    sims = index.get_nns_by_vector(inferred_vector, k, search_k=-1, include_distances=False)\n",
    "    return [dataset[idx] for idx in sims]\n",
    "\n",
    "################### Expert Sampling ############################\n",
    "\n",
    "# def sample_from_expert(curated_examples, topic, k=2):\n",
    "    \n",
    "#     curated_examples_topic = curated_examples[topic]\n",
    "#     sampled_positive = sample(curated_examples_topic['positive_examples'], k)\n",
    "#     sampled_negative = sample(curated_examples_topic['negative_examples'], k)\n",
    "    \n",
    "#     # Alternate between positive and negative examples to create pairs\n",
    "#     examples = []\n",
    "#     for idx in range(k):\n",
    "#         if idx % 2 == 0:\n",
    "#             examples.append(sampled_positive[idx])\n",
    "#         else:\n",
    "#             examples.append(sampled_negative[idx])\n",
    "    \n",
    "#     return examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Encoder for KNN Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transformer-based model\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "encoder = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode texts to embeddings\n",
    "def encode_to_embedding(example):\n",
    "    example['embeddings'] = encoder.encode(example['text'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority vote is: Apple\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def majority_voting(answers):\n",
    "    \"\"\"Apply majority voting to a list of arbitrary classification answers.\"\"\"\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common(2)  # Get the two most common answers\n",
    "    \n",
    "    if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:\n",
    "        return \"Tie\"\n",
    "    return most_common[0][0]\n",
    "\n",
    "# Example usage with arbitrary labels\n",
    "answers = [\"Apple\", \"Banana\", \"Apple\", \"Orange\"]\n",
    "print(f\"The majority vote is: {majority_voting(answers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k = 2\n",
    "# examples = sample_examples_random(dataset['train'], k) \n",
    "# examples = sample_examples_random_balanced(dataset['train'], k)\n",
    "# examples = sample_examples_knn(dataset['train'], k)\n",
    "# examples = sample_from_expert(curated_examples, topic, k)\n",
    "# sample_examples_knn(model, article_index, dataset[\"train\"][0][\"text\"], dataset[\"train\"], k)\n",
    "#print(\"Examples: \", examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model LeoLM/leo-mistral-hessianai-7b-chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for cannabis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [26:52<00:00,  3.18s/it]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3815/3815 [00:00<00:00, 403766.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 507/507 [00:00<00:00, 107339.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33702/33702 [00:00<00:00, 543887.78 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 224737/224737 [00:00<00:00, 593457.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers: [['Nein'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein'], ['Nein'], [\"Nein, die Webseite enthält keine relevante Informationen über 'die Legalisierung von Cannabis'.\"], ['Nein.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Nein, die Webseite enthält keine Informationen über die Legalisierung von Cannabis.'], ['Nein, diese Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein'], ['Nein, die Webseite enthält keine Informationen über die Legalisierung von Cannabis.'], ['Nein'], ['Nein.'], ['Nein'], ['Nein'], ['Nein.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Nein, die Webseite enthält keine relevante Informationen über die Legalisierung von Cannabis.'], ['Ja'], [\"Nein, die Webseite enthält keine Informationen über 'die Legalisierung von Cannabis'.\"], ['Nein.'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein'], ['Nein'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein.'], [\"Nein, die gegebene Webseite enthält keine relevante Informationen über 'die Legalisierung von Cannabis'.\"], ['Nein'], ['Nein'], ['Nein'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein.'], ['Nein'], [\"Nein, die Webseite enthält keine relevanten Informationen über 'die Legalisierung von Cannabis'.\"], ['Nein.'], ['Nein'], ['Nein.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein.'], ['Ja'], ['Nein'], ['Nein.'], ['Nein'], ['Nein'], ['Nein, die Webseite enthält keine Informationen über die Legalisierung von Cannabis.'], ['Nein, die Webseite enthält keine Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein'], ['Nein.'], ['Nein. Die Webseite enthält keine Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Nein.'], ['Nein.'], ['Nein, die Webseite enthält keine Informationen über die Legalisierung von Cannabis.'], [\"Nein, diese Webseite enthält keine relevante Informationen über 'die Legalisierung von Cannabis'.\"], ['Nein'], ['Nein.'], ['Nein.'], ['Nein'], ['Nein, die Webseite enthält keine Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Ja'], ['Nein. Der Text enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Nein.'], ['Ja'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Ja'], ['Nein.'], ['Nein'], ['Nein.'], ['Ja'], ['Ja'], ['Ja'], ['Nein'], ['Nein.'], ['Nein'], ['Nein'], ['Nein.'], ['Nein'], ['Nein'], ['Nein'], ['Nein'], ['Nein'], ['Nein'], ['Nein.'], ['Nein'], ['Nein'], ['Nein.'], ['Nein'], ['Nein.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein, die Webseite enthält keine relevanten Informationen über die Legalisierung von Cannabis.'], ['Nein.'], ['Nein.'], ['Nein.'], ['Ja'], ['Nein'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Nein'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Nein'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Nein.'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Nein.'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja'], ['Ja']]\n",
      "Metrics for LeoLM/leo-mistral-hessianai-7b-chat: {'accuracy': 0.9743589743589743, 'precision': 0.975975975975976, 'recall': 0.9848484848484849, 'f1': 0.9803921568627451}\n",
      "Loading dataset for energie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 64/579 [04:53<39:19,  4.58s/it] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m prompt_template \u001b[38;5;241m=\u001b[39m PROMPT_TEMPLATE_FEW_SHOT \u001b[38;5;28;01mif\u001b[39;00m K \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m PROMPT_TEMPLATE_ZERO_SHOT\n\u001b[1;32m     56\u001b[0m prompt \u001b[38;5;241m=\u001b[39m compile_prompt(row, prompt_template, PROMPT_TEMPLATE_EXAMPLES, topic, topic_desciptions, examples)\n\u001b[0;32m---> 57\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#print(\"Label:\", row[\"label\"])\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#print(\"Answers:\", answer)\u001b[39;00m\n\u001b[1;32m     60\u001b[0m answers\u001b[38;5;241m.\u001b[39mappend(answer)\n",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m, in \u001b[0;36mgenerate_answers\u001b[0;34m(model, tokenizer, prompt, params, remove_input)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput too long, truncating.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# encoded_input = encoded_input[:, :tokenizer.model_max_length]\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m generated_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#print(\"Generated outputs:\", tokenizer.decode(generated_outputs[0], skip_special_tokens=True))\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Decode and clean outputs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/generation/utils.py:2801\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2798\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2800\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2804\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2805\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2809\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1009\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1006\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1009\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1022\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:897\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    887\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    888\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    889\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m         use_cache,\n\u001b[1;32m    895\u001b[0m     )\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 897\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:626\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    625\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 626\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:298\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    296\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[0;32m--> 298\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[1;32m    301\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:687\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 687\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCxB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;66;03m# we converted 8-bit row major to turing/ampere format in the first inference pass\u001b[39;00m\n\u001b[1;32m    692\u001b[0m         \u001b[38;5;66;03m# we no longer need the row-major weight\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:562\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    561\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:327\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    326\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 327\u001b[0m CA, CAt, SCA, SCAt, coo_tensorA \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m coo_tensorA \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m state\u001b[38;5;241m.\u001b[39mhas_fp16_weights:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/bitsandbytes/functional.py:2191\u001b[0m, in \u001b[0;36mdouble_quant\u001b[0;34m(A, col_stats, row_stats, out_col, out_row, threshold)\u001b[0m\n\u001b[1;32m   2189\u001b[0m is_on_gpu([A, col_stats, row_stats, out_col, out_row])\n\u001b[1;32m   2190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m-> 2191\u001b[0m     nnz \u001b[38;5;241m=\u001b[39m \u001b[43mnnz_row_ptr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nnz \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2193\u001b[0m         coo_tensor \u001b[38;5;241m=\u001b[39m coo_zeros(\n\u001b[1;32m   2194\u001b[0m             A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], nnz_row_ptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(), device\n\u001b[1;32m   2195\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "eval_results = defaultdict(dict)\n",
    "\n",
    "for model_details in MODELS: #-------------------------------------------------------------\n",
    "\n",
    "    # Load model\n",
    "    model_name = model_details['model']\n",
    "    print(f\"Loading model {model_name}\")\n",
    "    tokenizer, model = load_model_and_tokenizer(model_details)\n",
    "    \n",
    "    for topic in TOPICS:  # ---------------------------------------------------------------\n",
    "\n",
    "        # Load dataset\n",
    "        print(f\"Loading dataset for {topic}\")\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "        #dataset['test'] = dataset['test'].filter(lambda x: x['token_count'] > 200)\n",
    "        #dataset[SPLIT] = sample_random_from_dataset(dataset, n=10, subset=SPLIT)\n",
    "        #print(dataset['test'][0])\n",
    "\n",
    "        # Few-Shot ------------------------------------------------------------------------\n",
    "\n",
    "        # Random Sampling\n",
    "        if DEMONSTR_SAMPLING == \"random\":\n",
    "            examples = sample_examples_random(dataset['train'], k=K) \n",
    "            #print(\"Examples: \", len(examples))  \n",
    "\n",
    "        # Random Sampling (Balanced by classes)\n",
    "        elif DEMONSTR_SAMPLING ==  \"random_balanced\":\n",
    "            examples = sample_examples_random_balanced(dataset['train'], k=K)\n",
    "\n",
    "        # KNN Cluster-based sampling\n",
    "        elif DEMONSTR_SAMPLING == \"knn\":\n",
    "            article_index = AnnoyIndex(encoder.get_sentence_embedding_dimension(), \"angular\")\n",
    "            article_index.load(f'../../data/indices/page_index_{topic}.ann')\n",
    "            \n",
    "        # # Use curated examples\n",
    "        # elif DEMONSTR_SAMPLING == \"expert\":\n",
    "        #     examples = sample_from_expert(curated_examples, topic, k)\n",
    "\n",
    "        # Zero-Shot ------------------------------------------------------------------------\n",
    "        \n",
    "        elif K == 0:\n",
    "            examples = []\n",
    "        else: \n",
    "            examples = []\n",
    "    \n",
    "        # Iterate over pages in test split\n",
    "        answers = [] \n",
    "        for row in tqdm(dataset[SPLIT]): # ---------------------------------------------------\n",
    "    \n",
    "            # Dynamic Sampling \n",
    "            if DEMONSTR_SAMPLING == \"knn\" and K > 0:\n",
    "                examples = sample_examples_knn(encoder, article_index, row[\"text\"], dataset[\"train\"], K)\n",
    "    \n",
    "            # Generate answers\n",
    "            prompt_template = PROMPT_TEMPLATE_FEW_SHOT if K > 0 else PROMPT_TEMPLATE_ZERO_SHOT\n",
    "            prompt = compile_prompt(row, prompt_template, PROMPT_TEMPLATE_EXAMPLES, topic, topic_desciptions, examples)\n",
    "            answer = generate_answers(model, tokenizer, prompt, params)\n",
    "            #print(\"Label:\", row[\"label\"])\n",
    "            #print(\"Answers:\", answer)\n",
    "            answers.append(answer)\n",
    "\n",
    "    \n",
    "        # Add answers to the dataset\n",
    "        dataset[SPLIT] = dataset[SPLIT].add_column(\"answers\", answers)\n",
    "        dataset.save_to_disk(f\"../../data/tmp/processed_dataset_{topic}_answers_{K}s_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_{model_name.split('/')[1]}_{FEATURES}_{SPLIT}_{DEMONSTR_SAMPLING}\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        print(\"Answers:\", answers)\n",
    "        answers_after_voting = [majority_voting(ans) for ans in answers]\n",
    "        #print(\"Answers after voting:\", answers_after_voting)\n",
    "        answers_parsed = [parse_response(ans) for ans in answers_after_voting]\n",
    "        metrics = calc_metrics(dataset[SPLIT]['label'], answers_parsed)\n",
    "        eval_results[model_name][topic] = metrics\n",
    "        print(f\"Metrics for {model_name}: {metrics}\")\n",
    "        \n",
    "    # Clear GPU memory to avoid memory errors\n",
    "    model.cpu()\n",
    "    torch.cuda.empty_cache()\n",
    "    del model, tokenizer\n",
    "    gc.collect()  # Explicitly invoking garbage collection\n",
    "    torch.cuda.empty_cache()  # Clear cache again after garbage collection\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '648c2adb8e8cadbd29095617',\n",
       " 'batch_id': 15,\n",
       " 'domain': 'schule.at',\n",
       " 'view_url': 'www.schule.at/gewinnspiel-buch-tori-twister-stuermisch-unterwegs',\n",
       " 'lang': 'de',\n",
       " 'text': 'Your browser does not support JavaScript! Webmail Bildungs-News Schul-Themen Unterrichts-Portale Schulführer Termine Tools & Apps Webmail schule.at Gewinnspiel: Buch \"Tori Twister - Stürmisch unterwegs\" © KOSMOS Verlag Tori Twister - Stürmisch unterwegs Die Autorin von \"Tori Twister - Stürmisch unterwegs\" verpackt Naturwissen zu Wetterphänomenen und Umweltschutz in eine spannende und lustige Geschichte. Wir haben bei Autorin Marikka Pfeiffer nachgefragt, was Ihre Beweggründe waren, dieses Buch zu schreiben. Und mit etwas Glück können Sie bei unserem Gewinnspiel ein Buchexemplar gewinnen . Als Geschichtenerzählerin liegt es mir sehr am Herzen, mit meinen Büchern die Freude am Lesen zu wecken, denn die Vorstellungskraft und das Lesen sind für mich zwei der wichtigsten Kulturgüter, die wir haben. Daher ist die Geschichte von TORI TWISTER vor allem ein spannendes Abenteuer voller Rätsel und einer Prise Magie. Das Wetter als Thema kam auf ganz simple Weise zu mir: Bei einem Post auf Social Media las ich das Wort - Wetterfabrik - und sah in meiner Vorstellung sofort eine geheime Wetterküche vor mir. Ich wusste auch prompt, wer die vor über hundert Jahren aus welchem Grund errichtet hat. Und während ich mich in dieses Bild hinein geträumt habe, tauchte Familie Twister vor meinem geistigen Auge auf und erzählte mir ihre Geschichte - die ich dann aufschrieb',\n",
       " 'text_length': 1370,\n",
       " 'word_count': 209,\n",
       " 'topic': 'kinder',\n",
       " 'category': 'other',\n",
       " 'good_for_training': 'True',\n",
       " 'good_for_augmentation': 'True',\n",
       " 'annotation_type': '04.urls-with-title',\n",
       " 'is_topic': False,\n",
       " 'label': 0,\n",
       " 'token_count': 329,\n",
       " 'chunk_id': 0,\n",
       " 'answers': ['Nein']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Output Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get chunk level predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path = f\"eval_results_icl_{K}s_{DEMONSTR_SAMPLING}_{SPLIT}_chat_chunks.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the dictionary from the JSON file\n",
    "with open(file_path, \"r\") as file:\n",
    "    eval_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all topics (assuming all models are evaluated on the same topics)\n",
    "topics = list(next(iter(eval_results.values())).keys())\n",
    "\n",
    "# Prepare headers for the table: each topic will have four metrics\n",
    "headers = [\"Model\"] + \\\n",
    "    [f\"{topic} {metric}\" for topic in topics for metric in [\n",
    "        \"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# Prepare rows: one row per model, containing metrics for each topic\n",
    "rows = []\n",
    "for model_name_t, topics_metrics in eval_results.items():\n",
    "    row = [model_name_t]  # Start with the model name\n",
    "    for topic in topics:\n",
    "        metrics = topics_metrics.get(topic, {})\n",
    "        row.extend([metrics.get('accuracy', 0.0), metrics.get(\n",
    "            'precision', 0.0), metrics.get('recall', 0.0), metrics.get('f1', 0.0)])\n",
    "    rows.append(row)\n",
    "\n",
    "# Generate the HTML table\n",
    "table_html = tabulate(rows, headers=headers, tablefmt=\"html\",\n",
    "                      showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model                              </th><th style=\"text-align: right;\">  cannabis Acc.</th><th style=\"text-align: right;\">  cannabis Prec.</th><th style=\"text-align: right;\">  cannabis Rec.</th><th style=\"text-align: right;\">  cannabis F1</th><th style=\"text-align: right;\">  energie Acc.</th><th style=\"text-align: right;\">  energie Prec.</th><th style=\"text-align: right;\">  energie Rec.</th><th style=\"text-align: right;\">  energie F1</th><th style=\"text-align: right;\">  kinder Acc.</th><th style=\"text-align: right;\">  kinder Prec.</th><th style=\"text-align: right;\">  kinder Rec.</th><th style=\"text-align: right;\">  kinder F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LeoLM/leo-mistral-hessianai-7b-chat</td><td style=\"text-align: right;\">          0.789</td><td style=\"text-align: right;\">           0.766</td><td style=\"text-align: right;\">          0.973</td><td style=\"text-align: right;\">        0.857</td><td style=\"text-align: right;\">         0.718</td><td style=\"text-align: right;\">          0.622</td><td style=\"text-align: right;\">         0.977</td><td style=\"text-align: right;\">       0.760</td><td style=\"text-align: right;\">        0.854</td><td style=\"text-align: right;\">         0.826</td><td style=\"text-align: right;\">        0.980</td><td style=\"text-align: right;\">      0.897</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get page level predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majority_voting(answers):\n",
    "    \"\"\"Apply majority voting to a list of arbitrary classification answers.\"\"\"\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common()  # Get all common answers sorted by frequency\n",
    "\n",
    "    if not most_common:\n",
    "        return 0 # Handle empty input scenario\n",
    "\n",
    "    # Check for ties at the highest count\n",
    "    max_votes = most_common[0][1]\n",
    "    tied_classes = [cls for cls, votes in most_common if votes == max_votes]\n",
    "\n",
    "    if len(tied_classes) > 1:\n",
    "        return max(tied_classes)  # Return the maximum class label in case of a tie\n",
    "    return tied_classes[0]  # Return the class with the most votes\n",
    "\n",
    "majority_voting([1, 1, 2, 2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model LeoLM/leo-mistral-hessianai-7b-chat on cannabis ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:00<00:00, 8988.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for LeoLM/leo-mistral-hessianai-7b-chat on cannabis: {'accuracy': 0.5116279069767442, 'precision': 0.4878048780487805, 'recall': 1.0, 'f1': 0.6557377049180327}\n",
      "\n",
      "\n",
      "###### Evaluating model LeoLM/leo-mistral-hessianai-7b-chat on energie ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:00<00:00, 9005.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for LeoLM/leo-mistral-hessianai-7b-chat on energie: {'accuracy': 0.5652173913043478, 'precision': 0.5348837209302325, 'recall': 1.0, 'f1': 0.696969696969697}\n",
      "\n",
      "\n",
      "###### Evaluating model LeoLM/leo-mistral-hessianai-7b-chat on kinder ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 8830.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Metrics for LeoLM/leo-mistral-hessianai-7b-chat on kinder: {'accuracy': 0.6976744186046512, 'precision': 0.6176470588235294, 'recall': 1.0, 'f1': 0.7636363636363637}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results_pages = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "    for model_details in MODELS: # -------------------------------------------------------------\n",
    "        \n",
    "        model_name = model_details['model']\n",
    "            \n",
    "        print(f\"\\n\\n###### Evaluating model {model_name} on {topic} ###### \\n\\n\")\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_answers_{K}s_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_{model_name.split('/')[1]}_{FEATURES}_{SPLIT}_{DEMONSTR_SAMPLING}\")\n",
    "        \n",
    "        #print(dataset)\n",
    "        \n",
    "        # Group dataset examples by URL, with a fallback to domain\n",
    "        grouped_dataset = {}\n",
    "        for example in tqdm(dataset[SPLIT]):\n",
    "            url = example.get(\"view_url\") or example.get(\"domain\")\n",
    "            example_filtered = {k: example[k] for k in [\"text\", \"domain\", \"answers\", \"label\", \"category\", \"annotation_type\", \"lang\"]}\n",
    "            grouped_dataset.setdefault(url, []).append(example_filtered)\n",
    "            \n",
    "        # Extract labels\n",
    "        labels = []\n",
    "        for url, chunks in grouped_dataset.items():\n",
    "            label = [chunk[\"label\"] for chunk in chunks]\n",
    "            labels.append(max(label))\n",
    "            \n",
    "        # Merge chunk level predictions\n",
    "        predictions = []\n",
    "        for url, chunks in grouped_dataset.items():\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                answers = [parse_response(ans) for ans in chunk[\"answers\"]]\n",
    "                chunk[\"pred\"] = majority_voting(answers)\n",
    "                \n",
    "            preds = [chunk[\"pred\"] for chunk in chunks]\n",
    "            pred = majority_voting([pred for pred in preds if pred > 0]) if max(preds) > 0 else 0\n",
    "            predictions.append(pred)\n",
    "\n",
    "        # Use the trained model to make predictions on the test set\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "        metrics = calc_metrics(labels, predictions)\n",
    "        print(f\"Metrics for {model_name} on {topic}: {metrics}\")\n",
    "        \n",
    "        # Update the eval_results dictionary\n",
    "        eval_results_pages[model_name][topic] = metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path = f\"eval_results_icl_{K}s_{DEMONSTR_SAMPLING}_{SPLIT}_chat_pages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results_pages, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    eval_results_pages = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all topics (assuming all models are evaluated on the same topics)\n",
    "topics = list(next(iter(eval_results_pages.values())).keys())\n",
    "\n",
    "# Prepare headers for the table: each topic will have four metrics\n",
    "headers = [\"Model\"] + [f\"{topic} {metric}\" for topic in topics for metric in [\"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# Prepare rows: one row per model, containing metrics for each topic\n",
    "rows = []\n",
    "for model, topics_metrics in eval_results_pages.items():\n",
    "    row = [model]  # Start with the model name\n",
    "    for topic in topics:\n",
    "        metrics = topics_metrics.get(topic, {})\n",
    "        row.extend([metrics.get('accuracy',0.0), metrics.get('precision',0.0), metrics.get('recall',0.0), metrics.get('f1',0.0)])\n",
    "    rows.append(row)\n",
    "\n",
    "# Generate the HTML table\n",
    "table_html = tabulate(rows, headers=headers, tablefmt=\"html\", showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model                              </th><th style=\"text-align: right;\">  cannabis Acc.</th><th style=\"text-align: right;\">  cannabis Prec.</th><th style=\"text-align: right;\">  cannabis Rec.</th><th style=\"text-align: right;\">  cannabis F1</th><th style=\"text-align: right;\">  energie Acc.</th><th style=\"text-align: right;\">  energie Prec.</th><th style=\"text-align: right;\">  energie Rec.</th><th style=\"text-align: right;\">  energie F1</th><th style=\"text-align: right;\">  kinder Acc.</th><th style=\"text-align: right;\">  kinder Prec.</th><th style=\"text-align: right;\">  kinder Rec.</th><th style=\"text-align: right;\">  kinder F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LeoLM/leo-mistral-hessianai-7b-chat</td><td style=\"text-align: right;\">          0.512</td><td style=\"text-align: right;\">           0.488</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">        0.656</td><td style=\"text-align: right;\">         0.565</td><td style=\"text-align: right;\">          0.535</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       0.697</td><td style=\"text-align: right;\">        0.698</td><td style=\"text-align: right;\">         0.618</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">      0.764</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grouped_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
