{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Back-Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Needed to import modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.database import *\n",
    "from utils.files import *\n",
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "from datasets import load_from_disk, Dataset, ClassLabel, Value, Features\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.model import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"cannabis\" #\"energie\" #\"kinder\" \"cannabis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:11<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"CohereForAI/aya-101\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the following text from German to English, retaining the original meaning and topic. Please return only the translated text without any additional content or formatting.\n",
      "\n",
      "Text: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRANSLATION_TEMPLATE = \"\"\"Translate the following text from {source_lang} to {target_lang}, retaining the original meaning and topic. Please return only the translated text without any additional content or formatting.\n",
    "\n",
    "Text: \"{webpage_text}\"\n",
    "\"\"\"\n",
    "\n",
    "# Test the template with a dummy text for translation from English to Spanish\n",
    "translation_test = TRANSLATION_TEMPLATE.format(source_lang='German', target_lang='English', webpage_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.')\n",
    "print(translation_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "        'early_stopping': True,        \n",
    "        #'num_beam_groups': 2,\n",
    "        'num_beams': 2,\n",
    "        'num_return_sequences': 1,\n",
    "        'max_new_tokens': 1024,\n",
    "        'min_new_tokens': 1,\n",
    "        'output_scores': True,\n",
    "        #'repetition_penalty': 1.0,\n",
    "        'temperature': 1.2,\n",
    "        'top_k': 50,\n",
    "        'top_p': 1.0 \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 2651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 295\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = load_from_disk(f\"../../data/tmp/processed_dataset_buff_{topic}_split_chunkified\")\n",
    "dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_random\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_prompt(article, template, source_lang, target_lang):\n",
    "    \"\"\"Compiles the translation prompt for the given article, specifying source and target languages.\"\"\"\n",
    "\n",
    "    # Extract the article text\n",
    "    article_text = article.get(\"text\")\n",
    "    # Format the template with the article text, source language, and target language\n",
    "    prompt = template.format(source_lang=source_lang, target_lang=target_lang, webpage_text=article_text)\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_input_length(prompt):\n",
    "    \"\"\" Calculates the length of the input sequence for the model. \"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False, truncation=False, padding=False)\n",
    "\n",
    "    # Calculate the length of the input sequence\n",
    "    input_length = tokenized_prompt.input_ids.size(1)\n",
    "\n",
    "    return input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model, tokenizer, prompt, params, remove_input=True):\n",
    "    \"\"\"Generates answers from a language model for a given prompt.\"\"\"\n",
    "\n",
    "    # Encode the prompt and generate the answers\n",
    "    encoded_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    if encoded_input.size()[1] > tokenizer.model_max_length:\n",
    "        print(\"Input too long, truncating.\")\n",
    "        #encoded_input = encoded_input[:, :tokenizer.model_max_length]\n",
    "    \n",
    "    generated_outputs = model.generate(encoded_input, **params)\n",
    "\n",
    "    # Decode and clean outputs\n",
    "    outputs = []\n",
    "    input_text_wo_st = tokenizer.decode(encoded_input[0], skip_special_tokens=True)\n",
    "    for output in generated_outputs:\n",
    "        decoded_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        cleaned_text = decoded_text.replace(input_text_wo_st, \"\").strip()\n",
    "        outputs.append(cleaned_text if remove_input else decoded_text)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_response(output_text):\n",
    "#     \"\"\"Determines if the model's output signifies \"Yes\" (1) or \"No\" (0).\"\"\"\n",
    "#     text = output_text.lower()\n",
    "#     return 1 if \"yes\" in text else 0 if \"no\" in text else ValueError(\"Ambiguous response.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text, tokenizer, model, template, source_lang, target_lang, params):\n",
    "    \"\"\"Translates the given text from source language to target language using the model and template.\"\"\"\n",
    "    \n",
    "    # Compile the prompt for translation using the provided template, source, and target languages\n",
    "    prompt = compile_prompt({\"text\": text}, template, source_lang=source_lang, target_lang=target_lang)\n",
    "    translated_text = generate_answers(model, tokenizer, prompt, params)[0]\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtranslate(text, tokenizer, model, template, original_lang, intermediate_lang, params):\n",
    "    \"\"\"\n",
    "    Performs backtranslation on the given text by first translating it to an intermediate language, and then back to the original language.\n",
    "    \"\"\"\n",
    "\n",
    "    # Translate from the original language to the intermediate language\n",
    "    translated_to_intermediate = translate(text, tokenizer, model, template, \n",
    "                                           source_lang=original_lang, target_lang=intermediate_lang, \n",
    "                                           params=params)\n",
    "    \n",
    "    # Translate back from the intermediate language to the original language\n",
    "    backtranslated_text = translate(translated_to_intermediate, tokenizer, model, template, \n",
    "                                    source_lang=intermediate_lang, target_lang=original_lang, \n",
    "                                    params=params)\n",
    "    \n",
    "    return backtranslated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Der Konsum von Cannabis ist in Deutschland verboten.\n",
      "Backtranslated (Paraphrased) text: Der Gebrauch von Marihuana ist in Deutschland verboten.\n"
     ]
    }
   ],
   "source": [
    "# Original text in German\n",
    "text = \"Der Konsum von Cannabis ist in Deutschland verboten.\"\n",
    "\n",
    "# Perform backtranslation for paraphrasing, using English as the intermediate language\n",
    "backtranslated_text = backtranslate(text, tokenizer, model, TRANSLATION_TEMPLATE, \n",
    "                                    original_lang='German', intermediate_lang='English', \n",
    "                                    params=params)\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Backtranslated (Paraphrased) text:\", backtranslated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 2651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 295\n",
       "    })\n",
       "    positive_sampled: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 2\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter positive examples and sample 20 percent of the positive exampl\n",
    "positive_examples = dataset['train'].filter(lambda example: example['label'] == 1)\n",
    "\n",
    "# Select the first 20% of the shuffled positive examples as your random sample\n",
    "positive_examples_shuffled = positive_examples.shuffle(seed=42)\n",
    "num_samples = int(len(positive_examples_shuffled) * 0.002) \n",
    "sampled_examples = positive_examples_shuffled.select(range(num_samples))\n",
    "\n",
    "# Generate new data points for the sampled positive examples\n",
    "dataset[f'positive_sampled'] = sampled_examples\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_data_points(text, n_examples=1):\n",
    "    \"\"\"Generates n new data points from the original text.\"\"\"\n",
    "    new_texts = [backtranslate(text, tokenizer, model, TRANSLATION_TEMPLATE, \n",
    "                                    original_lang='German', intermediate_lang='English', \n",
    "                                    params=params) for _ in range(n_examples)]\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [03:12<00:00, 96.42s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 1 new examples for each original example.\n",
      "Generating 2 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:00<00:00, 60.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 2 new examples for each original example.\n",
      "Generating 3 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [08:02<00:00, 241.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 3 new examples for each original example.\n",
      "Generating 4 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [03:50<00:00, 115.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 4 new examples for each original example.\n",
      "Generating 5 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [04:13<00:00, 126.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 5 new examples for each original example.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate new datapoints for n = 1, 2, 3, 4, 5\n",
    "for n in [1, 2, 3, 4, 5]:\n",
    "    print(f\"Generating {n} new examples for each original example...\")\n",
    "    \n",
    "    # Expand the dataset with new examples\n",
    "    expanded_examples = []\n",
    "    for example in tqdm(sampled_examples):\n",
    "        new_texts = generate_new_data_points(example['text'], n)\n",
    "        for new_text in new_texts:\n",
    "            new_example = example.copy()\n",
    "            new_example['text'] = new_text\n",
    "            expanded_examples.append(new_example)\n",
    "    \n",
    "    # Convert the list of new examples to a Dataset\n",
    "    expanded_dataset = Dataset.from_pandas(pd.DataFrame(expanded_examples))\n",
    "    dataset[f'expanded_{n}'] = expanded_dataset\n",
    "\n",
    "    print(f\"Completed generating {n} new examples for each original example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '64a0946b749484eec84ef27b',\n",
       " 'batch_id': 16,\n",
       " 'domain': 'mdr.de',\n",
       " 'view_url': 'www.mdr.de/brisant/cannabis-legalisierung-278.html',\n",
       " 'lang': 'de',\n",
       " 'text': 'Menü Startseite Sendungen TV - Programm Live Mediathek Teletext Service Über uns Zur optimalen Darstellung unserer Webseite benötigen Sie Javascript. Bitte aktivieren sie dies in Ihrem Browser. Brisant Zur Brisant - Startseite Startseite Prominent Ratgeber Podcast Redaktion Service Neuer Bereich Rauschmittel Legalisierung von Cannabis - Das soll sich beim Besitz, Konsum und Kauf jetzt ändern Hauptinhalt Stand : 29. Juni 2023, 11 : 45 Uhr Die geplante Freigabe von Cannabis ist in Deutschland umstritten. Nun hat die Bundesregierung die teilweise Legalisierung der Droge auf den Weg gebracht. Nach Angaben von Gesundheitsminister Lauterbach sollen Kauf und Besitz von Cannabis künftig erlaubt sein - allerdings stark reglementiert. Der freie Verkauf für Erwachsene ist damit vorerst vom Tisch. Wird Cannabis in Deutschland bald legal? Bundesgesundheitsminister Karl Lauterbach hat Eckpunkte vorgelegt, wie eine Cannabis - Legalisierung aussehen könnte. Bildrechte : imago / Christian Ohde Auf dieser Seite : Die wichtigsten Eckpunkte im Überblick : Die Legalisierung von Cannabis stand seit längerem auf der Agenda der Bundesregierung. Jetzt hat Bundesgesundheitsminister Karl Lauterbach ein überarbeitetes Eckpunkte - Papier präsentiert, mit dem die teilweise Legalisierung der Droge auf den Weg gebracht werden soll. Cannabis Cannabis ist der Name der indischen Hanfpflanze, die den psychoaktiven Wirkstoff Tetrahydrocannabinol ( THC ) enthält. Dieser Wirkstoff verursacht einen Rauschzustand. Hierzulande sind zwei Cannabis - Produkte als Rauschmittel gebräuchlich : Marihuana und Haschisch. Ersteres bezeichnet die getrockneten Blütenblätter, Stängel und Blätter der Pflanze. Unter Haschisch versteht man das getrocknete Harz aus den Drüsenhaaren der weiblichen Pflanze. Besitz von 25 Gramm Cannabis wird straffrei Künftig sollen Kauf und Besitz von maximal 25 Gramm Cannabis ab einem Alter von 18 Jahren grundsätzlich straffrei sein. Auch der Eigenanbau von bis zu drei Cannabis - Pflanzen soll erlaubt werden. Außerdem will die Bundesregierung den Anbau und die Abgabe der Droge in speziellen Vereinen ermöglichen. Die zunächst geplanten Cannabis - Fachgeschäfte, in denen Rausch - Produkte frei verkauft werden können, soll es allerdings nicht geben. Der Verkauf in solchen Geschäften soll erst in einem zweiten Schritt in Modellregionen erprobt werden - mit wissenschaftlicher Begleitung. Neuer Abschnitt Bildrechte : Colourbox. de Hintergrund Cannabis - so gefährlich ist die noch illegale Droge Cannabis - so gefährlich ist die bald legale Droge für Psyche und Gesundheit mehr Verkauf von Cannabis auch an Jugendliche? Grundsätzlich soll Cannabis rechtlich nicht mehr',\n",
       " 'text_length': 10054,\n",
       " 'word_count': 1340,\n",
       " 'is_topic': True,\n",
       " 'label': 1,\n",
       " 'chunk_id': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['positive_sampled'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '64a0946b749484eec84ef27b',\n",
       " 'batch_id': 16,\n",
       " 'domain': 'mdr.de',\n",
       " 'view_url': 'www.mdr.de/brisant/cannabis-legalisierung-278.html',\n",
       " 'lang': 'de',\n",
       " 'text': 'Entschuldigung, ich kann keine passende Antwort in der Datenbank finden.',\n",
       " 'text_length': 10054,\n",
       " 'word_count': 1340,\n",
       " 'is_topic': True,\n",
       " 'label': 1,\n",
       " 'chunk_id': 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Generated Trainig Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2651/2651 [00:00<00:00, 259701.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 295/295 [00:00<00:00, 55855.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2/2 [00:00<00:00, 373.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2/2 [00:00<00:00, 407.35 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4/4 [00:00<00:00, 766.19 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6/6 [00:00<00:00, 1150.81 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 8/8 [00:00<00:00, 1418.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 1816.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the expanded dataset\n",
    "dataset.save_to_disk(f\"../../data/tmp/augmented_dataset_{topic}_backtranslation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
