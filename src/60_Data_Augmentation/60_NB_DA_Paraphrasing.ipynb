{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Needed to import modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.database import *\n",
    "from utils.files import *\n",
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "from datasets import load_from_disk, Dataset, ClassLabel, Value, Features\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.model import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"cannabis\" #\"energie\" #\"kinder\" \"cannabis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 11/11 [00:10<00:00,  1.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"CohereForAI/aya-101\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.device_count() > 1:\n",
    "#     print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "\n",
    "# # Move model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paraphrase the following text, retaining the original language, meaning, and topic. Please return only the paraphrased text without any additional content or formatting.\n",
      "\n",
      "Text: \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\"\n",
      "\n",
      "Paraphrased Text:\n"
     ]
    }
   ],
   "source": [
    "PARAPHRASE_TEMPLATE = \"\"\"Paraphrase the following text, retaining the original language, meaning, and topic. Please return only the paraphrased text without any additional content or formatting.\n",
    "\n",
    "Text: \"{webpage_text}\"\n",
    "\n",
    "Paraphrased Text:\"\"\"\n",
    "\n",
    "# Test the template with a dummy text\n",
    "paraphrase_test = PARAPHRASE_TEMPLATE.format(webpage_text='Lorem ipsum dolor sit amet, consectetur adipiscing elit.')\n",
    "print(paraphrase_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'do_sample': True,\n",
    "        'early_stopping': True,        \n",
    "        #'num_beam_groups': 2,\n",
    "        'num_beams': 2,\n",
    "        'num_return_sequences': 1,\n",
    "        'max_new_tokens': 1024,\n",
    "        'min_new_tokens': 1,\n",
    "        'output_scores': True,\n",
    "        #'repetition_penalty': 1.0,\n",
    "        'temperature': 1.2,\n",
    "        'top_k': 50,\n",
    "        'top_p': 1.0 \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 2651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 295\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = load_from_disk(f\"../../data/tmp/processed_dataset_buff_{topic}_split_chunkified\")\n",
    "dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_random\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_prompt(article, template, topic, lang = 'German'):\n",
    "    \"\"\" Compiles the prompt for the given article and model.\"\"\"\n",
    "\n",
    "    # Extract the article headline and text\n",
    "    article_text = article.get(\"text\")\n",
    "    prompt = template.format(topic = topic, lang = lang, webpage_text=article_text)\n",
    "    #prompt = template.format(topic = \"Cannabis\", lang = 'German', webpage_text=article_text, positive_example=positive_example, negative_example=negative_example)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_input_length(prompt):\n",
    "    \"\"\" Calculates the length of the input sequence for the model. \"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False, truncation=False, padding=False)\n",
    "\n",
    "    # Calculate the length of the input sequence\n",
    "    input_length = tokenized_prompt.input_ids.size(1)\n",
    "\n",
    "    return input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model, tokenizer, prompt, params, remove_input=True):\n",
    "    \"\"\"Generates answers from a language model for a given prompt.\"\"\"\n",
    "\n",
    "    # Encode the prompt and generate the answers\n",
    "    encoded_input = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    if encoded_input.size()[1] > tokenizer.model_max_length:\n",
    "        print(\"Input too long, truncating.\")\n",
    "        #encoded_input = encoded_input[:, :tokenizer.model_max_length]\n",
    "    \n",
    "    generated_outputs = model.generate(encoded_input, **params)\n",
    "\n",
    "    # Decode and clean outputs\n",
    "    outputs = []\n",
    "    input_text_wo_st = tokenizer.decode(encoded_input[0], skip_special_tokens=True)\n",
    "    for output in generated_outputs:\n",
    "        decoded_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        cleaned_text = decoded_text.replace(input_text_wo_st, \"\").strip()\n",
    "        outputs.append(cleaned_text if remove_input else decoded_text)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_response(output_text):\n",
    "#     \"\"\"Determines if the model's output signifies \"Yes\" (1) or \"No\" (0).\"\"\"\n",
    "#     text = output_text.lower()\n",
    "#     return 1 if \"yes\" in text else 0 if \"no\" in text else ValueError(\"Ambiguous response.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(text, tokenizer, model, template, params):\n",
    "    \"\"\"Paraphrases the given text using the model and template.\"\"\"\n",
    "    prompt = compile_prompt({\"text\": text}, template, topic)\n",
    "    paraphrased_text = generate_answers(model, tokenizer, prompt, params)[0]\n",
    "    return paraphrased_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Der Konsum von Cannabis ist in Deutschland verboten.\n",
      "Paraphrased text: Cannabis ist in Deutschland verboten.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "#text = \"Das hier ist ein Test.\"\n",
    "text = \"Der Konsum von Cannabis ist in Deutschland verboten.\"\n",
    "paraphrased_text = paraphrase(text, tokenizer, model, PARAPHRASE_TEMPLATE, params)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Paraphrased text:\", paraphrased_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 2651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 295\n",
       "    })\n",
       "    positive_sampled: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter positive examples and sample 20 percent of the positive exampl\n",
    "positive_examples = dataset['train'].filter(lambda example: example['label'] == 1)\n",
    "\n",
    "# Select the first 20% of the shuffled positive examples as your random sample\n",
    "positive_examples_shuffled = positive_examples.shuffle(seed=42)\n",
    "num_samples = int(len(positive_examples_shuffled) * 0.004) \n",
    "sampled_examples = positive_examples_shuffled.select(range(num_samples))\n",
    "\n",
    "# Generate new data points for the sampled positive examples\n",
    "dataset[f'positive_sampled'] = sampled_examples\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_data_points(text, n_examples=1):\n",
    "    \"\"\"Generates n new data points from the original text.\"\"\"\n",
    "    new_texts = [paraphrase(text, tokenizer, model, PARAPHRASE_TEMPLATE, params) for _ in range(n_examples)]\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [03:42<00:00, 44.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 1 new examples for each original example.\n",
      "Generating 2 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [05:25<00:00, 65.19s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 2 new examples for each original example.\n",
      "Generating 3 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [12:06<00:00, 145.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 3 new examples for each original example.\n",
      "Generating 4 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:03<00:00, 132.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 4 new examples for each original example.\n",
      "Generating 5 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [15:24<00:00, 184.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 5 new examples for each original example.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate new datapoints for n = 1, 2, 3, 4, 5\n",
    "for n in [1, 2, 3, 4, 5]:\n",
    "    print(f\"Generating {n} new examples for each original example...\")\n",
    "    \n",
    "    # Expand the dataset with new examples\n",
    "    expanded_examples = []\n",
    "    for example in tqdm(sampled_examples):\n",
    "        new_texts = generate_new_data_points(example['text'], n)\n",
    "        for new_text in new_texts:\n",
    "            new_example = example.copy()\n",
    "            new_example['text'] = new_text\n",
    "            expanded_examples.append(new_example)\n",
    "    \n",
    "    # Convert the list of new examples to a Dataset\n",
    "    expanded_dataset = Dataset.from_pandas(pd.DataFrame(expanded_examples))\n",
    "    dataset[f'expanded_{n}'] = expanded_dataset\n",
    "\n",
    "    print(f\"Completed generating {n} new examples for each original example.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['expanded_1'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': '64a0946b749484eec84ef27b',\n",
       " 'batch_id': 16,\n",
       " 'domain': 'mdr.de',\n",
       " 'view_url': 'www.mdr.de/brisant/cannabis-legalisierung-278.html',\n",
       " 'lang': 'de',\n",
       " 'text': 'Menü Startseite Sendungen TV - Programm Live Mediathek Teletext Service Über uns Zur optimalen Darstellung unserer Webseite benötigen Sie Javascript. Bitte aktivieren sie dies in Ihrem Browser. Brisant Zur Brisant - Startseite Startseite Prominent Ratgeber Podcast Redaktion Service Neuer Bereich Rauschmittel Legalisierung von Cannabis - Das soll sich beim Besitz, Konsum und Kauf jetzt ändern Hauptinhalt Stand : 29. Juni 2023, 11 : 45 Uhr Die geplante Freigabe von Cannabis ist in Deutschland umstritten. Nun hat die Bundesregierung die teilweise Legalisierung der Droge auf den Weg gebracht.',\n",
       " 'text_length': 10054,\n",
       " 'word_count': 1340,\n",
       " 'is_topic': True,\n",
       " 'label': 1,\n",
       " 'chunk_id': 0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['expanded_1'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Generated Trainig Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2651/2651 [00:00<00:00, 234352.74 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 295/295 [00:00<00:00, 51188.14 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5/5 [00:00<00:00, 855.32 examples/s] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'save_to_disk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the expanded dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/tmp/augmented_dataset_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtopic\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_paraphrasing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/datasets/dataset_dict.py:1276\u001b[0m, in \u001b[0;36mDatasetDict.save_to_disk\u001b[0;34m(self, dataset_dict_path, fs, max_shard_size, num_shards, num_proc, storage_options)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m)}, f)\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 1276\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_to_disk\u001b[49m(\n\u001b[1;32m   1277\u001b[0m         posixpath\u001b[38;5;241m.\u001b[39mjoin(dataset_dict_path, k),\n\u001b[1;32m   1278\u001b[0m         num_shards\u001b[38;5;241m=\u001b[39mnum_shards\u001b[38;5;241m.\u001b[39mget(k),\n\u001b[1;32m   1279\u001b[0m         max_shard_size\u001b[38;5;241m=\u001b[39mmax_shard_size,\n\u001b[1;32m   1280\u001b[0m         num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m   1281\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m   1282\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'save_to_disk'"
     ]
    }
   ],
   "source": [
    "# Save the expanded dataset\n",
    "dataset.save_to_disk(f\"../../data/tmp/augmented_dataset_{topic}_paraphrasing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
