{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation: Word Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Needed to import modules from parent directory\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils.database import *\n",
    "from utils.files import *\n",
    "from tqdm import tqdm\n",
    "from bson import ObjectId\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForMaskedLM\n",
    "from datasets import load_from_disk, Dataset, ClassLabel, Value, Features\n",
    "from huggingface_hub import InferenceClient\n",
    "from transformers import BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.preprocessing import *\n",
    "from utils.accelerators import *\n",
    "from utils.multithreading import *\n",
    "from utils.database import *\n",
    "from utils.model import *\n",
    "from utils.files import *\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import hashlib\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"cannabis\" #\"energie\" #\"kinder\" \"cannabis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-large were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"deepset/gbert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 2651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 295\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset = load_from_disk(f\"../../data/tmp/processed_dataset_buff_{topic}_split_chunkified\")\n",
    "dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_random\")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on an Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_replace_tokens(text, tokenizer, model, mask_probability=0.15):\n",
    "    \"\"\" Randomly mask input tokens and predict the missing ones with a model. \"\"\"\n",
    "\n",
    "    # Tokenize the input text and prepare it for the model: Convert the text to input IDs, \n",
    "    # generate attention masks (to ignore padding in the attention mechanism), and ensure \n",
    "    # all inputs are of the same length by padding shorter texts and truncating longer ones.\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', add_special_tokens=True)\n",
    "    input_ids = inputs.input_ids.clone()\n",
    "    attention_mask = inputs.attention_mask\n",
    "    replaced_input_ids = input_ids.clone()\n",
    "\n",
    "    # Generate a random array of the same shape as input_ids. This will be used to decide\n",
    "    # which tokens to mask based on the mask_probability. Tokens corresponding to 'True' in\n",
    "    # this array will be considered for masking.\n",
    "    rand = torch.rand(input_ids.shape)\n",
    "    mask_arr = (rand < mask_probability) * (input_ids != tokenizer.cls_token_id) * \\\n",
    "               (input_ids != tokenizer.sep_token_id) * (input_ids != tokenizer.pad_token_id)\n",
    "\n",
    "    # Replace selected tokens with the mask token ID in input_ids.\n",
    "    selection = mask_arr.nonzero(as_tuple=False)[:, 1].tolist()\n",
    "    input_ids[0, selection] = tokenizer.mask_token_id\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    predictions = outputs.logits\n",
    "    \n",
    "    print(\"Selection: \", selection)\n",
    "    # For each token position that was masked, find the token with the highest score (most likely token)\n",
    "    # and replace the masked token with it in replaced_input_ids.\n",
    "    for i in selection:\n",
    "        # Get all predictions for this token and sort predictions by likelihood\n",
    "        all_predictions = predictions[0, i]\n",
    "        sorted_predictions = torch.argsort(all_predictions, descending=True) \n",
    "        \n",
    "        for pred_i in sorted_predictions:\n",
    "            # If the predicted token is different from the original, use it\n",
    "            \n",
    "            print(\"Pred_i: \", pred_i)\n",
    "            print(\"Replaced_input_ids: \", replaced_input_ids[0, i])\n",
    "            if pred_i != replaced_input_ids[0, i]:\n",
    "                replaced_input_ids[0, i] = pred_i\n",
    "                break  # Exit the loop once a different token is found\n",
    "\n",
    "    # Decode the replaced tokens back to a string, skipping special tokens\n",
    "    replaced_text = tokenizer.decode(replaced_input_ids[0], skip_special_tokens=True)\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_replace_tokens(text, tokenizer, model, mask_probability=0.15):\n",
    "    \"\"\"Elegantly replace tokens one by one, each with full context.\"\"\"\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', add_special_tokens=True)\n",
    "    input_ids = inputs.input_ids.clone()\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    # Identify non-special tokens for potential masking\n",
    "    non_special_token_indices = [i for i, token_id in enumerate(input_ids[0])\n",
    "                                 if token_id not in (tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id)]\n",
    "    \n",
    "    # Randomly select tokens for masking\n",
    "    num_tokens_to_mask = int(len(non_special_token_indices) * mask_probability)\n",
    "    tokens_to_mask = np.random.choice(non_special_token_indices, size=num_tokens_to_mask, replace=False)\n",
    "\n",
    "    for i in tokens_to_mask:\n",
    "        original_token_id = input_ids[0, i].item()  # Save the original token ID\n",
    "        masked_input_ids = input_ids.detach().clone()\n",
    "        masked_input_ids[0, i] = tokenizer.mask_token_id  # Mask the token\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(masked_input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        predictions = outputs.logits[0, i]\n",
    "        predictions[original_token_id] = -float('Inf')  # Invalidate the original token\n",
    "        best_pred_idx = predictions.argmax(dim=-1).item()\n",
    "        input_ids[0, i] = best_pred_idx  # Replace with the best prediction\n",
    "\n",
    "    replaced_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return replaced_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Das hier ist ein Test.\n",
      "Replaced text: Das hier ist ein Spiel!\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"Das hier ist ein Test.\"\n",
    "replaced_text = randomly_replace_tokens(text, tokenizer, model, mask_probability=0.35)\n",
    "print(\"Original text:\", text)\n",
    "print(\"Replaced text:\", replaced_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 2651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 295\n",
       "    })\n",
       "    positive_sampled: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter positive examples and sample 20 percent of the positive exampl\n",
    "positive_examples = dataset['train'].filter(lambda example: example['label'] == 1)\n",
    "\n",
    "# Select the first 20% of the shuffled positive examples as your random sample\n",
    "positive_examples_shuffled = positive_examples.shuffle(seed=42)\n",
    "num_samples = int(len(positive_examples_shuffled) * 0.01) \n",
    "sampled_examples = positive_examples_shuffled.select(range(num_samples))\n",
    "\n",
    "# Generate new data points for the sampled positive examples\n",
    "dataset[f'positive_sampled'] = sampled_examples\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_data_points(text, n_examples=1):\n",
    "    \"\"\"Generates n new data points from the original text.\"\"\"\n",
    "    new_texts = [randomly_replace_tokens(text, tokenizer, model, 0.35) for _ in range(n_examples)]\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [01:16<00:00,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 1 new examples for each original example.\n",
      "Generating 2 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [02:34<00:00, 11.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 2 new examples for each original example.\n",
      "Generating 3 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [03:49<00:00, 17.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 3 new examples for each original example.\n",
      "Generating 4 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [05:04<00:00, 23.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 4 new examples for each original example.\n",
      "Generating 5 new examples for each original example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [06:22<00:00, 29.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed generating 5 new examples for each original example.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n in [1, 2, 3, 4, 5]:\n",
    "    print(f\"Generating {n} new examples for each original example...\")\n",
    "    \n",
    "    # Placeholder for the expanded dataset\n",
    "    expanded_examples = []\n",
    "\n",
    "    # Iterate over each example in the sampled examples to generate new data points\n",
    "    for example in tqdm(sampled_examples):\n",
    "        new_texts = generate_new_data_points(example['text'], n)\n",
    "        for new_text in new_texts:\n",
    "            new_example = example.copy()\n",
    "            new_example['text'] = new_text\n",
    "            expanded_examples.append(new_example)\n",
    "    \n",
    "    # Convert the list of new examples to a Dataset\n",
    "    expanded_dataset = Dataset.from_pandas(pd.DataFrame(expanded_examples))\n",
    "    dataset[f'expanded_{n}'] = expanded_dataset\n",
    "\n",
    "    print(f\"Completed generating {n} new examples for each original example.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Generated Trainig Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 2651\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 295\n",
       "    })\n",
       "    positive_sampled: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "    expanded_1: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 13\n",
       "    })\n",
       "    expanded_2: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 26\n",
       "    })\n",
       "    expanded_3: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 39\n",
       "    })\n",
       "    expanded_4: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 52\n",
       "    })\n",
       "    expanded_5: Dataset({\n",
       "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'is_topic', 'label', 'chunk_id'],\n",
       "        num_rows: 65\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 2651/2651 [00:00<00:00, 130095.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 295/295 [00:00<00:00, 48139.12 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 13/13 [00:00<00:00, 2363.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 13/13 [00:00<00:00, 2308.76 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 26/26 [00:00<00:00, 4932.91 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 39/39 [00:00<00:00, 7137.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 52/52 [00:00<00:00, 9279.83 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 65/65 [00:00<00:00, 12175.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Save the expanded dataset\n",
    "dataset.save_to_disk(f\"../../data/tmp/augmented_dataset_{topic}_word_replacement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
