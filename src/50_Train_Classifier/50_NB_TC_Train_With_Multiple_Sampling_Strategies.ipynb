{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multiple Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set a seed for numpy module\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set a seed for torch module\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"kinder\", \"energie\"]\n",
    "MODEL = \"deepset/gelectra-large\"#\"deepset/gelectra-large\"\n",
    "STRATEGIES = [\"random\", \"stratified\" , \"clustered\"]#, \"shared_domain\"]\n",
    "SUFFIX = \"\" #\"\", \"_holdout\", \"_extended\"\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192, 384\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url_and_content\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,             # number of training epochs # TODO\n",
    "    weight_decay=0.01,  # Weight decay if we apply some form of weight regularization.\n",
    "    logging_dir='./logs',  # Directory where the training logs will be stored.\n",
    "    logging_strategy=\"steps\",  # The logging strategy determines when to log\n",
    "    logging_steps=100,  # Number of steps between logging of training loss.\n",
    "    evaluation_strategy=\"steps\",  # Evaluation is done\n",
    "    eval_steps=100,  # Number of steps between evaluations.\n",
    "    load_best_model_at_end=True,  # load the best model at the end of training.\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    lr_scheduler_type='linear',  # The scheduler type to use, e.g., 'linear', 'cosine'\n",
    "    warmup_ratio=0.1  # Proportion of training to perform linear learning rate warmup for.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract URL-path:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'view_url': 'https://www.google.com/search?q=python+url+path',\n",
       " 'url_path': 'search?q=python+url+path'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "def extract_url_path(example):\n",
    "    view_url = example['view_url']\n",
    "    if \"://\" not in view_url:\n",
    "        view_url = \"http://\" + view_url  # Assume http if no protocol specified\n",
    "    parsed_url = urlparse(view_url)\n",
    "    new_url = urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment))\n",
    "    example['url_path'] = new_url.lstrip('/')  # Store the result in a new field\n",
    "    return example\n",
    "\n",
    "\n",
    "extract_url_path({\"view_url\": \"https://www.google.com/search?q=python+url+path\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cannabis dataset using random strategy\n",
      "Training on cannabis dataset using stratified strategy\n",
      "Training on cannabis dataset using clustered strategy\n",
      "Training on kinder dataset using random strategy\n",
      "Training on kinder dataset using stratified strategy\n",
      "Training on kinder dataset using clustered strategy\n",
      "Training on energie dataset using random strategy\n",
      "Training on energie dataset using stratified strategy\n",
      "Training on energie dataset using clustered strategy\n"
     ]
    }
   ],
   "source": [
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "    for strategie in STRATEGIES: # -------------------------------------------------------------\n",
    "        print(f\"Training on {topic} dataset using {strategie} strategy\")\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{strategie}{SUFFIX}_{MAX_CONTENT_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cannabis dataset using random strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at deepset/gelectra-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 507/507 [00:00<00:00, 3174.25 examples/s]\n",
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='192' max='192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [192/192 03:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.309300</td>\n",
       "      <td>0.008089</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [64/64 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Results: {'eval_loss': 0.002923979191109538, 'eval_accuracy': 1.0, 'eval_runtime': 23.7244, 'eval_samples_per_second': 21.37, 'eval_steps_per_second': 2.698, 'epoch': 3.0}\n",
      "Training on cannabis dataset using stratified strategy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at deepset/gelectra-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4077/4077 [00:01<00:00, 3394.23 examples/s]\n",
      "Map: 100%|██████████| 507/507 [00:00<00:00, 2461.75 examples/s]\n",
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 57\u001b[0m\n\u001b[1;32m     47\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     48\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     49\u001b[0m     args\u001b[38;5;241m=\u001b[39mTRAINING_ARGS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m#callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m training_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Update the eval_results dictionary\u001b[39;00m\n\u001b[1;32m     60\u001b[0m training_results[strategie][topic] \u001b[38;5;241m=\u001b[39m training_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/trainer.py:1865\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_results = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "    \n",
    "    for strategie in STRATEGIES: # -------------------------------------------------------------\n",
    "\n",
    "        print(f\"Training on {topic} dataset using {strategie} strategy\")\n",
    "        \n",
    "        if FEATURES == \"url\":\n",
    "            dataset = load_from_disk(\n",
    "                f\"../../data/tmp/processed_dataset_{topic}_buffed_{strategie}\")\n",
    "            # Extract the path from the URL\n",
    "            dataset = dataset.map(extract_url_path)\n",
    "            # dataset['test'] = sample_random_from_dataset(dataset, n=5, subset='test')\n",
    "        else:\n",
    "            dataset = load_from_disk(\n",
    "                f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{strategie}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "            # Extract the path from the URL\n",
    "            dataset = dataset.map(extract_url_path)\n",
    "            # dataset['test'] = sample_random_from_dataset(dataset, n=5, subset='test')\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2)\n",
    "\n",
    "        # Define tokenization strategies\n",
    "        tokenization_strategies = {\n",
    "            \n",
    "            # Tokenize the content of the page\n",
    "            \"content\": lambda examples: tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True),\n",
    "            \n",
    "            # Tokenize the URL path\n",
    "            \"url\": lambda examples: tokenizer(examples[\"url_path\"], padding=\"max_length\", truncation=True),\n",
    "            \n",
    "            # Tokenize the URL path and the content of the page\n",
    "            \"url_and_content\": lambda examples: tokenizer(examples[\"url_path\"], examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "        }\n",
    "\n",
    "        # Tokenize dataset\n",
    "        tokenized_datasets = dataset.map(tokenization_strategies[FEATURES],\n",
    "            batched=True)\n",
    "        \n",
    "        # Create a Trainer object\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=TRAINING_ARGS,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            compute_metrics=compute_metrics,\n",
    "            #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        training_result = trainer.train()\n",
    "        \n",
    "        # Update the eval_results dictionary\n",
    "        training_results[strategie][topic] = training_result\n",
    "        \n",
    "        # Evaluate the model\n",
    "        eval_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "        print(\"Eval Results:\", eval_results)\n",
    "        \n",
    "        # Save the model\n",
    "        local_path = f\"../models_ccu/{MODEL.split('/')[0]}_sampling_{strategie.replace('/','_')}_{topic}_model_{FEATURES}\"\n",
    "        trainer.save_model(local_path)\n",
    "        tokenizer.save_pretrained(local_path)\n",
    "        \n",
    "        # Clear GPU memory to avoid memory errors\n",
    "        del model, tokenizer, tokenized_datasets, trainer\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path to save the dictionary\n",
    "file_path = f\"training_results_sampling_{FEATURES}.json\"\n",
    "\n",
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(training_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
