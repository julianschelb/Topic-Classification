{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multiple LIB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set a seed for numpy module\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set a seed for torch module\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "SPLIT = \"extended\" # \"train\", \"test\", \"holdout\", \"extende\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"kinder\", \"energie\"]\n",
    "#TOPICS = [\"cannabis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract URL-path:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'view_url': 'https://www.google.com/search?q=python+url+path',\n",
       " 'url_path': 'search?q=python+url+path'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "def extract_url_path(example):\n",
    "    view_url = example['view_url']\n",
    "    if \"://\" not in view_url:\n",
    "        view_url = \"http://\" + view_url  # Assume http if no protocol specified\n",
    "    parsed_url = urlparse(view_url)\n",
    "    new_url = urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment))\n",
    "    example['url_path'] = new_url.lstrip('/')  # Store the result in a new field\n",
    "    return example\n",
    "\n",
    "\n",
    "extract_url_path({\"view_url\": \"https://www.google.com/search?q=python+url+path\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Extract n-grams from a URL #################################\n",
    "\n",
    "def extract_ngrams(url, n):\n",
    "    \"\"\"Extracts n-grams of length n from the given URL.\"\"\"\n",
    "    return [url[i:i+n] for i in range(len(url) - n + 1)]\n",
    "\n",
    "################################# Calculate frequencies of n-grams #################################\n",
    "\n",
    "def calculate_ngram_frequencies(urls, genres, n_range):\n",
    "    \"\"\"Calculates the frequency of n-grams for each genre from the given URLs and genres.\"\"\"\n",
    "    \n",
    "    ngram_freq = defaultdict(lambda: defaultdict(int))\n",
    "    genre_counts = Counter(genres)\n",
    "    \n",
    "    for url, genre in zip(urls, genres):\n",
    "        for n in n_range:\n",
    "            ngrams = extract_ngrams(url, n)\n",
    "            for ngram in ngrams:\n",
    "                ngram_freq[genre][ngram] += 1\n",
    "    \n",
    "    return ngram_freq, genre_counts\n",
    "\n",
    "################################# Calculate the mutual information for n-grams #################################\n",
    "\n",
    "def calculate_mutual_information(ngram_freq, genre_counts):\n",
    "    \"\"\"Calculates mutual information for n-grams using their frequencies and genre counts.\"\"\"\n",
    "    \n",
    "    ngrams = list({ngram for genre in ngram_freq for ngram in ngram_freq[genre]})\n",
    "    X = np.zeros((len(ngram_freq), len(ngrams)))\n",
    "    y = list(genre_counts.keys())\n",
    "\n",
    "    for i, genre in enumerate(y):\n",
    "        for j, ngram in enumerate(ngrams):\n",
    "            X[i, j] = ngram_freq[genre][ngram]\n",
    "    \n",
    "    mi = mutual_info_classif(X, y, discrete_features=True)\n",
    "    return dict(zip(ngrams, mi))\n",
    "\n",
    "################################# Estimate the interpolation coefficients #################################\n",
    "\n",
    "def estimate_interpolation_coefficients(ngram_mi, n_range):\n",
    "    \"\"\"Estimates interpolation coefficients based on mutual information of n-grams.\"\"\"\n",
    "    \n",
    "    lambda_sum = sum(ngram_mi.values())\n",
    "    lambdas = {n: 0 for n in n_range}\n",
    "    \n",
    "    for ngram, mi in ngram_mi.items():\n",
    "        n = len(ngram)\n",
    "        lambdas[n] += mi / lambda_sum\n",
    "    \n",
    "    # Normalize the coefficients\n",
    "    total = sum(lambdas.values())\n",
    "    for n in lambdas:\n",
    "        lambdas[n] /= total\n",
    "    \n",
    "    return lambdas\n",
    "\n",
    "################################# Main training function #################################\n",
    "\n",
    "def train_genre_classifier(urls, genres, n_range=(2, 3, 4)):\n",
    "    \"\"\"\n",
    "    Trains the genre classifier by calculating n-gram frequencies, mutual information,\n",
    "    and interpolation coefficients from the given URLs and genres.\n",
    "    \"\"\"\n",
    "    # Calculate n-gram frequencies and genre counts\n",
    "    ngram_freq, genre_counts = calculate_ngram_frequencies(urls, genres, n_range)\n",
    "    \n",
    "    # Calculate mutual information for n-grams\n",
    "    ngram_mi = calculate_mutual_information(ngram_freq, genre_counts)\n",
    "    \n",
    "    # Estimate interpolation coefficients\n",
    "    lambdas = estimate_interpolation_coefficients(ngram_mi, n_range)\n",
    "    \n",
    "    return ngram_freq, genre_counts, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# Normalize the probabilities #################################\n",
    "\n",
    "def normalize_probs(probs):\n",
    "    \"\"\"Normalizes the probability dictionary so that the probabilities sum to 1.\"\"\"\n",
    "    total = sum(probs.values())\n",
    "    if total == 0:\n",
    "        return {k: 0 for k in probs}  # Return zero probabilities if total is zero\n",
    "    return {k: v / total for k, v in probs.items()}\n",
    "\n",
    "################################# Calculate the probability of an n-gram for a given genre #################################\n",
    "\n",
    "def calculate_ngram_prob(ngram, genre, ngram_freq, genre_counts, lambdas):\n",
    "    \"\"\"\n",
    "    Calculates the probability of an n-gram given a genre using pre-trained frequencies and interpolation coefficients.\n",
    "    \"\"\"\n",
    "    n = len(ngram)\n",
    "    lambda_n = lambdas.get(n, 0)\n",
    "    freq = ngram_freq[genre].get(ngram, 0)\n",
    "    total_ngrams = sum(ngram_freq[genre].values())\n",
    "    if total_ngrams == 0:\n",
    "        return 1e-10  # Return a small non-zero value if no n-grams are found\n",
    "    prob = (lambda_n * (freq / total_ngrams))\n",
    "    return prob\n",
    "\n",
    "################################# Extract n-grams from a URL #################################\n",
    "\n",
    "def extract_ngrams(url, n):\n",
    "    \"\"\"Extracts n-grams of length n from the given URL.\"\"\"\n",
    "    return [url[i:i+n] for i in range(len(url) - n + 1)]\n",
    "\n",
    "################################# Prediction function #################################\n",
    "\n",
    "def predict_genre(url, ngram_freq, genre_counts, lambdas, n_range=(2, 3, 4)):\n",
    "    \"\"\"\n",
    "    Predicts the genre of a given URL using pre-trained n-gram frequencies, genre counts, and interpolation coefficients.\n",
    "    \"\"\"\n",
    "    probs = {genre: np.log(count / sum(genre_counts.values())) if count > 0 else -np.inf for genre, count in genre_counts.items()}\n",
    "    Q = []\n",
    "    grams = set()\n",
    "    \n",
    "    for n in n_range:\n",
    "        Q.extend(extract_ngrams(url, n))\n",
    "    \n",
    "    while Q:\n",
    "        gram = Q.pop(0)\n",
    "        if any(gram in ngram_freq[genre] for genre in genre_counts):\n",
    "            grams.add(gram)\n",
    "        else:\n",
    "            if len(gram) > min(n_range):\n",
    "                Q.append(gram[:-1])\n",
    "    \n",
    "    if grams:\n",
    "        for genre in genre_counts:\n",
    "            for gram in grams:\n",
    "                ngram_prob = calculate_ngram_prob(gram, genre, ngram_freq, genre_counts, lambdas)\n",
    "                if ngram_prob > 0:\n",
    "                    probs[genre] += np.log(ngram_prob)\n",
    "    \n",
    "    probs = normalize_probs(probs)\n",
    "    predicted_genre = max(probs, key=probs.get)\n",
    "    \n",
    "    return predicted_genre, probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(train_data, features):\n",
    "    \"\"\"Prepares input data based on the specified features.\"\"\"\n",
    "    X_train = []\n",
    "    for row in train_data:\n",
    "        if features == \"content\":\n",
    "            input = row[\"text\"]\n",
    "        elif features == \"url\":\n",
    "            input = row[\"url_path\"]\n",
    "        elif features == \"url_and_content\":\n",
    "            input = row[\"url_path\"] + \" \" + row[\"text\"]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for features. Expected 'content', 'url', or 'url_and_content'.\")\n",
    "        X_train.append(input)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(tokenized_datasets, features, ngram_freq, genre_counts, lambdas, n_range, split=\"test\"):\n",
    "    \"\"\"Use the trained model to make predictions on the test set.\"\"\"\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    input_data = prepare_input_data(tokenized_datasets[split], features)\n",
    "    \n",
    "    for i, input in enumerate(tqdm(input_data)):\n",
    "        predicted_class, _ = predict_genre(input, ngram_freq, genre_counts, lambdas, n_range)\n",
    "        preds.append(predicted_class)\n",
    "        labels.append(tokenized_datasets[split][i][\"label\"])\n",
    "    \n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy, precision, recall, and F1 score for the given labels and predictions and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary'),\n",
    "        'recall': recall_score(labels, preds, average='binary'),\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_from_dataset(dataset, n=5, subset='test'):\n",
    "    \"\"\"\n",
    "    Samples n random examples from a specified subset of the dataset.\n",
    "    \"\"\"\n",
    "    n = min(n, len(dataset[subset]))\n",
    "    random_indices = random.sample(range(len(dataset[subset])), n)\n",
    "    sampled_dataset = dataset[subset].select(random_indices)\n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get chunk level predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model on cannabis ###### \n",
      "\n",
      "\n",
      "Trained model for cannabis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 5600/44432 [05:06<35:23, 18.28it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrained model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Use the trained model to make predictions on the test set\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m preds, labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFEATURES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenre_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSPLIT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m metrics \u001b[38;5;241m=\u001b[39m calc_metrics(labels, preds)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetrics for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(tokenized_datasets, features, ngram_freq, genre_counts, lambdas, n_range, split)\u001b[0m\n\u001b[1;32m      6\u001b[0m input_data \u001b[38;5;241m=\u001b[39m prepare_input_data(tokenized_datasets[split], features)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(input_data)):\n\u001b[0;32m----> 9\u001b[0m     predicted_class, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_genre\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenre_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     preds\u001b[38;5;241m.\u001b[39mappend(predicted_class)\n\u001b[1;32m     11\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(tokenized_datasets[split][i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 55\u001b[0m, in \u001b[0;36mpredict_genre\u001b[0;34m(url, ngram_freq, genre_counts, lambdas, n_range)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m genre \u001b[38;5;129;01min\u001b[39;00m genre_counts:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m gram \u001b[38;5;129;01min\u001b[39;00m grams:\n\u001b[0;32m---> 55\u001b[0m         ngram_prob \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_ngram_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenre\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mngram_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenre_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambdas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m ngram_prob \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     57\u001b[0m             probs[genre] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(ngram_prob)\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36mcalculate_ngram_prob\u001b[0;34m(ngram, genre, ngram_freq, genre_counts, lambdas)\u001b[0m\n\u001b[1;32m     17\u001b[0m lambda_n \u001b[38;5;241m=\u001b[39m lambdas\u001b[38;5;241m.\u001b[39mget(n, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m freq \u001b[38;5;241m=\u001b[39m ngram_freq[genre]\u001b[38;5;241m.\u001b[39mget(ngram, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m total_ngrams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mngram_freq\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgenre\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_ngrams \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1e-10\u001b[39m  \u001b[38;5;66;03m# Return a small non-zero value if no n-grams are found\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "\n",
    "    print(f\"\\n\\n###### Evaluating model on {topic} ###### \\n\\n\")\n",
    "    \n",
    "    if FEATURES == \"url\":\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_{SAMPLING}{SUFFIX}\")\n",
    "        \n",
    "        if SPLIT == \"holdout\":\n",
    "                dataset[\"holdout\"] = concatenate_datasets(\n",
    "                    [dataset[\"holdout\"], dataset[\"test\"]])\n",
    "        dataset = dataset.map(extract_url_path, num_proc=8) # Extract the path from the URL\n",
    "    else:\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "        \n",
    "        if SPLIT == \"holdout\":\n",
    "                dataset[\"holdout\"] = concatenate_datasets(\n",
    "                    [dataset[\"holdout\"], dataset[\"test\"]])\n",
    "                \n",
    "        dataset = dataset.map(extract_url_path, num_proc=8) # Extract the path from the URL\n",
    "\n",
    "    # Train the model\n",
    "    X_train = prepare_input_data(dataset['train'], FEATURES)\n",
    "    y_labels = dataset[\"train\"][\"label\"]\n",
    "    n_range = (2, 3, 4)\n",
    "    ngram_freq, genre_counts, lambdas = train_genre_classifier(X_train, y_labels, n_range)\n",
    "    print(f\"Trained model for {topic}\")\n",
    "    \n",
    "    # Use the trained model to make predictions on the test set\n",
    "    preds, labels = get_predictions(dataset, FEATURES, ngram_freq, genre_counts, lambdas, n_range, split=SPLIT)\n",
    "    metrics = calc_metrics(labels, preds)\n",
    "    print(f\"Metrics for {topic}: {metrics}\")\n",
    "    \n",
    "    # Add answers to the dataset\n",
    "    dataset[SPLIT] = dataset[SPLIT].add_column(\"preds\", preds)\n",
    "    dataset.save_to_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_LIB_{FEATURES}_{SPLIT}\")\n",
    "    \n",
    "    # Update the eval_results dictionary\n",
    "    eval_results[topic] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'cannabis': {'accuracy': 0.9565217391304348, 'precision': 0.9565217391304348, 'recall': 0.9565217391304348, 'f1': 0.9565217391304348}, 'kinder': {'accuracy': 0.8636363636363636, 'precision': 0.9444444444444444, 'recall': 0.7727272727272727, 'f1': 0.85}, 'energie': {'accuracy': 0.8695652173913043, 'precision': 0.9047619047619048, 'recall': 0.8260869565217391, 'f1': 0.8636363636363636}})\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Chunk Level Predictions and Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path =f\"eval_results_lib_{FEATURES}_{SPLIT}_chunks.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    eval_results = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majority_voting(answers):\n",
    "    \"\"\"Apply majority voting to a list of arbitrary classification answers.\"\"\"\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common()  # Get all common answers sorted by frequency\n",
    "\n",
    "    if not most_common:\n",
    "        return 0 # Handle empty input scenario\n",
    "\n",
    "    # Check for ties at the highest count\n",
    "    max_votes = most_common[0][1]\n",
    "    tied_classes = [cls for cls, votes in most_common if votes == max_votes]\n",
    "\n",
    "    if len(tied_classes) > 1:\n",
    "        return max(tied_classes)  # Return the maximum class label in case of a tie\n",
    "    return tied_classes[0]  # Return the class with the most votes\n",
    "\n",
    "majority_voting([1, 1, 2, 2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating on cannabis ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 410\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path', 'preds'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 3522.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for cannabis: {'accuracy': 0.9534883720930233, 'precision': 0.95, 'recall': 0.95, 'f1': 0.95}\n",
      "\n",
      "\n",
      "###### Evaluating on kinder ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 384\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path', 'preds'],\n",
      "        num_rows: 44\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:00<00:00, 3509.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for kinder: {'accuracy': 0.8837209302325582, 'precision': 0.9444444444444444, 'recall': 0.8095238095238095, 'f1': 0.8717948717948718}\n",
      "\n",
      "\n",
      "###### Evaluating on energie ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path', 'preds'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:00<00:00, 4048.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for energie: {'accuracy': 0.8695652173913043, 'precision': 0.9047619047619048, 'recall': 0.8260869565217391, 'f1': 0.8636363636363636}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results_pages = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "\n",
    "    print(f\"\\n\\n###### Evaluating on {topic} ###### \\n\\n\")\n",
    "    dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_LIB_{FEATURES}_{SPLIT}\")\n",
    "    \n",
    "    print(dataset)\n",
    "    \n",
    "    # Group dataset examples by URL, with a fallback to domain\n",
    "    grouped_dataset = {}\n",
    "    for example in tqdm(dataset[SPLIT]):\n",
    "        url = example.get(\"view_url\") or example.get(\"domain\")\n",
    "        example_filtered = {k: example[k] for k in [\"text\", \"domain\", \"preds\", \"label\", \"category\", \"annotation_type\", \"lang\"]}\n",
    "        grouped_dataset.setdefault(url, []).append(example_filtered)\n",
    "        \n",
    "    # Extract labels\n",
    "    labels = []\n",
    "    for url, chunks in grouped_dataset.items():\n",
    "        preds = [chunk[\"label\"] for chunk in chunks]\n",
    "        labels.append(max(preds))\n",
    "        \n",
    "    # Merge chunk level predictions\n",
    "    predictions = []\n",
    "    for url, chunks in grouped_dataset.items():\n",
    "        preds = [chunk[\"preds\"] for chunk in chunks]\n",
    "        pred = majority_voting([pred for pred in preds if pred > 0]) if max(preds) > 0 else 0\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Use the trained model to make predictions on the test set\n",
    "    metrics = calc_metrics(labels, predictions)\n",
    "    print(f\"Metrics for {topic}: {metrics}\")\n",
    "    \n",
    "    # Update the eval_results dictionary\n",
    "    eval_results_pages[topic] = metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'cannabis': {'accuracy': 0.9534883720930233, 'precision': 0.95, 'recall': 0.95, 'f1': 0.95}, 'kinder': {'accuracy': 0.8837209302325582, 'precision': 0.9444444444444444, 'recall': 0.8095238095238095, 'f1': 0.8717948717948718}, 'energie': {'accuracy': 0.8695652173913043, 'precision': 0.9047619047619048, 'recall': 0.8260869565217391, 'f1': 0.8636363636363636}})\n"
     ]
    }
   ],
   "source": [
    "print(eval_results_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Chunk Level Predictions and Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path = f\"eval_results_lib_{FEATURES}_{SPLIT}_pages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results_pages, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    eval_results_pages = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
