{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multiple Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set a seed for numpy module\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set a seed for torch module\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"\" #\"\", \"_holdout\", \"_extended\"\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url_and_content\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"kinder\", \"energie\"]\n",
    "#TOPICS = [\"cannabis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"distilbert/distilbert-base-multilingual-cased\",\n",
    "        \"google-bert/bert-base-multilingual-cased\", \n",
    "        \"FacebookAI/xlm-roberta-base\", \n",
    "        \"FacebookAI/xlm-roberta-large\", \n",
    "        \"dbmdz/bert-base-german-uncased\", \n",
    "        \"deepset/gelectra-large\",\n",
    "        \"deepset/gelectra-base\",\n",
    "        \"deepset/gbert-large\",\n",
    "        \"deepset/gbert-base\",\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "TRAINING_ARGS = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,             # number of training epochs # TODO\n",
    "    weight_decay=0.01,  # Weight decay if we apply some form of weight regularization.\n",
    "    logging_dir='./logs',  # Directory where the training logs will be stored.\n",
    "    logging_strategy=\"steps\",  # The logging strategy determines when to log\n",
    "    logging_steps=100,  # Number of steps between logging of training loss.\n",
    "    evaluation_strategy=\"steps\",  # Evaluation is done\n",
    "    eval_steps=100,  # Number of steps between evaluations.\n",
    "    load_best_model_at_end=True,  # load the best model at the end of training.\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    lr_scheduler_type='linear',  # The scheduler type to use, e.g., 'linear', 'cosine'\n",
    "    warmup_ratio=0.1  # Proportion of training to perform linear learning rate warmup for.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract URL-path:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'view_url': 'https://www.google.com/search?q=python+url+path',\n",
       " 'url_path': 'search?q=python+url+path'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "def extract_url_path(example):\n",
    "    view_url = example['view_url']\n",
    "    if \"://\" not in view_url:\n",
    "        view_url = \"http://\" + view_url  # Assume http if no protocol specified\n",
    "    parsed_url = urlparse(view_url)\n",
    "    new_url = urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment))\n",
    "    example['url_path'] = new_url.lstrip('/')  # Store the result in a new field\n",
    "    return example\n",
    "\n",
    "\n",
    "extract_url_path({\"view_url\": \"https://www.google.com/search?q=python+url+path\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy_score(p.label_ids, preds)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset for cannabis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/410 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 410/410 [00:00<00:00, 3440.53 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 2118.15 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model distilbert/distilbert-base-multilingual-cased on cannabis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'pre_classifier.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 410/410 [00:00<00:00, 3898.45 examples/s]\n",
      "Map: 100%|██████████| 46/46 [00:00<00:00, 2852.97 examples/s]\n",
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 21/156 00:04 < 00:31, 4.32 it/s, Epoch 0.38/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 53\u001b[0m\n\u001b[1;32m     43\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     44\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     45\u001b[0m     args\u001b[38;5;241m=\u001b[39mTRAINING_ARGS,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m#callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m training_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Update the eval_results dictionary\u001b[39;00m\n\u001b[1;32m     56\u001b[0m training_results[model_name][topic] \u001b[38;5;241m=\u001b[39m training_result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/transformers/trainer.py:1865\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "training_results = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "    \n",
    "    print(f\"Loading dataset for {topic}\")\n",
    "    if FEATURES == \"url\":\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_{SAMPLING}\")\n",
    "        dataset = dataset.map(extract_url_path) # Extract the path from the URL\n",
    "    else:\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "        dataset = dataset.map(extract_url_path) # Extract the path from the URL\n",
    "    \n",
    "    for model_name in MODELS: # -------------------------------------------------------------\n",
    "\n",
    "        print(f\"Training model {model_name} on {topic}\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    \n",
    "        # Define tokenization strategies\n",
    "        tokenization_strategies = {\n",
    "            \n",
    "            # Tokenize the content of the page\n",
    "            \"content\": lambda examples: tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True),\n",
    "            \n",
    "            # Tokenize the URL path\n",
    "            \"url\": lambda examples: tokenizer(examples[\"url_path\"], padding=\"max_length\", truncation=True),\n",
    "            \n",
    "            # Tokenize the URL path and the content of the page\n",
    "            \"url_and_content\": lambda examples: tokenizer(examples[\"url_path\"], examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "        }\n",
    "\n",
    "        # Tokenize dataset\n",
    "        tokenized_datasets = dataset.map(tokenization_strategies[FEATURES],\n",
    "            batched=True)\n",
    "        \n",
    "        \n",
    "        # Create a Trainer object\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=TRAINING_ARGS,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            compute_metrics=compute_metrics,\n",
    "            #callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        training_result = trainer.train()\n",
    "        \n",
    "        # Update the eval_results dictionary\n",
    "        training_results[model_name][topic] = training_result\n",
    "        \n",
    "        # Evaluate the model\n",
    "        eval_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "        print(\"Eval Results:\", eval_results)\n",
    "        \n",
    "        # Save the model\n",
    "        local_path = f\"../../models/{model_name.replace('/','_')}_{topic}_model_{FEATURES}\"\n",
    "        trainer.save_model(local_path)\n",
    "        tokenizer.save_pretrained(local_path)\n",
    "        \n",
    "        # Clear GPU memory to avoid memory errors\n",
    "        del model, tokenizer, tokenized_datasets, trainer\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path to save the dictionary\n",
    "file_path = f\"training_results_{FEATURES}.json\"\n",
    "\n",
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(training_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
