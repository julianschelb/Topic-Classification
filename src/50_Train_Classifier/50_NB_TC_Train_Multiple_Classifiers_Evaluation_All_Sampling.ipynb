{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Multiple Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from IPython.display import display, HTML\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set a seed for numpy module\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set a seed for torch module\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "SPLITS = [\"test\", \"holdout\", \"extended\"] # \"train\", \"test\", \"holdout\", \"extended\"\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url_and_content\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"kinder\", \"energie\"]\n",
    "#TOPICS = [\"cannabis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"deepset/gelectra-large\"  # \"deepset/gelectra-large\"\n",
    "STRATEGIES = [\"random\", \"stratified\", \"clustered\"]  # , \"shared_domain\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy, precision, recall, and F1 score for the given labels and predictions and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary'),\n",
    "        'recall': recall_score(labels, preds, average='binary'),\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_from_dataset(dataset, n=5, subset='test'):\n",
    "    \"\"\"\n",
    "    Samples n random examples from a specified subset of the dataset.\n",
    "    \"\"\"\n",
    "    n = min(n, len(dataset[subset]))\n",
    "    random_indices = random.sample(range(len(dataset[subset])), n)\n",
    "    sampled_dataset = dataset[subset].select(random_indices)\n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets, load_from_disk\n",
    "\n",
    "def load_and_merge_datasets(topic, model_name, sampling, suffix, max_content_length, features, splits):\n",
    "    \"\"\"Loads specified splits from disk and merges them into a single dataset. \"\"\"\n",
    "    datasets_to_merge = []\n",
    "    \n",
    "    for split in splits:\n",
    "        path = f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{sampling}{suffix}_{max_content_length}_sampling_{model_name.split('/')[1]}_{features}_{split}\"\n",
    "        try:\n",
    "            dataset = load_from_disk(path)\n",
    "            if split in dataset:\n",
    "                datasets_to_merge.append(dataset[split])\n",
    "            else:\n",
    "                print(f\"Warning: Split '{split}' not found in the loaded dataset from {path}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading split '{split}' from path '{path}': {e}\")\n",
    "    \n",
    "    if datasets_to_merge:\n",
    "        merged_dataset = concatenate_datasets(datasets_to_merge)\n",
    "        return merged_dataset\n",
    "    else:\n",
    "        print(\"No valid splits provided for merging.\")\n",
    "        return None\n",
    "\n",
    "def load_dataset(topic, model_name, sampling, suffix, max_content_length, features, split):\n",
    "    path = f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{sampling}{suffix}_{max_content_length}_s_{model_name.split('/')[1]}_{features}_{split}\"\n",
    "    return load_from_disk(path)\n",
    "\n",
    "def merge_dataset_splits(dataset_dict, splits):\n",
    "    \"\"\"Merges specified splits from a DatasetDict into a single dataset.\"\"\"\n",
    "    datasets_to_merge = []\n",
    "    \n",
    "    for split in splits:\n",
    "        if split in dataset_dict:\n",
    "            datasets_to_merge.append(dataset_dict[split])\n",
    "        else:\n",
    "            print(f\"Warning: Split '{split}' not found in dataset_dict.\")\n",
    "    \n",
    "    if datasets_to_merge:\n",
    "        merged_dataset = concatenate_datasets(datasets_to_merge)\n",
    "        return merged_dataset\n",
    "    else:\n",
    "        print(\"No valid splits provided for merging.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def group_dataset_by_url(dataset):\n",
    "    grouped_dataset = defaultdict(list)\n",
    "    keys_to_extract = [\"text\", \"domain\", \"preds\", \"label\", \"category\", \"annotation_type\", \"lang\"]\n",
    "\n",
    "    for example in dataset:\n",
    "        url = example.get(\"view_url\") or example.get(\"domain\")\n",
    "        example_filtered = {key: example[key] for key in keys_to_extract}\n",
    "        grouped_dataset[url].append(example_filtered)\n",
    "\n",
    "    return dict(grouped_dataset)\n",
    "\n",
    "\n",
    "def extract_labels(grouped_dataset):\n",
    "    labels = []\n",
    "    for chunks in grouped_dataset.values():\n",
    "        preds = [chunk[\"label\"] for chunk in chunks]\n",
    "        labels.append(max(preds))\n",
    "    return labels\n",
    "\n",
    "def merge_predictions(grouped_dataset):\n",
    "    predictions = []\n",
    "    for chunks in grouped_dataset.values():\n",
    "        preds = [chunk[\"preds\"] for chunk in chunks]\n",
    "        predictions.append(max(preds))\n",
    "    return predictions\n",
    "\n",
    "def merge_probabilities(grouped_dataset):\n",
    "    probas = []\n",
    "    for chunks in grouped_dataset.values():\n",
    "        probas.append(max(chunk[\"probas\"] for chunk in chunks))\n",
    "    return probas\n",
    "\n",
    "def plot_precision_recall_curve(recall, precision, pr_auc, model_name, topic, splits):\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f'PR AUC = {pr_auc:.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'P-R Curve {model_name} - {topic}')\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    filename = f'precision_recall_curve_{model_name.split(\"/\")[1]}_{topic}_{\"_\".join(splits)}.png'.replace(' ', '_')\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PR-Curves per Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Loading and processing data for cannabis and strategy random ###### \n",
      "\n",
      "\n",
      "\n",
      "###### Loading and processing data for kinder and strategy random ###### \n",
      "\n",
      "\n",
      "\n",
      "###### Loading and processing data for energie and strategy random ###### \n",
      "\n",
      "\n",
      "\n",
      "###### Loading and processing data for cannabis and strategy stratified ###### \n",
      "\n",
      "\n",
      "\n",
      "###### Loading and processing data for kinder and strategy stratified ###### \n",
      "\n",
      "\n",
      "\n",
      "###### Loading and processing data for energie and strategy stratified ###### \n",
      "\n",
      "\n",
      "\n",
      "###### Loading and processing data for cannabis and strategy clustered ###### \n",
      "\n",
      "\n",
      "\n",
      "###### Loading and processing data for kinder and strategy clustered ###### \n",
      "\n",
      "\n",
      "\n",
      "###### Loading and processing data for energie and strategy clustered ###### \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Finished processing stratified on cannabis.\n",
      "Finished processing random on cannabis.\n",
      "Finished processing clustered on cannabis.\n",
      "Finished processing stratified on energie.\n",
      "Finished processing random on energie.\n",
      "Finished processing clustered on energie.\n",
      "Finished processing stratified on kinder.\n",
      "Finished processing clustered on kinder.\n",
      "Finished processing random on kinder.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "# Dictionary to store the processed data\n",
    "data_dict = defaultdict(dict)\n",
    "\n",
    "def process_topic(strategy, topic):\n",
    "    print(f\"\\n\\n###### Loading and processing data for {topic} and strategy {strategy} ###### \\n\\n\")\n",
    "\n",
    "    try:\n",
    "        # Load and merge datasets\n",
    "        dataset = load_and_merge_datasets(topic, MODEL, strategy, SUFFIX, MAX_CONTENT_LENGTH, FEATURES, SPLITS)\n",
    "\n",
    "        # Group dataset by URL\n",
    "        grouped_dataset = group_dataset_by_url(dataset)\n",
    "\n",
    "        # Extract labels and probabilities\n",
    "        labels = extract_labels(grouped_dataset)\n",
    "        predictions = merge_predictions(grouped_dataset)\n",
    "\n",
    "        print(f\"Finished processing {strategy} on {topic}.\")\n",
    "        return (strategy, topic, labels, predictions)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {strategy} on {topic}: {e}\")\n",
    "        return (strategy, topic, None, None)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=9) as executor:\n",
    "    future_to_topic = {executor.submit(process_topic, strategy, topic): (strategy, topic) for strategy in STRATEGIES for topic in TOPICS}\n",
    "    for future in as_completed(future_to_topic):\n",
    "        strategy, topic = future_to_topic[future]\n",
    "        labels, predictions = future.result()[2], future.result()[3]\n",
    "        if labels is not None and predictions is not None:\n",
    "            data_dict[strategy][topic] = (labels, predictions)\n",
    "\n",
    "#print(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['stratified', 'random', 'clustered'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for deepset/gelectra-large on kinder: {'accuracy': 0.9938920756320374, 'precision': 0.15555555555555556, 'recall': 0.9545454545454546, 'f1': 0.267515923566879}\n",
      "Metrics for deepset/gelectra-large on energie: {'accuracy': 0.9768026328911937, 'precision': 0.0413564929693962, 'recall': 0.9259259259259259, 'f1': 0.0791765637371338}\n",
      "Metrics for deepset/gelectra-large on cannabis: {'accuracy': 0.9966075429815928, 'precision': 0.22966507177033493, 'recall': 0.9795918367346939, 'f1': 0.37209302325581395}\n",
      "Metrics for deepset/gelectra-large on kinder: {'accuracy': 0.9901920863946181, 'precision': 0.10260586319218241, 'recall': 0.9545454545454546, 'f1': 0.18529411764705883}\n",
      "Metrics for deepset/gelectra-large on energie: {'accuracy': 0.9881918819188192, 'precision': 0.07836990595611286, 'recall': 0.9259259259259259, 'f1': 0.14450867052023122}\n",
      "Metrics for deepset/gelectra-large on cannabis: {'accuracy': 0.9983037714907964, 'precision': 0.375, 'recall': 0.9795918367346939, 'f1': 0.5423728813559322}\n",
      "Metrics for deepset/gelectra-large on kinder: {'accuracy': 0.9861730755612209, 'precision': 0.07491082045184304, 'recall': 0.9545454545454546, 'f1': 0.13891951488423374}\n",
      "Metrics for deepset/gelectra-large on energie: {'accuracy': 0.9788371397227486, 'precision': 0.045167118337850046, 'recall': 0.9259259259259259, 'f1': 0.08613264427217916}\n",
      "Metrics for deepset/gelectra-large on cannabis: {'accuracy': 0.9978430674512596, 'precision': 0.32, 'recall': 0.9795918367346939, 'f1': 0.4824120603015075}\n",
      "{'random': {'kinder': {'accuracy': 0.9938920756320374, 'precision': 0.15555555555555556, 'recall': 0.9545454545454546, 'f1': 0.267515923566879}, 'energie': {'accuracy': 0.9768026328911937, 'precision': 0.0413564929693962, 'recall': 0.9259259259259259, 'f1': 0.0791765637371338}, 'cannabis': {'accuracy': 0.9966075429815928, 'precision': 0.22966507177033493, 'recall': 0.9795918367346939, 'f1': 0.37209302325581395}}, 'stratified': {'kinder': {'accuracy': 0.9901920863946181, 'precision': 0.10260586319218241, 'recall': 0.9545454545454546, 'f1': 0.18529411764705883}, 'energie': {'accuracy': 0.9881918819188192, 'precision': 0.07836990595611286, 'recall': 0.9259259259259259, 'f1': 0.14450867052023122}, 'cannabis': {'accuracy': 0.9983037714907964, 'precision': 0.375, 'recall': 0.9795918367346939, 'f1': 0.5423728813559322}}, 'clustered': {'kinder': {'accuracy': 0.9861730755612209, 'precision': 0.07491082045184304, 'recall': 0.9545454545454546, 'f1': 0.13891951488423374}, 'energie': {'accuracy': 0.9788371397227486, 'precision': 0.045167118337850046, 'recall': 0.9259259259259259, 'f1': 0.08613264427217916}, 'cannabis': {'accuracy': 0.9978430674512596, 'precision': 0.32, 'recall': 0.9795918367346939, 'f1': 0.4824120603015075}}}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Mapping of topics to legend texts\n",
    "legend_texts = {\n",
    "    'kinder': 'Children',\n",
    "    'energie': 'Energy',\n",
    "    'cannabis': 'Cannabis'\n",
    "}\n",
    "\n",
    "# Desired order of topics for calculation\n",
    "ordered_topics = ['kinder', 'energie', 'cannabis']\n",
    "\n",
    "# Dictionary to store evaluation results\n",
    "evaluation_results = defaultdict(dict)\n",
    "\n",
    "# Iterate over each model\n",
    "for strategy in STRATEGIES:\n",
    "    \n",
    "    for topic in ordered_topics:\n",
    "        if topic in data_dict[strategy]:\n",
    "            labels, predictions = data_dict[strategy][topic]\n",
    "\n",
    "            # Calculate additional metrics\n",
    "            predictions_binary = [1 if p >= 0.5 else 0 for p in predictions]\n",
    "            topic_precision = precision_score(labels, predictions)\n",
    "            topic_recall = recall_score(labels, predictions)\n",
    "            topic_accuracy = accuracy_score(labels, predictions)\n",
    "            topic_f1 = f1_score(labels, predictions)\n",
    "\n",
    "            # Store metrics in the dictionary\n",
    "            evaluation_results[strategy][topic] = {\n",
    "                'accuracy': topic_accuracy,\n",
    "                'precision': topic_precision,\n",
    "                'recall': topic_recall,\n",
    "                'f1': topic_f1\n",
    "            }\n",
    "\n",
    "            print(f\"Metrics for {MODEL} on {topic}: {evaluation_results[strategy][topic]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert the evaluation results to a nested dictionary\n",
    "evaluation_results = dict(evaluation_results)\n",
    "\n",
    "# Display evaluation results\n",
    "print(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the file path to save the dictionary\n",
    "file_path = f\"eval_results_{FEATURES}_sampling_all_pages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(evaluation_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
