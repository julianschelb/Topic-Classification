{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multiple Random Forrest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set a seed for numpy module\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set a seed for torch module\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "SPLIT = \"extended\" # \"train\", \"test\", \"holdout\", \"extended\"\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"kinder\", \"energie\"]\n",
    "#TOPICS = [\"cannabis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract URL-path:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'view_url': 'https://www.google.com/search?q=python+url+path',\n",
       " 'url_path': 'search?q=python+url+path'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "def extract_url_path(example):\n",
    "    view_url = example['view_url']\n",
    "    if \"://\" not in view_url:\n",
    "        view_url = \"http://\" + view_url  # Assume http if no protocol specified\n",
    "    parsed_url = urlparse(view_url)\n",
    "    new_url = urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment))\n",
    "    example['url_path'] = new_url.lstrip('/')  # Store the result in a new field\n",
    "    return example\n",
    "\n",
    "\n",
    "extract_url_path({\"view_url\": \"https://www.google.com/search?q=python+url+path\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jschelb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "german_stop_words = stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(train_data, features):\n",
    "    \"\"\"Prepares input data based on the specified features.\"\"\"\n",
    "    X_train = []\n",
    "    for row in train_data:\n",
    "        if features == \"content\":\n",
    "            input = row[\"text\"]\n",
    "        elif features == \"url\":\n",
    "            input = row[\"url_path\"]\n",
    "        elif features == \"url_and_content\":\n",
    "            input = row[\"url_path\"] + \" \" + row[\"text\"]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for features. Expected 'content', 'url', or 'url_and_content'.\")\n",
    "        X_train.append(input)\n",
    "    return X_train\n",
    "\n",
    "\n",
    "def train_model_rf(train_data, german_stop_words, features, max_features=10000):\n",
    "    \"\"\"Trains an RF model and returns the model and vectorizer.\"\"\"\n",
    "    \n",
    "    X_train = prepare_input_data(train_data, features)\n",
    "        \n",
    "    # Create a TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words=german_stop_words, max_features=max_features)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    y_train = np.array(train_data['label'])\n",
    "    \n",
    "    # Train an SVM classifier\n",
    "    # Initialize the Random Forest classifier\n",
    "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)  \n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "    return rf_classifier, vectorizer\n",
    "\n",
    "def get_predictions_rf(model, vectorizer, new_data):\n",
    "    \"\"\"Gets predictions for new data using the trained model and vectorizer.\"\"\"\n",
    "    X_new = vectorizer.transform(new_data)\n",
    "    return model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(tokenized_datasets, model, vectorizer, features, split=\"test\"):\n",
    "    \"\"\"Use the trained model to make predictions on the test set.\"\"\"\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    input_data = prepare_input_data(tokenized_datasets[split], features)\n",
    "    \n",
    "    for i, input in enumerate(tqdm(input_data)):\n",
    "        predicted_class = get_predictions_rf(model, vectorizer, [input])[0]\n",
    "        preds.append(predicted_class)\n",
    "        labels.append(tokenized_datasets[split][i][\"label\"])\n",
    "    \n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy, precision, recall, and F1 score for the given labels and predictions and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary'),\n",
    "        'recall': recall_score(labels, preds, average='binary'),\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_from_dataset(dataset, n=5, subset='test'):\n",
    "    \"\"\"\n",
    "    Samples n random examples from a specified subset of the dataset.\n",
    "    \"\"\"\n",
    "    n = min(n, len(dataset[subset]))\n",
    "    random_indices = random.sample(range(len(dataset[subset])), n)\n",
    "    sampled_dataset = dataset[subset].select(random_indices)\n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get chunk level predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model on cannabis ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 3448/3448 [00:00<00:00, 11158.17 examples/s]\n",
      "100%|██████████| 44432/44432 [04:04<00:00, 181.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for cannabis: {'accuracy': 0.9984470651782499, 'precision': 0.2872340425531915, 'recall': 0.9310344827586207, 'f1': 0.43902439024390244}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 410/410 [00:00<00:00, 26753.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 46/46 [00:00<00:00, 4460.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3448/3448 [00:00<00:00, 65420.97 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 44432/44432 [00:00<00:00, 212242.36 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model on kinder ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53253/53253 [04:53<00:00, 181.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for kinder: {'accuracy': 0.07729142020167878, 'precision': 0.000914968891057704, 'recall': 1.0, 'f1': 0.0018282649765372662}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 384/384 [00:00<00:00, 26622.58 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 44/44 [00:00<00:00, 4202.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 3722/3722 [00:00<00:00, 73213.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 53253/53253 [00:00<00:00, 230302.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model on energie ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45925/45925 [04:14<00:00, 180.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for energie: {'accuracy': 0.20651061513336963, 'precision': 0.0008499670980478175, 'recall': 1.0, 'f1': 0.0016984905350245186}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 408/408 [00:00<00:00, 26987.90 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 46/46 [00:00<00:00, 4473.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4164/4164 [00:00<00:00, 68431.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 45925/45925 [00:00<00:00, 225815.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "\n",
    "    print(f\"\\n\\n###### Evaluating model on {topic} ###### \\n\\n\")\n",
    "    \n",
    "    if FEATURES == \"url\":\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_{SAMPLING}{SUFFIX}\")\n",
    "        \n",
    "        if SPLIT == \"holdout\":\n",
    "                dataset[\"holdout\"] = concatenate_datasets(\n",
    "                    [dataset[\"holdout\"], dataset[\"test\"]])\n",
    "        dataset = dataset.map(extract_url_path, num_proc=8) # Extract the path from the URL\n",
    "    else:\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "        \n",
    "        if SPLIT == \"holdout\":\n",
    "                dataset[\"holdout\"] = concatenate_datasets(\n",
    "                    [dataset[\"holdout\"], dataset[\"test\"]])\n",
    "                \n",
    "        dataset = dataset.map(extract_url_path, num_proc=8) # Extract the path from the URL\n",
    "        \n",
    "\n",
    "    # Train Model\n",
    "    model, vectorizer = train_model_rf(dataset['train'], german_stop_words, FEATURES)\n",
    "    \n",
    "    # Use the trained model to make predictions on the test set\n",
    "    preds, labels = get_predictions(dataset, model, vectorizer, FEATURES, split=SPLIT)\n",
    "    metrics = calc_metrics(labels, preds)\n",
    "    print(f\"Metrics for {topic}: {metrics}\")\n",
    "    \n",
    "    # Add answers to the dataset\n",
    "    dataset[SPLIT] = dataset[SPLIT].add_column(\"preds\", preds)\n",
    "    dataset.save_to_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_RF_{FEATURES}_{SPLIT}\")\n",
    "    \n",
    "    # Update the eval_results dictionary\n",
    "    eval_results[topic] = metrics\n",
    "    \n",
    "    # Clear GPU memory to avoid memory errors\n",
    "    del model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'cannabis': {'accuracy': 0.9984470651782499, 'precision': 0.2872340425531915, 'recall': 0.9310344827586207, 'f1': 0.43902439024390244}, 'kinder': {'accuracy': 0.07729142020167878, 'precision': 0.000914968891057704, 'recall': 1.0, 'f1': 0.0018282649765372662}, 'energie': {'accuracy': 0.20651061513336963, 'precision': 0.0008499670980478175, 'recall': 1.0, 'f1': 0.0016984905350245186}})\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Chunk Level Predictions and Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path =f\"eval_results_rf_{FEATURES}_{SPLIT}_chunks.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    eval_results = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majority_voting(answers):\n",
    "    \"\"\"Apply majority voting to a list of arbitrary classification answers.\"\"\"\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common()  # Get all common answers sorted by frequency\n",
    "\n",
    "    if not most_common:\n",
    "        return 0 # Handle empty input scenario\n",
    "\n",
    "    # Check for ties at the highest count\n",
    "    max_votes = most_common[0][1]\n",
    "    tied_classes = [cls for cls, votes in most_common if votes == max_votes]\n",
    "\n",
    "    if len(tied_classes) > 1:\n",
    "        return max(tied_classes)  # Return the maximum class label in case of a tie\n",
    "    return tied_classes[0]  # Return the class with the most votes\n",
    "\n",
    "majority_voting([1, 1, 2, 2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating on cannabis ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 410\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 3448\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path', 'preds'],\n",
      "        num_rows: 44432\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44432/44432 [00:08<00:00, 5458.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for cannabis: {'accuracy': 0.9984865255596467, 'precision': 0.29347826086956524, 'recall': 0.9310344827586207, 'f1': 0.4462809917355372}\n",
      "\n",
      "\n",
      "###### Evaluating on kinder ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 384\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 44\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 3722\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path', 'preds'],\n",
      "        num_rows: 53253\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53253/53253 [00:09<00:00, 5639.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for kinder: {'accuracy': 0.07793833200591649, 'precision': 0.0009246132034765456, 'recall': 1.0, 'f1': 0.0018475181672619782}\n",
      "\n",
      "\n",
      "###### Evaluating on energie ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 408\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 46\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path'],\n",
      "        num_rows: 4164\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'url_path', 'preds'],\n",
      "        num_rows: 45925\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45925/45925 [00:08<00:00, 5643.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for energie: {'accuracy': 0.20651061513336963, 'precision': 0.0008499670980478175, 'recall': 1.0, 'f1': 0.0016984905350245186}\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results_pages = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "\n",
    "    print(f\"\\n\\n###### Evaluating on {topic} ###### \\n\\n\")\n",
    "    dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_RF_{FEATURES}_{SPLIT}\")\n",
    "    \n",
    "    print(dataset)\n",
    "    \n",
    "    # Group dataset examples by URL, with a fallback to domain\n",
    "    grouped_dataset = {}\n",
    "    for example in tqdm(dataset[SPLIT]):\n",
    "        url = example.get(\"view_url\") or example.get(\"domain\")\n",
    "        example_filtered = {k: example[k] for k in [\"text\", \"domain\", \"preds\", \"label\", \"category\", \"annotation_type\", \"lang\"]}\n",
    "        grouped_dataset.setdefault(url, []).append(example_filtered)\n",
    "        \n",
    "    # Extract labels\n",
    "    labels = []\n",
    "    for url, chunks in grouped_dataset.items():\n",
    "        preds = [chunk[\"label\"] for chunk in chunks]\n",
    "        labels.append(max(preds))\n",
    "        \n",
    "    # Merge chunk level predictions\n",
    "    predictions = []\n",
    "    for url, chunks in grouped_dataset.items():\n",
    "        preds = [chunk[\"preds\"] for chunk in chunks]\n",
    "        pred = majority_voting([pred for pred in preds if pred > 0]) if max(preds) > 0 else 0\n",
    "        predictions.append(pred)\n",
    "\n",
    "    # Use the trained model to make predictions on the test set\n",
    "    metrics = calc_metrics(labels, predictions)\n",
    "    print(f\"Metrics for {topic}: {metrics}\")\n",
    "    \n",
    "    # Update the eval_results dictionary\n",
    "    eval_results_pages[topic] = metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'cannabis': {'accuracy': 0.9984865255596467, 'precision': 0.29347826086956524, 'recall': 0.9310344827586207, 'f1': 0.4462809917355372}, 'kinder': {'accuracy': 0.07793833200591649, 'precision': 0.0009246132034765456, 'recall': 1.0, 'f1': 0.0018475181672619782}, 'energie': {'accuracy': 0.20651061513336963, 'precision': 0.0008499670980478175, 'recall': 1.0, 'f1': 0.0016984905350245186}})\n"
     ]
    }
   ],
   "source": [
    "print(eval_results_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Chunk Level Predictions and Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path = f\"eval_results_rf_{FEATURES}_{SPLIT}_pages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results_pages, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    eval_results_pages = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
