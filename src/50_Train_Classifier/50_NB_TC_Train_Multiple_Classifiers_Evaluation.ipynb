{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Multiple Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jschelb/.pyenv/versions/3.10.8/envs/s2j-content-analysis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed for random module\n",
    "random.seed(42)\n",
    "\n",
    "# Set a seed for numpy module\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set a seed for torch module\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING = \"random\" # \"random\", \"stratified\", \"clustered\", \"shared_domain\"\n",
    "SUFFIX = \"_extended\" #\"\", \"_holdout\", \"_extended\"\n",
    "SPLIT = \"test\" # \"train\", \"test\", \"holdout\", \"extende\n",
    "MAX_CONTENT_LENGTH = 384 # 496, 192\n",
    "OVERLAP = 64\n",
    "FEATURES = \"url_and_content\" # \"url\", \"content\", \"url_and_content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = [\"cannabis\", \"kinder\", \"energie\"]\n",
    "#TOPICS = [\"cannabis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = [\"distilbert/distilbert-base-multilingual-cased\",\n",
    "          \"google-bert/bert-base-multilingual-cased\", \n",
    "          #\"FacebookAI/xlm-roberta-base\", \n",
    "          #\"FacebookAI/xlm-roberta-large\", \n",
    "          #\"dbmdz/bert-base-german-uncased\", \n",
    "          #\"deepset/gelectra-large\",\n",
    "          #\"deepset/gelectra-base\",\n",
    "          #\"deepset/gbert-large\",\n",
    "          #\"deepset/gbert-base\",\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_ID = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract URL-path:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'view_url': 'https://www.google.com/search?q=python+url+path',\n",
       " 'url_path': 'search?q=python+url+path'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urlunparse\n",
    "\n",
    "def extract_url_path(example):\n",
    "    view_url = example['view_url']\n",
    "    if \"://\" not in view_url:\n",
    "        view_url = \"http://\" + view_url  # Assume http if no protocol specified\n",
    "    parsed_url = urlparse(view_url)\n",
    "    new_url = urlunparse(('', '', parsed_url.path, parsed_url.params, parsed_url.query, parsed_url.fragment))\n",
    "    example['url_path'] = new_url.lstrip('/')  # Store the result in a new field\n",
    "    return example\n",
    "\n",
    "\n",
    "extract_url_path({\"view_url\": \"https://www.google.com/search?q=python+url+path\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(tokenized_datasets, tokenizer, model, device, features, split=\"test\"):\n",
    "    \"\"\"Use the trained model to make predictions on the test set.\"\"\"\n",
    "    \n",
    "    preds = []\n",
    "    labels = []\n",
    "    probabilities = []\n",
    "    \n",
    "    for row in tqdm(tokenized_datasets[split]):\n",
    "        # Encode the text inputs\n",
    "        if features == \"content\":\n",
    "            inputs = tokenizer(row[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        elif features == \"url\":\n",
    "            inputs = tokenizer(row[\"url_path\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        elif features == \"url_and_content\":\n",
    "            inputs = tokenizer(row[\"url_path\"], row[\"text\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for FEATURES. Expected 'content', 'url', or 'url_and_content'.\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Forward pass\n",
    "            outputs = model(**inputs.to(device))\n",
    "            # Apply softmax to logits to get probabilities\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            # Get the predicted class (the one with the highest probability)\n",
    "            predicted_class = torch.argmax(predictions).item()\n",
    "        \n",
    "        # Store the predictions, labels, and probabilities\n",
    "        preds.append(predicted_class)\n",
    "        labels.append(row[\"label\"])\n",
    "        probabilities.append(predictions.cpu().numpy().tolist()[0][1])# Store the probability of the positive class\n",
    "    \n",
    "    return preds, labels, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(labels, preds):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy, precision, recall, and F1 score for the given labels and predictions and returns them in a dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, preds),\n",
    "        'precision': precision_score(labels, preds, average='binary'),\n",
    "        'recall': recall_score(labels, preds, average='binary'),\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_random_from_dataset(dataset, n=5, subset='test'):\n",
    "    \"\"\"\n",
    "    Samples n random examples from a specified subset of the dataset.\n",
    "    \"\"\"\n",
    "    n = min(n, len(dataset[subset]))\n",
    "    random_indices = random.sample(range(len(dataset[subset])), n)\n",
    "    sampled_dataset = dataset[subset].select(random_indices)\n",
    "    return sampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get chunk level predictions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model distilbert/distilbert-base-multilingual-cased on cannabis ###### \n",
      "\n",
      "\n",
      "Loading model from ../../models/distilbert_distilbert-base-multilingual-cased_cannabis_model_url_and_content/\n",
      "Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:09<00:00, 54.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for distilbert/distilbert-base-multilingual-cased on cannabis: {'accuracy': 0.9960552268244576, 'precision': 0.9939759036144579, 'recall': 1.0, 'f1': 0.9969788519637462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3815/3815 [00:00<00:00, 231292.38 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 507/507 [00:00<00:00, 71800.39 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33702/33702 [00:00<00:00, 529002.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 224737/224737 [00:00<00:00, 573958.03 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model google-bert/bert-base-multilingual-cased on cannabis ###### \n",
      "\n",
      "\n",
      "Loading model from ../../models/google-bert_bert-base-multilingual-cased_cannabis_model_url_and_content/\n",
      "Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:15<00:00, 32.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for google-bert/bert-base-multilingual-cased on cannabis: {'accuracy': 0.9960552268244576, 'precision': 0.9939759036144579, 'recall': 1.0, 'f1': 0.9969788519637462}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3815/3815 [00:00<00:00, 288326.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 507/507 [00:00<00:00, 72572.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33702/33702 [00:00<00:00, 530252.99 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 224737/224737 [00:00<00:00, 595474.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model distilbert/distilbert-base-multilingual-cased on kinder ###### \n",
      "\n",
      "\n",
      "Loading model from ../../models/distilbert_distilbert-base-multilingual-cased_kinder_model_url_and_content/\n",
      "Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:04<00:00, 63.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for distilbert/distilbert-base-multilingual-cased on kinder: {'accuracy': 0.9968354430379747, 'precision': 1.0, 'recall': 0.9950980392156863, 'f1': 0.9975429975429976}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3628/3628 [00:00<00:00, 286754.89 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 316/316 [00:00<00:00, 40845.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33730/33730 [00:00<00:00, 547713.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 266322/266322 [00:00<00:00, 584014.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model google-bert/bert-base-multilingual-cased on kinder ###### \n",
      "\n",
      "\n",
      "Loading model from ../../models/google-bert_bert-base-multilingual-cased_kinder_model_url_and_content/\n",
      "Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:09<00:00, 32.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for google-bert/bert-base-multilingual-cased on kinder: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 3628/3628 [00:00<00:00, 288708.04 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 316/316 [00:00<00:00, 39063.93 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 33730/33730 [00:00<00:00, 552218.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 266322/266322 [00:00<00:00, 600956.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model distilbert/distilbert-base-multilingual-cased on energie ###### \n",
      "\n",
      "\n",
      "Loading model from ../../models/distilbert_distilbert-base-multilingual-cased_energie_model_url_and_content/\n",
      "Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:09<00:00, 64.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for distilbert/distilbert-base-multilingual-cased on energie: {'accuracy': 0.9965457685664939, 'precision': 0.9924812030075187, 'recall': 1.0, 'f1': 0.9962264150943396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4227/4227 [00:00<00:00, 297596.69 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 579/579 [00:00<00:00, 73486.31 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 39782/39782 [00:00<00:00, 517229.75 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 229661/229661 [00:00<00:00, 579267.89 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model google-bert/bert-base-multilingual-cased on energie ###### \n",
      "\n",
      "\n",
      "Loading model from ../../models/google-bert_bert-base-multilingual-cased_energie_model_url_and_content/\n",
      "Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:17<00:00, 32.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for google-bert/bert-base-multilingual-cased on energie: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 4227/4227 [00:00<00:00, 303158.63 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 579/579 [00:00<00:00, 71677.40 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 39782/39782 [00:00<00:00, 546716.26 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 229661/229661 [00:00<00:00, 598711.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "    \n",
    "    #print(f\"Loading dataset for {topic}\")\n",
    "    \n",
    "    for model_name in MODELS: # -------------------------------------------------------------\n",
    "\n",
    "        print(f\"\\n\\n###### Evaluating model {model_name} on {topic} ###### \\n\\n\")\n",
    "            \n",
    "        if FEATURES == \"url\":\n",
    "            dataset = load_from_disk(\n",
    "                f\"../../data/tmp/processed_dataset_{topic}_buffed_{SAMPLING}{SUFFIX}\")\n",
    "\n",
    "            if SPLIT == \"holdout\":\n",
    "                dataset[\"holdout\"] = concatenate_datasets(\n",
    "                    [dataset[\"holdout\"], dataset[\"test\"]])\n",
    "            # Extract the path from the URL\n",
    "            dataset = dataset.map(extract_url_path, num_proc=8)\n",
    "        else:\n",
    "            dataset = load_from_disk(\n",
    "                f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}\")\n",
    "\n",
    "            if SPLIT == \"holdout\":\n",
    "                dataset[\"holdout\"] = concatenate_datasets(\n",
    "                    [dataset[\"holdout\"], dataset[\"test\"]])\n",
    "                \n",
    "            # Extract the path from the URL\n",
    "            dataset = dataset.map(extract_url_path)\n",
    "            # dataset['test'] = sample_random_from_dataset(dataset, n=5, subset='test')\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        model_name_local = f\"../../models/{model_name.replace('/','_')}_{topic}_model_{FEATURES}/\"\n",
    "        print(f\"Loading model from {model_name_local}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name_local, num_labels=2, local_files_only=True)\n",
    "        \n",
    "        # Use multiple GPUs if available\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "            model = torch.nn.DataParallel(model)\n",
    "            \n",
    "        # Move model to GPU if available\n",
    "        DEVICE = torch.device(f\"cuda:{CUDA_ID}\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(DEVICE)\n",
    "        \n",
    "        # Use the trained model to make predictions on the test set\n",
    "        preds, labels, probas = get_predictions(dataset, tokenizer, model, DEVICE, FEATURES, split=SPLIT)\n",
    "        metrics = calc_metrics(labels, preds)\n",
    "        print(f\"Metrics for {model_name} on {topic}: {metrics}\")\n",
    "        \n",
    "        # Add answers to the dataset\n",
    "        dataset[SPLIT] = dataset[SPLIT].add_column(\"preds\", preds)\n",
    "        dataset[SPLIT] = dataset[SPLIT].add_column(\"probas\", probas)\n",
    "        dataset.save_to_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_{model_name.split('/')[1]}_{FEATURES}_{SPLIT}\")\n",
    "        \n",
    "        # Update the eval_results dictionary\n",
    "        eval_results[model_name][topic] = metrics\n",
    "        \n",
    "        # Clear GPU memory to avoid memory errors\n",
    "        del model, tokenizer\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'distilbert/distilbert-base-multilingual-cased': {'cannabis': {'accuracy': 0.9960552268244576, 'precision': 0.9939759036144579, 'recall': 1.0, 'f1': 0.9969788519637462}, 'kinder': {'accuracy': 0.9968354430379747, 'precision': 1.0, 'recall': 0.9950980392156863, 'f1': 0.9975429975429976}, 'energie': {'accuracy': 0.9965457685664939, 'precision': 0.9924812030075187, 'recall': 1.0, 'f1': 0.9962264150943396}}, 'google-bert/bert-base-multilingual-cased': {'cannabis': {'accuracy': 0.9960552268244576, 'precision': 0.9939759036144579, 'recall': 1.0, 'f1': 0.9969788519637462}, 'kinder': {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}, 'energie': {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}}})\n"
     ]
    }
   ],
   "source": [
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Chunk Level Predictions and Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path =f\"eval_results_{FEATURES}_{SPLIT}_chunks.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    eval_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all topics (assuming all models are evaluated on the same topics)\n",
    "topics = list(next(iter(eval_results.values())).keys())\n",
    "\n",
    "# Prepare headers for the table: each topic will have four metrics\n",
    "headers = [\"Model\"] + [f\"{topic} {metric}\" for topic in topics for metric in [\"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# Prepare rows: one row per model, containing metrics for each topic\n",
    "rows = []\n",
    "for model, topics_metrics in eval_results.items():\n",
    "    row = [model]  # Start with the model name\n",
    "    for topic in topics:\n",
    "        metrics = topics_metrics.get(topic, {})\n",
    "        row.extend([metrics.get('accuracy',0.0), metrics.get('precision',0.0), metrics.get('recall',0.0), metrics.get('f1',0.0)])\n",
    "    rows.append(row)\n",
    "\n",
    "# Generate the HTML table\n",
    "table_html = tabulate(rows, headers=headers, tablefmt=\"html\", showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model                                        </th><th style=\"text-align: right;\">  cannabis Acc.</th><th style=\"text-align: right;\">  cannabis Prec.</th><th style=\"text-align: right;\">  cannabis Rec.</th><th style=\"text-align: right;\">  cannabis F1</th><th style=\"text-align: right;\">  kinder Acc.</th><th style=\"text-align: right;\">  kinder Prec.</th><th style=\"text-align: right;\">  kinder Rec.</th><th style=\"text-align: right;\">  kinder F1</th><th style=\"text-align: right;\">  energie Acc.</th><th style=\"text-align: right;\">  energie Prec.</th><th style=\"text-align: right;\">  energie Rec.</th><th style=\"text-align: right;\">  energie F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>distilbert/distilbert-base-multilingual-cased</td><td style=\"text-align: right;\">          0.996</td><td style=\"text-align: right;\">           0.994</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">        0.997</td><td style=\"text-align: right;\">        0.997</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        0.995</td><td style=\"text-align: right;\">      0.998</td><td style=\"text-align: right;\">         0.997</td><td style=\"text-align: right;\">          0.992</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       0.996</td></tr>\n",
       "<tr><td>google-bert/bert-base-multilingual-cased     </td><td style=\"text-align: right;\">          0.996</td><td style=\"text-align: right;\">           0.994</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">        0.997</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">      1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       1.000</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(table_html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is: google-bert/bert-base-multilingual-cased\n",
      "Average F1 score of the best model: 0.9990\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average F1 score for each model\n",
    "average_f1_scores = {}\n",
    "for model, categories in eval_results.items():\n",
    "    total_f1 = 0\n",
    "    count = 0\n",
    "    for category, metrics in categories.items():\n",
    "        total_f1 += metrics[\"f1\"]\n",
    "        count += 1\n",
    "    average_f1_scores[model] = total_f1 / count\n",
    "\n",
    "# Find the model with the highest average F1 score\n",
    "best_model = max(average_f1_scores, key=average_f1_scores.get)\n",
    "best_model_average_f1 = average_f1_scores[best_model]\n",
    "\n",
    "print(f\"The best model is: {best_model}\")\n",
    "print(f\"Average F1 score of the best model: {best_model_average_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Level Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def majority_voting(answers):\n",
    "    \"\"\"Apply majority voting to a list of arbitrary classification answers.\"\"\"\n",
    "    count = Counter(answers)\n",
    "    most_common = count.most_common()  # Get all common answers sorted by frequency\n",
    "\n",
    "    if not most_common:\n",
    "        return 0 # Handle empty input scenario\n",
    "\n",
    "    # Check for ties at the highest count\n",
    "    max_votes = most_common[0][1]\n",
    "    tied_classes = [cls for cls, votes in most_common if votes == max_votes]\n",
    "\n",
    "    if len(tied_classes) > 1:\n",
    "        return max(tied_classes)  # Return the maximum class label in case of a tie\n",
    "    return tied_classes[0]  # Return the class with the most votes\n",
    "\n",
    "majority_voting([1, 1, 2, 2, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###### Evaluating model distilbert/distilbert-base-multilingual-cased on cannabis ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 3815\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
      "        num_rows: 507\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 33702\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 224737\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:00<00:00, 4613.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for distilbert/distilbert-base-multilingual-cased on cannabis: {'accuracy': 0.9767441860465116, 'precision': 0.9523809523809523, 'recall': 1.0, 'f1': 0.975609756097561}\n",
      "\n",
      "\n",
      "###### Evaluating model google-bert/bert-base-multilingual-cased on cannabis ###### \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 3815\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
      "        num_rows: 507\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 33702\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 224737\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 507/507 [00:00<00:00, 4553.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for google-bert/bert-base-multilingual-cased on cannabis: {'accuracy': 0.9767441860465116, 'precision': 0.9523809523809523, 'recall': 1.0, 'f1': 0.975609756097561}\n",
      "\n",
      "\n",
      "###### Evaluating model distilbert/distilbert-base-multilingual-cased on kinder ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 3628\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
      "        num_rows: 316\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 33730\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 266322\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 4643.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for distilbert/distilbert-base-multilingual-cased on kinder: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "\n",
      "\n",
      "###### Evaluating model google-bert/bert-base-multilingual-cased on kinder ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 3628\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
      "        num_rows: 316\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 33730\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 266322\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316/316 [00:00<00:00, 5264.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for google-bert/bert-base-multilingual-cased on kinder: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n",
      "\n",
      "\n",
      "###### Evaluating model distilbert/distilbert-base-multilingual-cased on energie ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 4227\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
      "        num_rows: 579\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 39782\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 229661\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:00<00:00, 6195.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for distilbert/distilbert-base-multilingual-cased on energie: {'accuracy': 0.9782608695652174, 'precision': 0.9583333333333334, 'recall': 1.0, 'f1': 0.9787234042553191}\n",
      "\n",
      "\n",
      "###### Evaluating model google-bert/bert-base-multilingual-cased on energie ###### \n",
      "\n",
      "\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 4227\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path', 'preds', 'probas'],\n",
      "        num_rows: 579\n",
      "    })\n",
      "    holdout: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 39782\n",
      "    })\n",
      "    extended: Dataset({\n",
      "        features: ['_id', 'batch_id', 'domain', 'view_url', 'lang', 'text', 'text_length', 'word_count', 'topic', 'category', 'good_for_training', 'good_for_augmentation', 'annotation_type', 'is_topic', 'label', 'token_count', 'chunk_id', 'url_path'],\n",
      "        num_rows: 229661\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 579/579 [00:00<00:00, 6128.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for google-bert/bert-base-multilingual-cased on energie: {'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'f1': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "eval_results_pages = defaultdict(dict)\n",
    "\n",
    "for topic in TOPICS: # ----------------------------------------------------------------------\n",
    "    for model_name in MODELS: # -------------------------------------------------------------\n",
    "\n",
    "        print(f\"\\n\\n###### Evaluating model {model_name} on {topic} ###### \\n\\n\")\n",
    "        dataset = load_from_disk(f\"../../data/tmp/processed_dataset_{topic}_buffed_chunkified_{SAMPLING}{SUFFIX}_{MAX_CONTENT_LENGTH}_s_{model_name.split('/')[1]}_{FEATURES}_{SPLIT}\")\n",
    "        \n",
    "        print(dataset)\n",
    "        \n",
    "        # Group dataset examples by URL, with a fallback to domain\n",
    "        grouped_dataset = {}\n",
    "        for example in tqdm(dataset[SPLIT]):\n",
    "            url = example.get(\"view_url\") or example.get(\"domain\")\n",
    "            example_filtered = {k: example[k] for k in [\"text\", \"domain\", \"preds\", \"label\", \"category\", \"annotation_type\", \"lang\"]}\n",
    "            grouped_dataset.setdefault(url, []).append(example_filtered)\n",
    "            \n",
    "        # Extract labels\n",
    "        labels = []\n",
    "        for url, chunks in grouped_dataset.items():\n",
    "            preds = [chunk[\"label\"] for chunk in chunks]\n",
    "            labels.append(max(preds))\n",
    "            \n",
    "        # Merge chunk level predictions\n",
    "        predictions = []\n",
    "        for url, chunks in grouped_dataset.items():\n",
    "            preds = [chunk[\"preds\"] for chunk in chunks]\n",
    "            pred = majority_voting([pred for pred in preds if pred > 0]) if max(preds) > 0 else 0\n",
    "            predictions.append(pred)\n",
    "    \n",
    "        # Use the trained model to make predictions on the test set\n",
    "        metrics = calc_metrics(labels, predictions)\n",
    "        print(f\"Metrics for {model_name} on {topic}: {metrics}\")\n",
    "        \n",
    "        # Update the eval_results dictionary\n",
    "        eval_results_pages[model_name][topic] = metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Chunk Level Predictions and Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to save the dictionary\n",
    "file_path = f\"eval_results_{FEATURES}_{SPLIT}_pages.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dictionary to disk as JSON\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(eval_results_pages, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, \"r\") as file:\n",
    "    eval_results_pages = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all topics (assuming all models are evaluated on the same topics)\n",
    "topics = list(next(iter(eval_results_pages.values())).keys())\n",
    "\n",
    "# Prepare headers for the table: each topic will have four metrics\n",
    "headers = [\"Model\"] + [f\"{topic} {metric}\" for topic in topics for metric in [\"Acc.\", \"Prec.\", \"Rec.\", \"F1\"]]\n",
    "\n",
    "# Prepare rows: one row per model, containing metrics for each topic\n",
    "rows = []\n",
    "for model, topics_metrics in eval_results_pages.items():\n",
    "    row = [model]  # Start with the model name\n",
    "    for topic in topics:\n",
    "        metrics = topics_metrics.get(topic, {})\n",
    "        row.extend([metrics.get('accuracy',0.0), metrics.get('precision',0.0), metrics.get('recall',0.0), metrics.get('f1',0.0)])\n",
    "    rows.append(row)\n",
    "\n",
    "# Generate the HTML table\n",
    "table_html = tabulate(rows, headers=headers, tablefmt=\"html\", showindex=\"never\", floatfmt=\".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Model                                        </th><th style=\"text-align: right;\">  cannabis Acc.</th><th style=\"text-align: right;\">  cannabis Prec.</th><th style=\"text-align: right;\">  cannabis Rec.</th><th style=\"text-align: right;\">  cannabis F1</th><th style=\"text-align: right;\">  kinder Acc.</th><th style=\"text-align: right;\">  kinder Prec.</th><th style=\"text-align: right;\">  kinder Rec.</th><th style=\"text-align: right;\">  kinder F1</th><th style=\"text-align: right;\">  energie Acc.</th><th style=\"text-align: right;\">  energie Prec.</th><th style=\"text-align: right;\">  energie Rec.</th><th style=\"text-align: right;\">  energie F1</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>distilbert/distilbert-base-multilingual-cased</td><td style=\"text-align: right;\">          0.977</td><td style=\"text-align: right;\">           0.952</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">        0.976</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">      1.000</td><td style=\"text-align: right;\">         0.978</td><td style=\"text-align: right;\">          0.958</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       0.979</td></tr>\n",
       "<tr><td>google-bert/bert-base-multilingual-cased     </td><td style=\"text-align: right;\">          0.977</td><td style=\"text-align: right;\">           0.952</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">        0.976</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">        1.000</td><td style=\"text-align: right;\">      1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">          1.000</td><td style=\"text-align: right;\">         1.000</td><td style=\"text-align: right;\">       1.000</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(table_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is: google-bert/bert-base-multilingual-cased\n",
      "Average F1 score of the best model: 0.9919\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average F1 score for each model\n",
    "average_f1_scores = {}\n",
    "for model, categories in eval_results_pages.items():\n",
    "    total_f1 = 0\n",
    "    count = 0\n",
    "    for category, metrics in categories.items():\n",
    "        total_f1 += metrics[\"f1\"]\n",
    "        count += 1\n",
    "    average_f1_scores[model] = total_f1 / count\n",
    "\n",
    "# Find the model with the highest average F1 score\n",
    "best_model = max(average_f1_scores, key=average_f1_scores.get)\n",
    "best_model_average_f1 = average_f1_scores[best_model]\n",
    "\n",
    "print(f\"The best model is: {best_model}\")\n",
    "print(f\"Average F1 score of the best model: {best_model_average_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2j-content-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
